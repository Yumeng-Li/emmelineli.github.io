<!DOCTYPE html>
<html class="has-navbar-fixed-top">
<head>
    <meta charset="utf-8">
<title>YumengsQuantLab</title>
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/outdated-browser/1.1.5/outdatedbrowser.min.css">




<meta name="description" content="">





    <meta property="og:type" content="website">
<meta property="og:title" content="YumengsQuantLab">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="YumengsQuantLab">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Yumeng Li">
<meta name="twitter:card" content="summary">





<link rel="icon" href="/favicon.png">


<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Ovo|Source+Code+Pro">
<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/bulma/0.6.2/css/bulma.min.css">


<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/lightgallery/1.6.8/css/lightgallery.min.css">
<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/justifiedGallery/3.6.5/css/justifiedGallery.min.css">


<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/atom-one-light.min.css">


<link rel="stylesheet" href="/css/style.css">


<script defer src="//use.fontawesome.com/releases/v5.0.8/js/all.js"></script>


    
    
    
    
    
    
    
    
    
    

    


<meta name="generator" content="Hexo 5.0.0"></head>
<body>
    
<nav class="navbar is-transparent is-fixed-top navbar-main" role="navigation" aria-label="main navigation">
    <div class="container">
        <div class="navbar-brand">
            <a class="navbar-item navbar-logo" href="/">
                
                    
                    YUMENG&#39;S QUANT LAB
                    
                
            </a>
            <div class="navbar-burger">
                <span></span>
                <span></span>
                <span></span>
            </div>
        </div>
        
        <div class="navbar-menu navbar-start">
            
            <a class="navbar-item "
               href="/archives">Archives</a>
            
            <a class="navbar-item "
               href="/categories">Category</a>
            
            <a class="navbar-item "
               target="_blank" rel="noopener" href="http://yumengsblog.com/">Blog</a>
            
            <a class="navbar-item "
               target="_blank" rel="noopener" href="https://www.linkedin.com/in/yumenglovestheworld/">Linkedin</a>
            
        </div>
        
        <div class="navbar-menu navbar-end">
            
            <a class="navbar-item search" title="Search" href="javascript:;">
                <i class="fas fa-search"></i>
            </a>
            
            
            
            <a class="navbar-item" title="GitHub" target="_blank" rel="noopener" href="https://github.com/ppoffice/hexo-theme-minos">
                
                <i class="fab fa-github"></i>
                
            </a>
               
            
        </div>
    </div>
</nav>

    <section class="section">
    <div class="container">
    
        <article class="article content gallery" itemscope itemprop="blogPost">
    <h1 class="article-title is-size-3 is-size-4-mobile" itemprop="name">
        
            <a href="/2021/09/26/projectcovidGLM/" itemprop="url">Project Covid Admissions Based on Vaccine Progress, Part 2 -- Generalized Linear Model, From Poisson to Negative Binomial</a>
        
    </h1>
    <div class="article-meta columns is-variable is-1 is-multiline is-mobile is-size-7-mobile">
        <span class="column is-narrow">
            <time datetime="2021-09-27T02:08:38.646Z" itemprop="datePublished">Sep 26 2021</time>
        </span>
        
        
        <span class="column is-narrow">
            
            
            18 minutes read (About 2659 words)
        </span>
        
    </div>
    <div class="article-entry is-size-6-mobile" itemprop="articleBody">
    
        <html><head></head><body><p>In the last blog, we try to use linear regressions to model hospital admissions and vaccinations. The findings were the linear model tend to over-estimating how fast the admission rate is increasing on the higher range of vaccination rates, and residual variance is negatively correlated with vaccinations rates, suggesting heteroskedasticity. Generalized linear models which allow for the response variable to have an error distribution other than the normal distribution can fix those issues. So this will be our topic today.</p>
<h2 id="Preliminaries"><a href="#Preliminaries" class="headerlink" title="Preliminaries"></a>Preliminaries</h2><figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np</span><br><span class="line"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd</span><br><span class="line"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt</span><br><span class="line"><span class="hljs-keyword">import</span> matplotlib.colors</span><br><span class="line"><span class="hljs-keyword">import</span> seaborn <span class="hljs-keyword">as</span> sns</span><br><span class="line"><span class="hljs-keyword">import</span> statsmodels.api <span class="hljs-keyword">as</span> sm</span><br><span class="line"><span class="hljs-keyword">from</span> statsmodels.discrete.discrete_model <span class="hljs-keyword">import</span> Poisson</span><br><span class="line"><span class="hljs-keyword">from</span> statsmodels.discrete.discrete_model <span class="hljs-keyword">import</span> NegativeBinomial</span><br><span class="line"><span class="hljs-keyword">import</span> sys</span><br><span class="line"><span class="hljs-keyword">import</span> covid_analysis <span class="hljs-keyword">as</span> covid</span><br></pre></td></tr></tbody></table></figure>

<p>The data I use includes hospital admissions and vaccinations from the CDC website, and the most up-to-date populations by state from the US Census Bureau.</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data_dir=<span class="hljs-string">&quot;../data&quot;</span></span><br></pre></td></tr></tbody></table></figure>


<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">hospitalizations=pd.read_csv(<span class="hljs-string">f&quot;<span class="hljs-subst">{data_dir}</span>/covid_hospitalizations.csv&quot;</span>,parse_dates=[<span class="hljs-string">&quot;date&quot;</span>])</span><br><span class="line">vaccinations=pd.read_csv(<span class="hljs-string">f&quot;<span class="hljs-subst">{data_dir}</span>/covid_vaccinations.csv&quot;</span>,parse_dates=[<span class="hljs-string">&quot;date&quot;</span>])</span><br><span class="line">population=pd.read_csv(<span class="hljs-string">f&quot;<span class="hljs-subst">{data_dir}</span>/population.csv&quot;</span>)</span><br><span class="line"></span><br><span class="line">event=<span class="hljs-string">&quot;admissions&quot;</span></span><br><span class="line">data=hospitalizations</span><br><span class="line"></span><br><span class="line">data=data.merge(vaccinations,on=[<span class="hljs-string">&quot;date&quot;</span>,<span class="hljs-string">&quot;state&quot;</span>])</span><br><span class="line">data=data.merge(population,on=<span class="hljs-string">&quot;state&quot;</span>)</span><br><span class="line">data[<span class="hljs-string">&quot;vaccinated&quot;</span>]=data[<span class="hljs-string">&quot;vaccinated&quot;</span>]/data[<span class="hljs-string">&quot;population&quot;</span>]</span><br><span class="line">data.head()</span><br></pre></td></tr></tbody></table></figure>


<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>date</th>
      <th>state</th>
      <th>used_beds</th>
      <th>admissions</th>
      <th>vaccinated</th>
      <th>population</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>2021-06-10</td>
      <td>MT</td>
      <td>66.0</td>
      <td>16.0</td>
      <td>0.397597</td>
      <td>1080577</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2021-06-21</td>
      <td>MT</td>
      <td>57.0</td>
      <td>13.0</td>
      <td>0.411614</td>
      <td>1080577</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2021-08-05</td>
      <td>MT</td>
      <td>149.0</td>
      <td>23.0</td>
      <td>0.440153</td>
      <td>1080577</td>
    </tr>
    <tr>
      <th>3</th>
      <td>2021-07-15</td>
      <td>MT</td>
      <td>62.0</td>
      <td>9.0</td>
      <td>0.431722</td>
      <td>1080577</td>
    </tr>
    <tr>
      <th>4</th>
      <td>2021-08-21</td>
      <td>MT</td>
      <td>224.0</td>
      <td>59.0</td>
      <td>0.449196</td>
      <td>1080577</td>
    </tr>
  </tbody>
</table>
</div>



<h2 id="Poisson-Regression"><a href="#Poisson-Regression" class="headerlink" title="Poisson Regression"></a>Poisson Regression</h2><p>Like what we did with the linear regression, we can choose either to use the  <strong>Poisson Regression</strong> model from <a target="_blank" rel="noopener" href="https://www.statsmodels.org/stable/generated/statsmodels.discrete.discrete_model.Poisson.html">statsmodels</a>, or the GLM from the same lib. The two will give us the exact same estimations and statistics.</p>
<h3 id="Model-Definition"><a href="#Model-Definition" class="headerlink" title="Model Definition"></a>Model Definition</h3><p>We have data from multiple dates, so it makes sense to incorporate Time as a extra variable.</p>
<p>\begin{eqnarray}<br>    &amp;\eta_i &amp;=c + \beta_1 x_i  + \beta_2 t_i + \beta_3 t_i^2 \newline<br>    \mathbb{E}(y_i|x_i) =&amp; \lambda_iP_i &amp;= e^{\eta_i }P_i \newline<br>    &amp;p(y_i|x_i) &amp; = \text{Poisson}(y;\lambda_i P_i)<br>\end{eqnarray}<br>where </p>
<ul>
<li>Each observation $i$ represents a single state at one particular date.</li>
<li>$y_i$ is the number of hospital admissions on that state.</li>
<li>$x_i$ is the percentage of the state population fully vaccinated that state.</li>
<li>$t_i$ is the number of days elapsed since the beginning of the training period.</li>
<li>$P_i$ is the population of the  state.</li>
<li>the parameters $c$ and $\beta_1,\beta_2,\beta_3$ will be fitted to the observed data to maximize agreement with the model</li>
</ul>
<p>We including coefficients for $t$ and $t^2$ the dependence of the admission rate on time can have some curvature and does not need to be linear.</p>
<p>This can be fitter as a <strong>linear model</strong> where the inputs are $x_i$, $t_i$ and $t_i^2$</p>
<p>First we select the training period:</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">start_train=<span class="hljs-string">&quot;2021-07-15&quot;</span></span><br><span class="line">end_train=<span class="hljs-string">&quot;2021-08-15&quot;</span></span><br><span class="line">train_data=data[(data[<span class="hljs-string">&apos;date&apos;</span>] &gt;= start_train) &amp; (data[<span class="hljs-string">&apos;date&apos;</span>] &lt;= end_train)].copy()</span><br><span class="line">date0=train_data[<span class="hljs-string">&quot;date&quot;</span>].min()</span><br></pre></td></tr></tbody></table></figure>

<p>Then a 7-day testing period:</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">test_period=<span class="hljs-number">7</span> <span class="hljs-comment"># days</span></span><br><span class="line">test_end=pd.Timestamp(end_train)+pd.DateOffset(days=test_period)</span><br><span class="line">test_data=data[(data[<span class="hljs-string">&quot;date&quot;</span>]&gt;end_train) &amp; (data[<span class="hljs-string">&quot;date&quot;</span>]&lt;=test_end)].copy()</span><br></pre></td></tr></tbody></table></figure>


<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">X_train,P_train=covid.define_variables(train_data,date0)</span><br><span class="line">Y_train=train_data[event]</span><br><span class="line">X_test,P_test=covid.define_variables(test_data,date0)</span><br><span class="line">Y_test=test_data[event]</span><br></pre></td></tr></tbody></table></figure>

<p>Fit the model to the train data,</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mod=Poisson(Y_train,X_train,exposure=P_train.values)</span><br><span class="line">res=mod.fit()</span><br><span class="line">res.summary()</span><br></pre></td></tr></tbody></table></figure>

<pre><code>Optimization terminated successfully.
         Current function value: 32.115992
         Iterations 6</code></pre>
<table class="simpletable">
<caption>Poisson Regression Results</caption>
<tbody><tr>
  <th>Dep. Variable:</th>      <td>admissions</td>    <th>  No. Observations:  </th>  <td>  1632</td> 
</tr>
<tr>
  <th>Model:</th>                <td>Poisson</td>     <th>  Df Residuals:      </th>  <td>  1628</td> 
</tr>
<tr>
  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>  <td>     3</td> 
</tr>
<tr>
  <th>Date:</th>            <td>Sun, 26 Sep 2021</td> <th>  Pseudo R-squ.:     </th>  <td>0.4493</td> 
</tr>
<tr>
  <th>Time:</th>                <td>19:44:49</td>     <th>  Log-Likelihood:    </th> <td> -52413.</td>
</tr>
<tr>
  <th>converged:</th>             <td>True</td>       <th>  LL-Null:           </th> <td> -95175.</td>
</tr>
<tr>
  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th>  <td> 0.000</td> 
</tr>
</tbody></table>
<table class="simpletable">
<tbody><tr>
       <td></td>         <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P&gt;|z|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>vaccinated</th> <td>   -6.7589</td> <td>    0.030</td> <td> -225.710</td> <td> 0.000</td> <td>   -6.818</td> <td>   -6.700</td>
</tr>
<tr>
  <th>T</th>          <td>    0.0750</td> <td>    0.001</td> <td>   75.492</td> <td> 0.000</td> <td>    0.073</td> <td>    0.077</td>
</tr>
<tr>
  <th>T2</th>         <td>   -0.0009</td> <td> 2.84e-05</td> <td>  -31.349</td> <td> 0.000</td> <td>   -0.001</td> <td>   -0.001</td>
</tr>
<tr>
  <th>const</th>      <td>   -8.4423</td> <td>    0.015</td> <td> -547.566</td> <td> 0.000</td> <td>   -8.473</td> <td>   -8.412</td>
</tr>
</tbody></table>




<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mod=sm.GLM(Y_train,X_train,exposure=P_train,family=sm.families.Poisson(sm.families.links.log()))</span><br><span class="line">glm_res=mod.fit()</span><br><span class="line">glm_res.summary()</span><br></pre></td></tr></tbody></table></figure>




<table class="simpletable">
<caption>Generalized Linear Model Regression Results</caption>
<tbody><tr>
  <th>Dep. Variable:</th>      <td>admissions</td>    <th>  No. Observations:  </th>  <td>  1632</td> 
</tr>
<tr>
  <th>Model:</th>                  <td>GLM</td>       <th>  Df Residuals:      </th>  <td>  1628</td> 
</tr>
<tr>
  <th>Model Family:</th>         <td>Poisson</td>     <th>  Df Model:          </th>  <td>     3</td> 
</tr>
<tr>
  <th>Link Function:</th>          <td>log</td>       <th>  Scale:             </th> <td>  1.0000</td>
</tr>
<tr>
  <th>Method:</th>                <td>IRLS</td>       <th>  Log-Likelihood:    </th> <td> -52413.</td>
</tr>
<tr>
  <th>Date:</th>            <td>Sun, 26 Sep 2021</td> <th>  Deviance:          </th> <td>  95443.</td>
</tr>
<tr>
  <th>Time:</th>                <td>19:44:50</td>     <th>  Pearson chi2:      </th> <td>1.26e+05</td>
</tr>
<tr>
  <th>No. Iterations:</th>          <td>6</td>        <th>                     </th>     <td> </td>   
</tr>
<tr>
  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>                     </th>     <td> </td>   
</tr>
</tbody></table>
<table class="simpletable">
<tbody><tr>
       <td></td>         <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P&gt;|z|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>vaccinated</th> <td>   -6.7589</td> <td>    0.030</td> <td> -225.710</td> <td> 0.000</td> <td>   -6.818</td> <td>   -6.700</td>
</tr>
<tr>
  <th>T</th>          <td>    0.0750</td> <td>    0.001</td> <td>   75.492</td> <td> 0.000</td> <td>    0.073</td> <td>    0.077</td>
</tr>
<tr>
  <th>T2</th>         <td>   -0.0009</td> <td> 2.84e-05</td> <td>  -31.349</td> <td> 0.000</td> <td>   -0.001</td> <td>   -0.001</td>
</tr>
<tr>
  <th>const</th>      <td>   -8.4423</td> <td>    0.015</td> <td> -547.566</td> <td> 0.000</td> <td>   -8.473</td> <td>   -8.412</td>
</tr>
</tbody></table>



<p>Poisson regression is fit by maximum likelihood, there are several <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Logistic_regression#Pseudo-R-squared">choices of <strong>Pseudo R-square</strong></a><br>Here, what statsmodels implements for poisson regression is McFadden $R_{McF}^2$ that is defined as ratio of log likelihood for the fitted model and log likelihood of a model fitted to just a constant.</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment"># Pseudo R-squ McF</span></span><br><span class="line">res.prsquared</span><br></pre></td></tr></tbody></table></figure>




<pre><code>0.4492953669633617</code></pre>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">res.llf, res.llnull</span><br></pre></td></tr></tbody></table></figure>




<pre><code>(-52413.29834953645, -95174.97258108124)</code></pre>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-number">1</span>-res.llf/res.llnull</span><br></pre></td></tr></tbody></table></figure>




<pre><code>0.4492953669633617</code></pre>
<p>Certainly, we can implement other measures of the Pseudo R-square ourselves. Here I did $R_{L}^2$, which is deviance-based. It&#x2019;s the most analogous index to the squared multiple correlations in linear regression. We will stick to it in our analysis from now on.</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">poisson_deviance</span>(<span class="hljs-params">y,y_pred</span>):</span></span><br><span class="line">    <span class="hljs-keyword">return</span> <span class="hljs-number">2</span>*np.sum(y*np.log(np.maximum(y,<span class="hljs-number">1e-12</span>)/y_pred)-(y-y_pred))</span><br></pre></td></tr></tbody></table></figure>


<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">mu_train=Y_train.sum()/P_train.sum()</span><br><span class="line">Y_pred=glm_res.predict(X_train,exposure=P_train)</span><br><span class="line"></span><br><span class="line">deviance=poisson_deviance(Y_train,Y_pred)</span><br><span class="line">null_deviance=poisson_deviance(Y_train,mu_train*P_train)</span><br><span class="line">R2=<span class="hljs-number">1</span>-deviance/null_deviance</span><br><span class="line"><span class="hljs-comment"># Pseudo R-squ</span></span><br><span class="line">R2</span><br></pre></td></tr></tbody></table></figure>




<pre><code>0.4725915412837366</code></pre>
<p>If you don&#x2019;t want bother yourself with the mathematical formulas, here&#x2019;s a simpler way (my favorite as the laziest person ever). Just fit the model with a constant, and take the log-likelihood/deviance as your null case.</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">mod=sm.GLM(Y_train,X_train[<span class="hljs-string">&apos;const&apos;</span>],exposure=P_train,family=sm.families.Poisson(sm.families.links.log()))</span><br><span class="line">res_null=mod.fit()</span><br><span class="line"><span class="hljs-comment"># deviance for a const fit model</span></span><br><span class="line">res_null.deviance</span><br></pre></td></tr></tbody></table></figure>




<pre><code>180966.73552551522</code></pre>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment"># null deviance from our math formula</span></span><br><span class="line">null_deviance</span><br></pre></td></tr></tbody></table></figure>




<pre><code>180966.7355255152</code></pre>
<p>After all, the R-squared we have for now is 0.473.</p>
<p>Next let&#x2019;s check on how the model predicts. </p>
<h3 id="Extrapolation-of-Time-trends"><a href="#Extrapolation-of-Time-trends" class="headerlink" title="Extrapolation of Time trends"></a>Extrapolation of Time trends</h3><p>Remember what we saw last time with the linear model? Its projection was a straight line. But the poisson model is telling us a different story, that the admissions will flatten out assuming the current vaccination rate. Well, it is onto something, isn&#x2019;t it?</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">covid.plot_time_extrapolation(train_data,glm_res,event,date0)</span><br></pre></td></tr></tbody></table></figure>

<img src="http://yumeng-li.github.io/projectcovidGLM_files/projectcovidGLM_30_0.png">



<h3 id="In-sample-model-Fit"><a href="#In-sample-model-Fit" class="headerlink" title="In-sample model Fit"></a>In-sample model Fit</h3><p>To better evaluate the model fit, we visualize the real observations and model predictions. Here&#x2019;s some snapshots of the fit for a few selected dates on the training period.</p>
<p>As we can see, curves from the poisson regression model fits the data much better than the linear model, indicating the correlation between admissions and vaccinations are not linear.</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">covid.plot_time_facets(train_data,glm_res,event,date0)</span><br></pre></td></tr></tbody></table></figure>


<img src="http://yumeng-li.github.io/projectcovidGLM_files/projectcovidGLM_32_0.png">


<p>But is this enough? Certainly not, let&#x2019;s verify the deviance residuals by visualizing them,</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">plot_deviance_residuals</span>(<span class="hljs-params">data,event,res</span>):</span></span><br><span class="line">    P=data[<span class="hljs-string">&quot;population&quot;</span>]</span><br><span class="line">    <span class="hljs-comment"># Create two subplots and unpack the output array immediately</span></span><br><span class="line">    f, (ax1, ax2) = plt.subplots(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>,sharex=<span class="hljs-literal">True</span>,figsize=(<span class="hljs-number">16</span>,<span class="hljs-number">4</span>))</span><br><span class="line">    norm=matplotlib.colors.LogNorm(vmin=<span class="hljs-number">100</span>_000, vmax=P.max(), clip=<span class="hljs-literal">False</span>)</span><br><span class="line">    m=ax1.scatter(data[<span class="hljs-string">&quot;vaccinated&quot;</span>],data[event]/data[<span class="hljs-string">&quot;population&quot;</span>]*<span class="hljs-number">100</span>_000,</span><br><span class="line">                c=P,norm=norm,cmap=<span class="hljs-string">&quot;Blues&quot;</span>,label=event)</span><br><span class="line">    plt.colorbar(m,label=<span class="hljs-string">&quot;State Population&quot;</span>)</span><br><span class="line">    x=np.linspace(<span class="hljs-number">0.3</span>,<span class="hljs-number">0.75</span>,<span class="hljs-number">201</span>)</span><br><span class="line">    x= pd.DataFrame({<span class="hljs-string">&apos;vaccinated&apos;</span>:x, <span class="hljs-string">&apos;T&apos;</span>:<span class="hljs-number">31</span>, <span class="hljs-string">&apos;T2&apos;</span>:<span class="hljs-number">31</span>*<span class="hljs-number">31</span>})</span><br><span class="line">    x[<span class="hljs-string">&quot;const&quot;</span>]=np.ones(<span class="hljs-number">201</span>)</span><br><span class="line">    y_pred=res.predict(x)</span><br><span class="line">    ax1.plot(x.vaccinated, y_pred*<span class="hljs-number">100</span>_000,<span class="hljs-string">&quot;k--&quot;</span>,label=<span class="hljs-string">&quot;predicted&quot;</span>) </span><br><span class="line">    </span><br><span class="line">    idx = data.index</span><br><span class="line">    ax2.scatter(data[<span class="hljs-string">&quot;vaccinated&quot;</span>], res.resid_deviance[idx],</span><br><span class="line">                c=P,norm=norm,cmap=<span class="hljs-string">&quot;Blues&quot;</span>,label=event)</span><br><span class="line">    ax2.set_title(<span class="hljs-string">&quot;Residuals&quot;</span>)</span><br><span class="line">    ax2.set_xlabel(<span class="hljs-string">&quot;vaccination&quot;</span>)</span><br><span class="line">    ax2.set_ylabel(<span class="hljs-string">&quot;residual&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="hljs-comment"># select outlier state</span></span><br><span class="line">    outlier_idx=np.argpartition(np.array(res.resid_deviance[idx]), <span class="hljs-number">-2</span>)[<span class="hljs-number">-2</span>:]</span><br><span class="line">    </span><br><span class="line">    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> outlier_idx:</span><br><span class="line">        outlier=data.iloc[i]</span><br><span class="line">        ax1.annotate(outlier[<span class="hljs-string">&quot;state&quot;</span>],(outlier[<span class="hljs-string">&quot;vaccinated&quot;</span>]+<span class="hljs-number">0.01</span>,outlier[event]/outlier[<span class="hljs-string">&quot;population&quot;</span>]*<span class="hljs-number">100</span>_000))</span><br><span class="line">        ax2.annotate(outlier[<span class="hljs-string">&quot;state&quot;</span>],(outlier[<span class="hljs-string">&quot;vaccinated&quot;</span>]+<span class="hljs-number">0.01</span>,res.resid_deviance[idx].iloc[i]))</span><br></pre></td></tr></tbody></table></figure>

<p>The date I choose is Aug.15,</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">offset=<span class="hljs-number">9</span> </span><br><span class="line">interested_date=data[<span class="hljs-string">&quot;date&quot;</span>].max()-pd.offsets.Day(offset)</span><br><span class="line">interested_data=data[data[<span class="hljs-string">&quot;date&quot;</span>]==interested_date]</span><br></pre></td></tr></tbody></table></figure>


<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plot_deviance_residuals(interested_data,event,glm_res)</span><br></pre></td></tr></tbody></table></figure>

<img src="http://yumeng-li.github.io/projectcovidGLM_files/projectcovidGLM_37_0.png">



<p>The deviance residuals are defined as the signed square roots of the unit deviances, representing the contributions of individual samples to the deviance. The deviance indicates the extent to which the likelihood of the saturated model exceeds the likelihood of the proposed model. If the proposed model has a good fit, the deviance will be small. If the proposed model has a bad fit, the deviance will be high. </p>
<p>Thus, the deviance residuals are analogous to the conventional residuals: when they are squared, we obtain the sum of squares that we use for assessing the fit of the model. However, while the sum of squares is the residual sum of squares for linear models, for GLMs, this is the deviance.</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">deviance=poisson_deviance(Y_train,Y_pred)</span><br><span class="line">sum_of_squared_deviance_residuals = sum(glm_res.resid_deviance**<span class="hljs-number">2</span>)</span><br><span class="line">deviance, sum_of_squared_deviance_residuals</span><br></pre></td></tr></tbody></table></figure>




<pre><code>(95443.38706242564, 95443.38706242583)</code></pre>
<p>In a <em>properly specified model</em>, the deviance is approximately chi-square distributed with n-k-1 degrees of freedom, and the deviance residuals would be independent, standard normal random variables, i.e., ~ norm(0,1). So we would expect the residuals to be mostly in range of  -1 to 1, and rarely fall outside the &#xB1; 3 limits.</p>
<p>However, the model residuals are 10 times larger than expected and we say there is <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Overdispersion"><strong>overdispersion</strong></a>. If overdispersion is present in a dataset, the estimated standard errors and test statistics, the overall goodness-of-fit will be distorted and adjustments must be made. </p>
<p>A more appropriate model will be a Negative Binomial regression.</p>
<h2 id="Model-Prediction"><a href="#Model-Prediction" class="headerlink" title="Model Prediction"></a>Model Prediction</h2><p>We will now use the model <strong>without recalibrating</strong> to make predictions about admission rates on new data.</p>
<p>This is <strong>out of sample</strong> evaluation. It is the only way to make sure the model really works.</p>
<h3 id="Out-of-Sample-R-2"><a href="#Out-of-Sample-R-2" class="headerlink" title="Out of Sample $R^2$"></a>Out of Sample $R^2$</h3><figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">test_admission_mean=Y_test.sum()/P_test.sum()</span><br><span class="line">Y_pred=glm_res.predict(X_test,exposure=P_test)</span><br><span class="line"></span><br><span class="line">null_deviance_test=poisson_deviance(Y_test,P_test*test_admission_mean)</span><br><span class="line">deviance_test=poisson_deviance(Y_test,Y_pred)</span><br><span class="line">R2 = <span class="hljs-number">1</span>-deviance_test/null_deviance_test</span><br><span class="line">R2</span><br></pre></td></tr></tbody></table></figure>




<pre><code>0.371917369051069</code></pre>
<p>Out of sample $R^2$ is worse than for the in sample (approx. 0.473). I think we&#x2019;ll all agree, predicting the future is hard.</p>
<h2 id="Negative-Binomial-Regression"><a href="#Negative-Binomial-Regression" class="headerlink" title="Negative Binomial Regression"></a>Negative Binomial Regression</h2><h3 id="Model-Definition-1"><a href="#Model-Definition-1" class="headerlink" title="Model Definition"></a>Model Definition</h3><p>We have data from multiple dates, so it makes sense to incorporate Time as a extra variable.</p>
<p>\begin{eqnarray}<br>    &amp;\eta_i &amp;=c + \beta_1 x_i  + \beta_2 t_i + \beta_3 t_i^2 \newline<br>      \mathbb{E}(y_i|x_i) =&amp; \lambda_i P_i &amp;= e^{\eta_i } P_i \newline<br>    &amp;p(y_i|x_i) &amp; = \text{NB}(y_i; \lambda_i P_i, \alpha)<br>\end{eqnarray}<br>where </p>
<ul>
<li>Each observation $i$ represents a single state at one particular date.</li>
<li>$y_i$ is the number of hospital admissions on that state.</li>
<li>$x_i$ is the percentage of the state population fully vaccinated that state.</li>
<li>$t_i$ is the number of days elapsed since the beginning of the training period.</li>
<li>$P_i$ is the population of the  state.</li>
<li>the parameters $c$ and $\beta_1,\beta_2,\beta_3$ will be fitted to the observed data to maximize agreement with the model</li>
</ul>
<p>We including coefficients for $t$ and $t^2$ the dependence of the admission rate on time can have some curvature and does not need to be linear.</p>
<p>This can be fitter as a <strong>linear model</strong> where the inputs are $x_i$, $t_i$ and $t_i^2$</p>
<h4 id="Variance-Comparison-to-Poisson-Model"><a href="#Variance-Comparison-to-Poisson-Model" class="headerlink" title="Variance Comparison to Poisson Model"></a>Variance Comparison to Poisson Model</h4><p>Introducing a free additional parameter $\alpha$ give more accurate models than simple parametric models like the Poisson distribution by allowing the mean and variance to be different, unlike the Poisson. The negative binomial distribution has a variance $\hat{y}+\hat{y}^2\alpha$ . This can make the distribution a useful overdispersed alternative to the Poisson distribution.</p>
<p>As we can see, with an extra parameter $\alpha$ to control the variance, the expected variance of observations is larger with the Negative Binomial Model than with the Poisson model.</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">alpha = <span class="hljs-number">0.5</span></span><br><span class="line">y_hat=np.linspace(<span class="hljs-number">0</span>,<span class="hljs-number">10</span>,<span class="hljs-number">201</span>)</span><br><span class="line">plt.plot(y_hat,y_hat,<span class="hljs-string">&quot;k--&quot;</span>,label=<span class="hljs-string">&quot;Poisson&quot;</span>)</span><br><span class="line">plt.plot(y_hat,y_hat+alpha*y_hat**<span class="hljs-number">2</span>,<span class="hljs-string">&quot;k-&quot;</span>,linewidth=<span class="hljs-number">3</span>,label=<span class="hljs-string">&quot;Negative Binomial&quot;</span>)</span><br><span class="line">plt.xlabel(<span class="hljs-string">r&quot;$\hat{y}$&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="hljs-string">r&quot;Var($y|\hat{y}$)&quot;</span>)</span><br><span class="line">plt.legend()</span><br></pre></td></tr></tbody></table></figure>


<img src="http://yumeng-li.github.io/projectcovidGLM_files/projectcovidGLM_49_1.png">


<h4 id="Choosing-alpha"><a href="#Choosing-alpha" class="headerlink" title="Choosing $\alpha$"></a>Choosing $\alpha$</h4><p>Negative Binomial is a GLM model with an extra parameter $\alpha$ that we must choose by maximizing the log likelihood. </p>
<p>The train/test data we fit to the Negative Binomial Regression is the same as what we create at start.</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">lls=[]</span><br><span class="line">alphas=np.linspace(<span class="hljs-number">0.1</span>,<span class="hljs-number">0.5</span>,<span class="hljs-number">500</span>)</span><br><span class="line"><span class="hljs-keyword">for</span> alpha <span class="hljs-keyword">in</span> alphas:</span><br><span class="line">    glm_model=sm.GLM(Y_train,X_train,exposure=P_train,family=sm.families.NegativeBinomial(sm.families.links.log(),alpha=alpha))</span><br><span class="line">    glm_res=glm_model.fit()</span><br><span class="line">    ll=glm_res.llf</span><br><span class="line">    lls.append(ll)</span><br><span class="line">plt.semilogx(alphas,lls) </span><br><span class="line">plt.xlabel(<span class="hljs-string">r&quot;$\alpha$&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="hljs-string">&quot;Log Likelihood&quot;</span>)</span><br><span class="line">alpha=alphas[np.argmax(lls)]</span><br><span class="line">print(<span class="hljs-string">&quot;alpha&quot;</span>,alpha)</span><br></pre></td></tr></tbody></table></figure>

<pre><code>alpha 0.3004008016032064</code></pre>
<img src="http://yumeng-li.github.io/projectcovidGLM_files/projectcovidGLM_52_1.png">


<p>The best alpha is 0.3004 given a run of 500 times. Or easier, we can go for the  <strong>Negative Regression</strong> model from <a target="_blank" rel="noopener" href="https://www.statsmodels.org/stable/generated/statsmodels.discrete.discrete_model.Poisson.html">statsmodels</a>, which can do the dirty work, optimizing alpha for us.</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment">#mod=sm.GLM(Y_train,X_train,exposure=P_train,family=nb)</span></span><br><span class="line">mod=NegativeBinomial(Y_train,X_train,exposure=P_train)</span><br><span class="line">res=mod.fit(method=<span class="hljs-string">&quot;lbfgs&quot;</span>)</span><br><span class="line">res.summary()</span><br></pre></td></tr></tbody></table></figure>

<pre><code>/opt/anaconda3/lib/python3.7/site-packages/statsmodels/discrete/discrete_model.py:2642: RuntimeWarning: divide by zero encountered in log
  llf = coeff + size*np.log(prob) + endog*np.log(1-prob)
/opt/anaconda3/lib/python3.7/site-packages/statsmodels/discrete/discrete_model.py:2642: RuntimeWarning: invalid value encountered in multiply
  llf = coeff + size*np.log(prob) + endog*np.log(1-prob)
/opt/anaconda3/lib/python3.7/site-packages/statsmodels/base/model.py:568: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals
  ConvergenceWarning)</code></pre>
<table class="simpletable">
<caption>NegativeBinomial Regression Results</caption>
<tbody><tr>
  <th>Dep. Variable:</th>      <td>admissions</td>    <th>  No. Observations:  </th>  <td>  1632</td> 
</tr>
<tr>
  <th>Model:</th>           <td>NegativeBinomial</td> <th>  Df Residuals:      </th>  <td>  1628</td> 
</tr>
<tr>
  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>  <td>     3</td> 
</tr>
<tr>
  <th>Date:</th>            <td>Sun, 26 Sep 2021</td> <th>  Pseudo R-squ.:     </th>  <td>0.08843</td>
</tr>
<tr>
  <th>Time:</th>                <td>20:24:21</td>     <th>  Log-Likelihood:    </th> <td> -7946.2</td>
</tr>
<tr>
  <th>converged:</th>             <td>False</td>      <th>  LL-Null:           </th> <td> -8717.1</td>
</tr>
<tr>
  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th>  <td> 0.000</td> 
</tr>
</tbody></table>
<table class="simpletable">
<tbody><tr>
       <td></td>         <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P&gt;|z|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>vaccinated</th> <td>   -6.7968</td> <td>    0.179</td> <td>  -38.019</td> <td> 0.000</td> <td>   -7.147</td> <td>   -6.446</td>
</tr>
<tr>
  <th>T</th>          <td>    0.0525</td> <td>    0.006</td> <td>    8.645</td> <td> 0.000</td> <td>    0.041</td> <td>    0.064</td>
</tr>
<tr>
  <th>T2</th>         <td>   -0.0002</td> <td>    0.000</td> <td>   -1.262</td> <td> 0.207</td> <td>   -0.001</td> <td>    0.000</td>
</tr>
<tr>
  <th>const</th>      <td>   -8.4877</td> <td>    0.092</td> <td>  -92.109</td> <td> 0.000</td> <td>   -8.668</td> <td>   -8.307</td>
</tr>
<tr>
  <th>alpha</th>      <td>    0.3025</td> <td>    0.011</td> <td>   26.621</td> <td> 0.000</td> <td>    0.280</td> <td>    0.325</td>
</tr>
</tbody></table>



<p>The alpha values are close,</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">p=res.params</span><br><span class="line">nb=sm.families.NegativeBinomial(alpha=p[<span class="hljs-string">&quot;alpha&quot;</span>])</span><br><span class="line">nb.alpha</span><br></pre></td></tr></tbody></table></figure>




<pre><code>0.3024936677385029</code></pre>
<p><strong>Compared to Poisson:</strong><br><strong>- The regression coefficients are very similar.</strong><br><strong>- The confidence intervals are much wider.</strong><br><strong>- The t statistics are smaller because we assume a larger variance of residuals.</strong></p>
<p><strong>In summary, switching from Poisson to Negative Binominal yields stable coefficient estimates, but a higher chance for the null hepothesis to be rejected and more reliable confidence intervals.</strong></p>
<p>The <code>statsmodel.family.NegativeBinomial</code> object knows how to compute deviance, and we use it to calculate the R squared,</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">mu_train=Y_train.sum()/P_train.sum()</span><br><span class="line">Y_pred=res.predict(X_train,exposure=P_train)</span><br><span class="line">deviance=nb.deviance(Y_train,Y_pred)</span><br><span class="line">null_deviance=nb.deviance(Y_train,mu_train*P_train)</span><br><span class="line">R2=<span class="hljs-number">1</span>-deviance/null_deviance</span><br><span class="line">R2</span><br></pre></td></tr></tbody></table></figure>




<pre><code>0.5937123540474427</code></pre>
<h3 id="In-sample-model-Fit-1"><a href="#In-sample-model-Fit-1" class="headerlink" title="In-sample model Fit"></a>In-sample model Fit</h3><figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">covid.plot_time_facets(train_data,res,event,date0)</span><br></pre></td></tr></tbody></table></figure>

<img src="http://yumeng-li.github.io/projectcovidGLM_files/projectcovidGLM_61_0.png">


<p>Then most excitingly, let&#x2019;s check out the deviance residuals again. The residuals are now well behaved, randomly fall into the range of -1 to 1. Though FL, KY and a few other states seem to be outliers. But no worries, we will address them in our next blog, using a more advanced method - mixed models.</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plot_deviance_residuals(interested_data,event,glm_res)</span><br></pre></td></tr></tbody></table></figure>

<img src="http://yumeng-li.github.io/projectcovidGLM_files/projectcovidGLM_63_0.png">


<h2 id="Model-Prediction-1"><a href="#Model-Prediction-1" class="headerlink" title="Model Prediction"></a>Model Prediction</h2><p>Finally, always take a look at the out-of-sample fit. </p>
<p>Though the R squared is not as good as the in-sample, but there&#x2019;s still an improvement comparing to Poisson regression and the linear regressions.</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">test_admission_mean=Y_test.sum()/P_test.sum()</span><br><span class="line">Y_pred=res.predict(X_test,exposure=P_test)</span><br><span class="line"></span><br><span class="line">null_deviance_test=nb.deviance(Y_test,P_test*test_admission_mean)</span><br><span class="line">deviance_test=nb.deviance(Y_test,Y_pred)</span><br><span class="line">R2 = <span class="hljs-number">1</span>-deviance_test/null_deviance_test</span><br><span class="line">R2</span><br></pre></td></tr></tbody></table></figure>




<pre><code>0.4595499732848517</code></pre>
</body></html>
    
    </div>
    
    
</article>




    
        <article class="article content gallery" itemscope itemprop="blogPost">
    <h1 class="article-title is-size-3 is-size-4-mobile" itemprop="name">
        
            <a href="/2021/09/21/projectcovid/" itemprop="url">Project Covid Admissions Based on Vaccine Progress, Part 1 -- From Simple Linear Regression to Weighted Time Series Regression</a>
        
    </h1>
    <div class="article-meta columns is-variable is-1 is-multiline is-mobile is-size-7-mobile">
        <span class="column is-narrow">
            <time datetime="2021-09-21T20:37:06.431Z" itemprop="datePublished">Sep 21 2021</time>
        </span>
        
        
        <span class="column is-narrow">
            
            
            16 minutes read (About 2473 words)
        </span>
        
    </div>
    <div class="article-entry is-size-6-mobile" itemprop="articleBody">
    
        <html><head></head><body><p>Lately I&#x2019;ve been working on a project predicting credit losses for mortgages with a bunch of loan-level attributes. To be honest, the data wasn&#x2019;t rich. In pursuit of good predictive power, I had to try from the simple multi-period regression, to generalized linear model with distributions beyond the Gaussian family, then mixed effect models, and even used some curve fitting techniques like splines at last to fix the oversimple (or wrong) assumptions I made at the beginning. Overall, I think the journey was inspiring as it basically shows that one can almost crack any problem with regression analysis, more importantly, in an elegant way.</p>
<p>When it comes to predictive modeling, people always turn to linear regressions, which isn&#x2019;t wrong, just so you know that linear regression models and OLS make a number of assumptions about the predictor variables, the response variable, and their relationship. Violating these assumptions can result in biased predictions or imprecise coefficients, in short cause your model to be less predictive. Well, the good news is that there are numerous extensions have been developed that allow each of these assumptions to be relaxed, and in some cases eliminated entirely, but only if you know what they are and when to use them. More commonly, I see people get stuck and choose to live with a just-fine-fit model without knowing that they can easily fix it by employing a more generalized regression models. Now let&#x2019;s see how powerful regressions can get through a trendy modeling case - project covid admissions based on the vaccinate progress.</p>
<h2 id="Preliminaries"><a href="#Preliminaries" class="headerlink" title="Preliminaries"></a>Preliminaries</h2><figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np</span><br><span class="line"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd</span><br><span class="line"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt</span><br><span class="line"><span class="hljs-keyword">import</span> matplotlib.colors</span><br><span class="line"><span class="hljs-keyword">import</span> seaborn <span class="hljs-keyword">as</span> sns</span><br><span class="line"><span class="hljs-keyword">import</span> statsmodels.api <span class="hljs-keyword">as</span> sm</span><br><span class="line"><span class="hljs-keyword">import</span> sys</span><br><span class="line"><span class="hljs-keyword">import</span> covid_analysis <span class="hljs-keyword">as</span> covid <span class="hljs-comment"># some functions developed for this blog</span></span><br></pre></td></tr></tbody></table></figure>

<p>The data I use includes hospital admissions and vaccinations from the CDC website, and the most up-to-date populations by state from the US Census Bureau.</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data_dir=<span class="hljs-string">&quot;../data&quot;</span></span><br></pre></td></tr></tbody></table></figure>


<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">hospitalizations=pd.read_csv(<span class="hljs-string">f&quot;<span class="hljs-subst">{data_dir}</span>/covid_hospitalizations.csv&quot;</span>,parse_dates=[<span class="hljs-string">&quot;date&quot;</span>])</span><br><span class="line">vaccinations=pd.read_csv(<span class="hljs-string">f&quot;<span class="hljs-subst">{data_dir}</span>/covid_vaccinations.csv&quot;</span>,parse_dates=[<span class="hljs-string">&quot;date&quot;</span>])</span><br><span class="line">population=pd.read_csv(<span class="hljs-string">f&quot;<span class="hljs-subst">{data_dir}</span>/population.csv&quot;</span>)</span><br><span class="line"></span><br><span class="line">event=<span class="hljs-string">&quot;admissions&quot;</span></span><br><span class="line">data=hospitalizations</span><br><span class="line"></span><br><span class="line">data=data.merge(vaccinations,on=[<span class="hljs-string">&quot;date&quot;</span>,<span class="hljs-string">&quot;state&quot;</span>])</span><br><span class="line">data=data.merge(population,on=<span class="hljs-string">&quot;state&quot;</span>)</span><br><span class="line">data[<span class="hljs-string">&quot;vaccinated&quot;</span>]=data[<span class="hljs-string">&quot;vaccinated&quot;</span>]/data[<span class="hljs-string">&quot;population&quot;</span>]</span><br><span class="line">data.head()</span><br></pre></td></tr></tbody></table></figure>


<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>date</th>
      <th>state</th>
      <th>used_beds</th>
      <th>admissions</th>
      <th>vaccinated</th>
      <th>population</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>2021-06-10</td>
      <td>MT</td>
      <td>66.0</td>
      <td>16.0</td>
      <td>0.397597</td>
      <td>1080577</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2021-06-21</td>
      <td>MT</td>
      <td>57.0</td>
      <td>13.0</td>
      <td>0.411614</td>
      <td>1080577</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2021-08-05</td>
      <td>MT</td>
      <td>149.0</td>
      <td>23.0</td>
      <td>0.440153</td>
      <td>1080577</td>
    </tr>
    <tr>
      <th>3</th>
      <td>2021-07-15</td>
      <td>MT</td>
      <td>62.0</td>
      <td>9.0</td>
      <td>0.431722</td>
      <td>1080577</td>
    </tr>
    <tr>
      <th>4</th>
      <td>2021-08-21</td>
      <td>MT</td>
      <td>224.0</td>
      <td>59.0</td>
      <td>0.449196</td>
      <td>1080577</td>
    </tr>
  </tbody>
</table>
</div>



<br>
Now we are good to go. As always, let&#x2019;s first check on our old friend, simple linear regression with one single explanatory variable, as a warm-up. 

<p>Here specifically, I am using weighted least squares rather than ordinary least squares to incorporate the knowledge of state populations. WLS is a generalization of OLS and a specialization of generalized least squares, which can relax the assumptions of constant variance, allowing the variances of the observations to be unequal i.e., heteroscedasticity (btw one of my favorite words in statistics, the pronunciation&#x2019;s fun).</p>
<h2 id="Single-Period-Regression"><a href="#Single-Period-Regression" class="headerlink" title="Single Period Regression"></a>Single Period Regression</h2><h3 id="Model-Definition"><a href="#Model-Definition" class="headerlink" title="Model Definition"></a>Model Definition</h3><p>The weighted linear regression model is defined as:<br>\begin{eqnarray}<br>    \hat{y}_i &amp;= c + \beta x_i \newline<br>    y_i &amp;=\hat{y}_i+ \epsilon_i \newline<br>\end{eqnarray}<br>and we minimize the <strong>weighted least squares error</strong> :<br>\begin{equation}<br>    E_w = \sum_i P_i(y - \hat{y}_i)^2<br>\end{equation}<br>where </p>
<ul>
<li>Each observation $i$ represents a single state.</li>
<li>$\hat{y}_i$ is the predicted probability of hospital admissions <em>per  persons</em> on that state.</li>
<li>$y_i$ is the actual number of events per person observed.</li>
<li>$x_i$ is the percentage of the state population fully vaccinated.</li>
<li>$P_i$ is the population of the  state.</li>
<li>$\epsilon_i$ is a Gaussian uncorrelated noise with constant variance $\sigma$ </li>
<li>the parameters $c$ and $\beta$ will be fitted to the observed data to maximize agreement with the model</li>
</ul>
<p>We first fit the regression model to <strong>single  day</strong> of state hospitalization data.</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">offset=<span class="hljs-number">9</span> <span class="hljs-comment"># if we do not want to look at the last day, subtract here.</span></span><br></pre></td></tr></tbody></table></figure>

<p>Let&#x2019;s take Aug.15,</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">last_date=data[<span class="hljs-string">&quot;date&quot;</span>].max()-pd.offsets.Day(offset)</span><br><span class="line">last_data=data[data[<span class="hljs-string">&quot;date&quot;</span>]==last_date]</span><br></pre></td></tr></tbody></table></figure>


<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">X=sm.add_constant(last_data[<span class="hljs-string">&quot;vaccinated&quot;</span>])</span><br><span class="line">P=last_data[<span class="hljs-string">&quot;population&quot;</span>]</span><br><span class="line">Y=last_data[event]/last_data[<span class="hljs-string">&quot;population&quot;</span>]</span><br></pre></td></tr></tbody></table></figure>

<h3 id="Use-statmodels-for-Weighted-Linear-Regression"><a href="#Use-statmodels-for-Weighted-Linear-Regression" class="headerlink" title="Use statmodels for Weighted Linear Regression"></a>Use statmodels for Weighted Linear Regression</h3><p>Let&#x2019;s first use the  <strong>Weighted Linear Regression</strong> model from <a target="_blank" rel="noopener" href="https://www.statsmodels.org/stable/generated/statsmodels.discrete.discrete_model.Poisson.html">statmodels</a>.<br>The exact same estimation can be done by a different way, which I will show later.</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mod=sm.WLS(Y,X,P)</span><br><span class="line">res=mod.fit()</span><br><span class="line">res.params</span><br></pre></td></tr></tbody></table></figure>




<pre><code>const         0.000127
vaccinated   -0.000190
dtype: float64</code></pre>
<p>So here&#x2019;s our estimation of the coefficients and some statistics that are helpful for diagnosis. It&#x2019;s easy to read that the R-squared is 0.327. But we can also calculate it ourselves.</p>
<p>The R-square is defined as ratio of square loss to target variance,</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">square_error</span>(<span class="hljs-params">y,y_hat,P</span>):</span></span><br><span class="line">    err=y-y_hat</span><br><span class="line">    <span class="hljs-keyword">return</span> np.sum(P*err**<span class="hljs-number">2</span>)</span><br></pre></td></tr></tbody></table></figure>


<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Y_pred=res.predict(X)</span><br><span class="line">Y_bar=(Y*P).sum()/P.sum()</span><br><span class="line">R2=<span class="hljs-number">1</span> - square_error(Y,Y_pred,P)/square_error(Y,Y_bar,P)</span><br><span class="line">R2</span><br></pre></td></tr></tbody></table></figure>




<pre><code>0.3266337795171833</code></pre>
<h3 id="Use-statmodels-GLM-for-Weighted-Linear-Regression"><a href="#Use-statmodels-GLM-for-Weighted-Linear-Regression" class="headerlink" title="Use statmodels GLM for Weighted Linear Regression"></a>Use statmodels GLM for Weighted Linear Regression</h3><p>Exactly the same model can be written in the more general language of <strong>Generalized Linear models</strong> as</p>
<h3 id="Model-Definition-1"><a href="#Model-Definition-1" class="headerlink" title="Model Definition"></a>Model Definition</h3><p>The linear regression  model is defined as:<br>\begin{eqnarray}<br>    &amp;\eta_i &amp;=c + \beta*x_i \newline<br>    \mathbb{E}(y_i|x_i) =&amp; \hat{y}_i &amp;= \eta_i  \newline<br>    &amp;p(y_i|x_i) &amp; = N(\hat{y}_i,\sigma)<br>\end{eqnarray}</p>
<p>The fact that $\eta_i=\hat{y}_i$ means we are using the <strong>identity</strong> link between the Gaussian family and the linear model $\eta$. One should be aware here that linear model is just a specical case of GLM.</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">glm_model=sm.GLM(Y,X,family=sm.families.Gaussian(sm.families.links.identity()),var_weights=P)</span><br><span class="line">glm_res=glm_model.fit()</span><br><span class="line">glm_res.params</span><br></pre></td></tr></tbody></table></figure>




<pre><code>const         0.000127
vaccinated   -0.000190
dtype: float64</code></pre>
<p>Results <strong>are the same</strong>. But with the <code>GLM</code> language we will be able to perform regression many more kinds of variables. </p>
<p>The generalization of square error for a GLM model is called the  <strong>deviance</strong>. This is  the <em>square error</em> the Gaussian Family used  in linear regression. Let&#x2019;s calculate the in-sample R-squared in the GLM way and compare it with the previous.</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">glm_res.deviance, square_error(Y,Y_pred,P)</span><br></pre></td></tr></tbody></table></figure>




<pre><code>(0.12246143037040369, 0.12246143037040369)</code></pre>
<p>The resuls also include the deviance for the <strong>null model</strong> fitted to just a constant</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">glm_res.null_deviance,square_error(Y,Y_bar,P)</span><br></pre></td></tr></tbody></table></figure>




<pre><code>(0.1818645287591, 0.1818645287591)</code></pre>
<p>The ratio defines an $R^2$ value. Not surprisingly, the results are exactly the same. Overall, I admit a 32.7% R-squared model can barely be said to be a good model. But no worries, all can be fixed.</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-number">1</span>-glm_res.deviance/glm_res.null_deviance</span><br></pre></td></tr></tbody></table></figure>




<pre><code>0.3266337795171833</code></pre>
<p>Now, as we have a better understanding of the data and the modeling methodologies, let&#x2019;s move on to model the time series. </p>
<p>The data we have here is time series data, indeed panel data, as time series and cross-sectional data are both special cases of panel data that are in one dimension only. But we will deal with the multi-subjects matter in the next blog(probably should have a spoiler tag oops). Anyways, let&#x2019;s first start with the multi-period regression, here I have trend variables added to the model to represent time.</p>
<h2 id="Multiple-Period-Regression"><a href="#Multiple-Period-Regression" class="headerlink" title="Multiple Period Regression"></a>Multiple Period Regression</h2><h3 id="Model-Definition-2"><a href="#Model-Definition-2" class="headerlink" title="Model Definition"></a>Model Definition</h3><p>\begin{eqnarray}<br>    &amp;\eta_i &amp;=c + \beta_1 x_i  + \beta_2 t_i + \beta_3 t_i^2 \newline<br>    \mathbb{E}(y_i|x_i) =&amp; \hat{y}_i &amp;= \eta_i  \newline<br>    &amp;p(y_i|x_i) &amp; = N(\hat{y_i},\sigma^2)<br>\end{eqnarray}<br>where </p>
<ul>
<li>Each observation $i$ represents a single state at one particular date.</li>
<li>$y_i$ is the number of events on that state.</li>
<li>$\hat{y}_i$ is the <em>predicted</em> number of events on that state.</li>
<li>$x_i$ is the percentage of the state population fully vaccinated on that state.</li>
<li>$t_i$ is the number of days elapsed since the beginning of the training period.</li>
<li>$\sigma^2$ is the level of <em>noise</em> of the observations (the variance of the residuals).</li>
<li>the parameters $c$ and $\beta_1,\beta_2,\beta_3$ will be fitted to the observed data to maximize agreement with the model</li>
</ul>
<p>This can be fitter as a <strong>linear model</strong> where the inputs are $x_i$, $t_i$ and $t_i^2$</p>
<p>First we select the training period:</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">start_train=<span class="hljs-string">&quot;2021-07-15&quot;</span></span><br><span class="line">end_train=<span class="hljs-string">&quot;2021-08-15&quot;</span></span><br><span class="line">train_data=data[(data[<span class="hljs-string">&apos;date&apos;</span>] &gt;= start_train) &amp; (data[<span class="hljs-string">&apos;date&apos;</span>] &lt;= end_train)].copy()</span><br><span class="line">date0=train_data[<span class="hljs-string">&quot;date&quot;</span>].min()</span><br></pre></td></tr></tbody></table></figure>

<p>Then a 7-day testing period:</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">test_period=<span class="hljs-number">7</span> <span class="hljs-comment"># days</span></span><br><span class="line">test_end=pd.Timestamp(end_train)+pd.DateOffset(days=test_period)</span><br><span class="line">test_data=data[(data[<span class="hljs-string">&quot;date&quot;</span>]&gt;end_train) &amp; (data[<span class="hljs-string">&quot;date&quot;</span>]&lt;=test_end)].copy()</span><br></pre></td></tr></tbody></table></figure>

<p>I write a function to generate the <strong>desired matrix</strong>,</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">define_variables</span>(<span class="hljs-params">data,date0</span>):</span></span><br><span class="line">        N=len(data)</span><br><span class="line">        T=(data[<span class="hljs-string">&quot;date&quot;</span>]-date0).dt.days</span><br><span class="line">        X=data[<span class="hljs-string">&quot;vaccinated&quot;</span>].copy().to_frame()</span><br><span class="line">        X[<span class="hljs-string">&quot;T&quot;</span>]=T</span><br><span class="line">        X[<span class="hljs-string">&quot;T2&quot;</span>]=T**<span class="hljs-number">2</span></span><br><span class="line">        P=data[<span class="hljs-string">&quot;population&quot;</span>]</span><br><span class="line">        X[<span class="hljs-string">&quot;const&quot;</span>]=np.ones(N)</span><br><span class="line">        <span class="hljs-keyword">return</span> X,P</span><br></pre></td></tr></tbody></table></figure>


<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">X_train,P_train=covid.define_variables(train_data,date0)</span><br><span class="line">Y_train=train_data[event]/train_data[<span class="hljs-string">&quot;population&quot;</span>]</span><br><span class="line">X_test,P_test=covid.define_variables(test_data,date0)</span><br><span class="line">Y_test=test_data[event]/test_data[<span class="hljs-string">&quot;population&quot;</span>]</span><br></pre></td></tr></tbody></table></figure>

<h3 id="Weighted-Multi-period-Linear-Regression"><a href="#Weighted-Multi-period-Linear-Regression" class="headerlink" title="Weighted Multi-period Linear Regression"></a>Weighted Multi-period Linear Regression</h3><figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mod=sm.WLS(Y_train,X_train,P_train)</span><br><span class="line">res=mod.fit()</span><br><span class="line">res.summary()</span><br></pre></td></tr></tbody></table></figure>




<table class="simpletable">
<caption>WLS Regression Results</caption>
<tbody><tr>
  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th>  <td>   0.373</td> 
</tr>
<tr>
  <th>Model:</th>                   <td>WLS</td>       <th>  Adj. R-squared:    </th>  <td>   0.371</td> 
</tr>
<tr>
  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th>  <td>   322.3</td> 
</tr>
<tr>
  <th>Date:</th>             <td>Tue, 21 Sep 2021</td> <th>  Prob (F-statistic):</th>  <td>3.12e-164</td>
</tr>
<tr>
  <th>Time:</th>                 <td>15:16:26</td>     <th>  Log-Likelihood:    </th>  <td>  15151.</td> 
</tr>
<tr>
  <th>No. Observations:</th>      <td>  1632</td>      <th>  AIC:               </th> <td>-3.029e+04</td>
</tr>
<tr>
  <th>Df Residuals:</th>          <td>  1628</td>      <th>  BIC:               </th> <td>-3.027e+04</td>
</tr>
<tr>
  <th>Df Model:</th>              <td>     3</td>      <th>                     </th>      <td> </td>    
</tr>
<tr>
  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>      <td> </td>    
</tr>
</tbody></table>
<table class="simpletable">
<tbody><tr>
       <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P&gt;|t|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>vaccinated</th> <td>   -0.0002</td> <td> 6.13e-06</td> <td>  -24.647</td> <td> 0.000</td> <td>   -0.000</td> <td>   -0.000</td>
</tr>
<tr>
  <th>T</th>          <td> 1.038e-06</td> <td> 1.82e-07</td> <td>    5.686</td> <td> 0.000</td> <td>  6.8e-07</td> <td>  1.4e-06</td>
</tr>
<tr>
  <th>T2</th>         <td> -1.18e-09</td> <td> 5.69e-09</td> <td>   -0.207</td> <td> 0.836</td> <td>-1.23e-08</td> <td> 9.98e-09</td>
</tr>
<tr>
  <th>const</th>      <td>   8.1e-05</td> <td> 3.17e-06</td> <td>   25.544</td> <td> 0.000</td> <td> 7.48e-05</td> <td> 8.72e-05</td>
</tr>
</tbody></table>
<table class="simpletable">
<tbody><tr>
  <th>Omnibus:</th>       <td>1418.486</td> <th>  Durbin-Watson:     </th> <td>   0.372</td> 
</tr>
<tr>
  <th>Prob(Omnibus):</th>  <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>43359.192</td>
</tr>
<tr>
  <th>Skew:</th>           <td> 4.022</td>  <th>  Prob(JB):          </th> <td>    0.00</td> 
</tr>
<tr>
  <th>Kurtosis:</th>       <td>26.936</td>  <th>  Cond. No.          </th> <td>6.93e+03</td> 
</tr>
</tbody></table><br><br>Notes:<br>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br>[2] The condition number is large, 6.93e+03. This might indicate that there are<br>strong multicollinearity or other numerical problems.

<br>
Calculate the R-squared again, still, not good enough.So let&apos;s take a deep dive and see where can be improved.


<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Y_bar=np.sum(Y_train*P_train)/np.sum(P_train) <span class="hljs-comment"># weighted average</span></span><br><span class="line">Y_pred=res.predict(X_train)</span><br><span class="line"></span><br><span class="line">deviance=square_error(Y_train,Y_pred,P_train)</span><br><span class="line">null_deviance=square_error(Y_train,Y_bar,P_train)</span><br><span class="line">R2=<span class="hljs-number">1</span>-deviance/null_deviance</span><br><span class="line">R2</span><br></pre></td></tr></tbody></table></figure>




<pre><code>0.3726019173009557</code></pre>
<h3 id="Extrapolation-of-Time-trends"><a href="#Extrapolation-of-Time-trends" class="headerlink" title="Extrapolation of Time trends"></a>Extrapolation of Time trends</h3><p>The linear model projects the admission to be increasing steadily if assuming current vaccination rate.</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">covid.plot_time_extrapolation(train_data,res,event,date0)</span><br></pre></td></tr></tbody></table></figure>
<img src="http://yumeng-li.github.io/projectcovid_files/projectcovid_43_0.png">


<h3 id="In-sample-model-Fit"><a href="#In-sample-model-Fit" class="headerlink" title="In-sample model Fit"></a>In-sample model Fit</h3><p>To better evaluate the model fit, we visualize the real observations and model prodictions. Here&#x2019;s some <strong>snapshots</strong> of the fit for a few selected dates on the training period. </p>
<p>Observations are arranged by state and date.</p>
<p>Don&#x2019;t have to say much, you can tell the weighted linear model doesn&#x2019;t fit well as the corelation between vaccinations and admissions looks more like a curve than a straight line.</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">covid.plot_time_facets(train_data,res,event,date0)</span><br></pre></td></tr></tbody></table></figure>

<img src="http://yumeng-li.github.io/projectcovid_files/projectcovid_45_0.png">


<p>We can further plot out the residuals. I choose one date from the dates above, Aug.15 and apply the following function to generate its residual plot.</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">plot_residuals</span>(<span class="hljs-params">data,event,res</span>):</span></span><br><span class="line">    P=data[<span class="hljs-string">&quot;population&quot;</span>]</span><br><span class="line">    <span class="hljs-comment"># Create two subplots and unpack the output array immediately</span></span><br><span class="line">    f, (ax1, ax2) = plt.subplots(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>,sharex=<span class="hljs-literal">True</span>,figsize=(<span class="hljs-number">16</span>,<span class="hljs-number">4</span>))</span><br><span class="line">    norm=matplotlib.colors.LogNorm(vmin=<span class="hljs-number">100</span>_000, vmax=P.max(), clip=<span class="hljs-literal">False</span>)</span><br><span class="line">    m=ax1.scatter(data[<span class="hljs-string">&quot;vaccinated&quot;</span>],data[event]/data[<span class="hljs-string">&quot;population&quot;</span>]*<span class="hljs-number">100</span>_000,</span><br><span class="line">                c=P,norm=norm,cmap=<span class="hljs-string">&quot;Blues&quot;</span>,label=event)</span><br><span class="line">    plt.colorbar(m,label=<span class="hljs-string">&quot;State Population&quot;</span>)</span><br><span class="line">    x=np.linspace(<span class="hljs-number">0.3</span>,<span class="hljs-number">0.75</span>,<span class="hljs-number">201</span>)</span><br><span class="line">    x= pd.DataFrame({<span class="hljs-string">&apos;vaccinated&apos;</span>:x, <span class="hljs-string">&apos;T&apos;</span>:<span class="hljs-number">31</span>, <span class="hljs-string">&apos;T2&apos;</span>:<span class="hljs-number">31</span>*<span class="hljs-number">31</span>})</span><br><span class="line">    x[<span class="hljs-string">&quot;const&quot;</span>]=np.ones(<span class="hljs-number">201</span>)</span><br><span class="line">    y_pred=res.predict(x)</span><br><span class="line">    ax1.plot(x.vaccinated, y_pred*<span class="hljs-number">100</span>_000,<span class="hljs-string">&quot;k--&quot;</span>,label=<span class="hljs-string">&quot;predicted&quot;</span>) </span><br><span class="line">    </span><br><span class="line">    idx = data.index</span><br><span class="line">    ax2.scatter(data[<span class="hljs-string">&quot;vaccinated&quot;</span>], res.resid[idx],</span><br><span class="line">                c=P,norm=norm,cmap=<span class="hljs-string">&quot;Blues&quot;</span>,label=event)</span><br><span class="line">    ax2.set_ybound(lower=<span class="hljs-number">-0.00008</span>,upper=<span class="hljs-number">0.00008</span>)</span><br><span class="line">    ax2.set_title(<span class="hljs-string">&quot;Residuals&quot;</span>)</span><br><span class="line">    ax2.set_xlabel(<span class="hljs-string">&quot;vaccination&quot;</span>)</span><br><span class="line">    ax2.set_ylabel(<span class="hljs-string">&quot;residual&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="hljs-comment"># select outlier state</span></span><br><span class="line">    outlier_idx=np.argpartition(np.array(res.resid[idx]), <span class="hljs-number">-2</span>)[<span class="hljs-number">-2</span>:]</span><br><span class="line">    </span><br><span class="line">    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> outlier_idx:</span><br><span class="line">        outlier=data.iloc[i]</span><br><span class="line">        ax1.annotate(outlier[<span class="hljs-string">&quot;state&quot;</span>],(outlier[<span class="hljs-string">&quot;vaccinated&quot;</span>]+<span class="hljs-number">0.01</span>,outlier[event]/outlier[<span class="hljs-string">&quot;population&quot;</span>]*<span class="hljs-number">100</span>_000))</span><br><span class="line">        ax2.annotate(outlier[<span class="hljs-string">&quot;state&quot;</span>],(outlier[<span class="hljs-string">&quot;vaccinated&quot;</span>]+<span class="hljs-number">0.01</span>,res.resid[idx].iloc[i]))</span><br></pre></td></tr></tbody></table></figure>

<p>We can see that </p>
<ul>
<li>Predicted events can be negative for large vaccination rates, saying we are <strong>over estimating</strong> how fast the admission rate is increasing.</li>
<li>residual variance is negatively correlated with vaccinations rate i.e., smaller vaccination rates have larger magnitude residuals. A <strong>Poisson model</strong> can fix those issues, which will be the topic of our next blog.</li>
<li>Some states like FL and KY look like outliers, indicating that they may have different intercepts than the others, which is not surprising as states are different subjects and each may have unique fundamentals. A <strong>mixed model</strong> would be a good solution for this. </li>
</ul>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">plot_residuals(last_data,event,res)</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>

<img src="http://yumeng-li.github.io/projectcovid_files/projectcovid_49_0.png">


<h2 id="Model-Prediction"><a href="#Model-Prediction" class="headerlink" title="Model Prediction"></a>Model Prediction</h2><p>We will now use the model <strong>without recalibrating</strong> to make predictions about admission rates on new data.</p>
<p>This is <strong>out of sample</strong> evaluation. It is the only way to make sure the model really works.</p>
<h3 id="Out-of-Sample-Model-Fit"><a href="#Out-of-Sample-Model-Fit" class="headerlink" title="Out of Sample Model Fit"></a>Out of Sample Model Fit</h3><p>Besides greater residuals, the out of sample evaluation is basically telling us a same story. </p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">covid.plot_time_facets(test_data,res,event,date0)</span><br></pre></td></tr></tbody></table></figure>

<img src="http://yumeng-li.github.io/projectcovid_files/projectcovid_53_0.png">


<h2 id="Day-of-the-Week-Effects"><a href="#Day-of-the-Week-Effects" class="headerlink" title="Day of the Week Effects"></a>Day of the Week Effects</h2><p>Before we move on to GLM, there&#x2019;s another trick to play with. The data reporting obviously has day of the week effects,</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">aggregate=data.groupby([<span class="hljs-string">&quot;date&quot;</span>])[event].sum()</span><br><span class="line">aggregate=aggregate.to_frame().reset_index()</span><br><span class="line">aggregate.plot(x=<span class="hljs-string">&quot;date&quot;</span>,y=event)</span><br></pre></td></tr></tbody></table></figure>

<img src="http://yumeng-li.github.io/projectcovid_files/projectcovid_55_1.png">


<p>We use a new function to generate the train data, more dummy variables are added to represent days of a week,</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">define_variables2</span>(<span class="hljs-params">data,date0</span>):</span></span><br><span class="line">        N=len(data)</span><br><span class="line">        T=(data[<span class="hljs-string">&quot;date&quot;</span>]-date0).dt.days</span><br><span class="line">        X=data[<span class="hljs-string">&quot;vaccinated&quot;</span>].copy().to_frame()</span><br><span class="line">        X[<span class="hljs-string">&quot;T&quot;</span>]=T</span><br><span class="line">        X[<span class="hljs-string">&quot;T2&quot;</span>]=T**<span class="hljs-number">2</span></span><br><span class="line">        X[<span class="hljs-string">&quot;const&quot;</span>]=np.ones(N)</span><br><span class="line">        Z=pd.get_dummies(data[<span class="hljs-string">&quot;date&quot;</span>].dt.day_name(),drop_first=<span class="hljs-literal">True</span>)</span><br><span class="line">        X=pd.concat([X,Z],axis=<span class="hljs-number">1</span>)</span><br><span class="line">        P=data[<span class="hljs-string">&quot;population&quot;</span>]</span><br><span class="line">        <span class="hljs-keyword">return</span> X,P</span><br></pre></td></tr></tbody></table></figure>


<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X_train,P_train=define_variables2(train_data,date0)</span><br><span class="line">X_train.head()</span><br></pre></td></tr></tbody></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>vaccinated</th>
      <th>T</th>
      <th>T2</th>
      <th>const</th>
      <th>Monday</th>
      <th>Saturday</th>
      <th>Sunday</th>
      <th>Thursday</th>
      <th>Tuesday</th>
      <th>Wednesday</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>2</th>
      <td>0.440153</td>
      <td>21</td>
      <td>441</td>
      <td>1.0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.431722</td>
      <td>0</td>
      <td>0</td>
      <td>1.0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>6</th>
      <td>0.435852</td>
      <td>10</td>
      <td>100</td>
      <td>1.0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>8</th>
      <td>0.441906</td>
      <td>24</td>
      <td>576</td>
      <td>1.0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>10</th>
      <td>0.437877</td>
      <td>15</td>
      <td>225</td>
      <td>1.0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>




<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mod=sm.WLS(Y_train,X_train,P_train)</span><br><span class="line">res=mod.fit()</span><br><span class="line">res.params</span><br></pre></td></tr></tbody></table></figure>




<pre><code>vaccinated   -1.510900e-04
T             1.028112e-06
T2           -6.639196e-10
const         8.186223e-05
Monday       -2.235516e-06
Saturday     -8.318830e-07
Sunday       -3.189080e-06
Thursday     -2.233662e-08
Tuesday       3.232847e-07
Wednesday    -2.672040e-07
dtype: float64</code></pre>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">res.rsquared</span><br></pre></td></tr></tbody></table></figure>




<pre><code>0.37572844250241666</code></pre>
<p>$R^2$ is 0.376, higher than the 0.373 we previously have. Day of the week provides a <strong>small improvement</strong> on $R^2$ compared to original model.</p>
</body></html>
    
    </div>
    
    
</article>




    
        <article class="article content gallery" itemscope itemprop="blogPost">
    <h1 class="article-title is-size-3 is-size-4-mobile" itemprop="name">
        
            <a href="/2020/08/02/word2vec-on-bbg/" itemprop="url">Training Word Embedding on Bloomberg News</a>
        
    </h1>
    <div class="article-meta columns is-variable is-1 is-multiline is-mobile is-size-7-mobile">
        <span class="column is-narrow">
            <time datetime="2020-08-03T02:52:13.685Z" itemprop="datePublished">Aug 2 2020</time>
        </span>
        
        <span class="column is-narrow article-category">
            <i class="far fa-folder"></i>
            <a class="article-category-link" href="/categories/NLP/">NLP</a>
        </span>
        
        
        <span class="column is-narrow">
            
            
            9 minutes read (About 1350 words)
        </span>
        
    </div>
    <div class="article-entry is-size-6-mobile" itemprop="articleBody">
    
        <html><head></head><body><p>While working from home (stuck at home indeed), I get a bit more time, so revisited some old deep learning notes last month. Would it be awesome if I can come up with a project to play with? As we all know knowledge would go rusty if lack of use!</p>
<p>Initially, I was thinking about a machine translator using Attention Models, because I was once asked in an interview. The fund showed a particular interest in developing such a tool for the purpose of distributing research reports to non-English-speaking countries. But I was completely set back by the data sets asking for literally thousands of dollars&#x2026; While if labeled dataset is that expensive, why not run some unsurprised learning? Training word2vec doesn&#x2019;t sound like a bad idea. I&#x2019;ve been writing some views on FICC (fixed income, currencies, and commodities) on my other <a target="_blank" rel="noopener" href="https://yumengsblog.com/">non-geeky-at-all blog</a>. Won&#x2019;t it be fun to run word embeddings on Bloomberg news, see if people talk about the right things when they analyze inflation and gold prices?</p>
<p>So here we go, rock and roll!</p>
<p>First it&#x2019;s a bit introduction to word embedding. The term refers to a set of techniques in natural language processing that can map words into high dimensional vectors of real numbers. The vectors/embeddings can be seen as featurized representations of words, which preserve semantic properties, hence are commonly used as inputs in NLP tasks.</p>
<p>Here is an example, </p>
<img src="http://yumeng-li.github.io/pics/embeddingVectors.png" alt="embedding vectors" width="450">

<p>While in real cases, the dimension of vectors is usually a lot higher.</p>
<p>We will use a tool called word2vec to convert words into embeddings, <a target="_blank" rel="noopener" href="https://radimrehurek.com/gensim/">gensim</a> has it implemented in a very easy-to-use way. </p>
<blockquote>
<p>Word2vec is a technique for natural language processing. The word2vec algorithm uses a neural network model to learn word associations from a large corpus of text. Once trained, such a model can detect synonymous words or suggest additional words for a partial sentence.</p>
</blockquote>
<p>More explanation on word2vec can be found <a target="_blank" rel="noopener" href="https://www.wikiwand.com/en/Word2vec">here</a>.</p>
<h2 id="Creating-Corpus"><a href="#Creating-Corpus" class="headerlink" title="Creating Corpus"></a>Creating Corpus</h2><h3 id="Scarping-Addresses-of-Bloomberg-Articles"><a href="#Scarping-Addresses-of-Bloomberg-Articles" class="headerlink" title="Scarping Addresses of Bloomberg Articles"></a>Scarping Addresses of Bloomberg Articles</h3><p>I would like to run the embeddings only on the most recent posts, so first need to grab their urls through a search page.</p>
<p>Import the libraries,</p>
<figure class="highlight bash hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">import re</span><br><span class="line">import requests</span><br><span class="line">import string</span><br><span class="line">import numpy as np</span><br><span class="line">import gensim.models.word2vec as word2vec</span><br><span class="line">from bs4 import BeautifulSoup</span><br><span class="line">from time import sleep</span><br></pre></td></tr></tbody></table></figure>
<p>A function that can scrape the urls of articles that show up in the top p pages related to a specific topic,</p>
<figure class="highlight bash hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">def scrape_bloomberg_urls(subject, maxPage, headers):</span><br><span class="line">    urls = []</span><br><span class="line">    <span class="hljs-keyword">for</span> p <span class="hljs-keyword">in</span> range(1,maxPage):</span><br><span class="line">        searchUrl = <span class="hljs-string">&apos;https://www.bloomberg.com/search?query=&apos;</span> + subject + <span class="hljs-string">&apos;&amp;sort=relevance:desc&apos;</span> + <span class="hljs-string">&apos;&amp;page=&apos;</span> + str(p)</span><br><span class="line">        response = requests.get(searchUrl, headers=headers)</span><br><span class="line">        soup = BeautifulSoup(response.content, <span class="hljs-string">&apos;html.parser&apos;</span>)</span><br><span class="line">        regex = re.compile(<span class="hljs-string">&apos;.*headline.*&apos;</span>)</span><br><span class="line">        <span class="hljs-keyword">for</span> tag <span class="hljs-keyword">in</span> soup.findAll(<span class="hljs-string">&apos;a&apos;</span>, {<span class="hljs-string">&quot;class&quot;</span> : regex}, href = True):</span><br><span class="line">            href = tag.attrs[<span class="hljs-string">&apos;href&apos;</span>]</span><br><span class="line">            <span class="hljs-keyword">if</span> <span class="hljs-string">&apos;/news/articles/&apos;</span> <span class="hljs-keyword">in</span> href and href not <span class="hljs-keyword">in</span> urls:</span><br><span class="line">                urls.append(href)            </span><br><span class="line">    <span class="hljs-built_in">return</span> urls</span><br></pre></td></tr></tbody></table></figure>

<p>Set up headers to pass to the request, websites have gone crazy about blocking robots,</p>
<figure class="highlight bash hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">headers = {</span><br><span class="line">    <span class="hljs-string">&apos;user-agent&apos;</span>: <span class="hljs-string">&apos;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36&apos;</span>,</span><br><span class="line">    <span class="hljs-string">&apos;referrer&apos;</span>: <span class="hljs-string">&apos;https://google.com&apos;</span>,</span><br><span class="line">    <span class="hljs-string">&apos;Accept&apos;</span>: <span class="hljs-string">&apos;text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8&apos;</span>,</span><br><span class="line">    <span class="hljs-string">&apos;Accept-Encoding&apos;</span>: <span class="hljs-string">&apos;gzip, deflate, br&apos;</span>,</span><br><span class="line">    <span class="hljs-string">&apos;Accept-Language&apos;</span>: <span class="hljs-string">&apos;en-US,en;q=0.9&apos;</span>,</span><br><span class="line">    <span class="hljs-string">&apos;Pragma&apos;</span>: <span class="hljs-string">&apos;no-cache&apos;</span>}</span><br></pre></td></tr></tbody></table></figure>

<p>We get 155 articles here, not a large dataset so it may give us weird results, but should be good enough for an exercise,</p>
<figure class="highlight bash hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">In: len(urls)</span><br><span class="line">Out: 155</span><br></pre></td></tr></tbody></table></figure>

<figure class="highlight bash hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">In: urls[:5]</span><br><span class="line">Out: </span><br><span class="line">[<span class="hljs-string">&apos;https://www.bloomberg.com/news/articles/2020-08-07/inflation-trend-a-friend-in-real-yield-contest-seasia-rates&apos;</span>,</span><br><span class="line"> <span class="hljs-string">&apos;https://www.bloomberg.com/news/articles/2020-08-06/blackrock-joins-crescendo-of-inflation-warnings-amid-virus-fight&apos;</span>,</span><br><span class="line"> <span class="hljs-string">&apos;https://www.bloomberg.com/news/articles/2020-08-06/china-inflation-rate-headed-for-0-on-cooling-food-prices-chart&apos;</span>,</span><br><span class="line"> <span class="hljs-string">&apos;https://www.bloomberg.com/news/articles/2020-08-06/inflation-binds-czechs-after-fast-rate-cuts-decision-day-guide&apos;</span>,</span><br><span class="line"> <span class="hljs-string">&apos;https://www.bloomberg.com/news/articles/2020-08-07/jpmorgan-rejects-threat-to-dollar-status-flagged-by-goldman&apos;</span>]</span><br></pre></td></tr></tbody></table></figure>
<h3 id="Parsing-Articles"><a href="#Parsing-Articles" class="headerlink" title="Parsing Articles"></a>Parsing Articles</h3><p>This step is to parse articles through the urls and save the corpus into a local text file.</p>
<figure class="highlight bash hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">def parse_article(urls, headers):</span><br><span class="line">    corpus = []</span><br><span class="line">    <span class="hljs-keyword">for</span> url <span class="hljs-keyword">in</span> urls:</span><br><span class="line">        response = requests.get(url, headers=headers)</span><br><span class="line">        soup = BeautifulSoup(response.content,<span class="hljs-string">&apos;lxml&apos;</span>)</span><br><span class="line">        <span class="hljs-keyword">for</span> tag <span class="hljs-keyword">in</span> soup.find_all(<span class="hljs-string">&apos;p&apos;</span>):</span><br><span class="line">            content = tag.get_text()</span><br><span class="line">            cleanedContent = content.tra nslate(str.maketrans(<span class="hljs-string">&apos;&apos;</span>, <span class="hljs-string">&apos;&apos;</span>, string.punctuation)).lower()</span><br><span class="line">            corpus.append(cleanedContent)</span><br><span class="line">        seconds = np.random.randint(low=5, high=20)</span><br><span class="line">        sleep(seconds)</span><br><span class="line">    <span class="hljs-built_in">return</span> corpus</span><br></pre></td></tr></tbody></table></figure>
<p>A function serves as a writer,</p>
<figure class="highlight bash hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">def write_to_txt(outdir, subject, contentList):</span><br><span class="line">    outputfile = f<span class="hljs-string">&apos;{outdir}/{subject}.txt&apos;</span></span><br><span class="line">    with open(outputfile, <span class="hljs-string">&apos;a&apos;</span>) as file:</span><br><span class="line">        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> contentList:</span><br><span class="line">            file.write(f<span class="hljs-string">&apos;{i}\n&apos;</span>)</span><br></pre></td></tr></tbody></table></figure>

<p>Run the functions,</p>
<figure class="highlight bash hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">corpus = parse_article(urls, headers)</span><br><span class="line">write_to_txt(<span class="hljs-string">&apos;Documents/Blog/&apos;</span>, <span class="hljs-string">&apos;corpus&apos;</span>, corpus)</span><br></pre></td></tr></tbody></table></figure>

<h2 id="Training-the-Model"><a href="#Training-the-Model" class="headerlink" title="Training the Model"></a>Training the Model</h2><p>Now that we have created the corpus, let&#x2019;s train the model using word2vec from gensim.</p>
<p>Gensim has a cool function <em>LineSentence</em> that can directly read sentences from a text file with one sentence a line. Now you probably get why I had the corpus saved this way.</p>
<figure class="highlight bash hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sentences = word2vec.LineSentence(<span class="hljs-string">&apos;corpus.txt&apos;</span>)</span><br><span class="line">model = word2vec.Word2Vec(sentences, min_count=10, workers=8, iter=500, window=15, size=300, negative=50)</span><br></pre></td></tr></tbody></table></figure>
<p>The model I choose is using the <a target="_blank" rel="noopener" href="https://www.wikiwand.com/en/N-gram">Skip-Gram</a> algorithm with Negative Sampling. Hyper-parameters above are what I find work well, see below for their definitions, just in case you would like to tune them yourself,</p>
<blockquote>
<ul>
<li>size &#x2013; Dimensionality of the word vectors.</li>
<li>window &#x2013; Maximum distance between the current and predicted word within a sentence.</li>
<li>min_count &#x2013; Ignores all words with total frequency lower than this.</li>
<li>workers &#x2013; Use these many worker threads to train the model (=faster training with multicore machines).</li>
<li>negative  &#x2013; If &gt; 0, negative sampling will be used, the int for negative specifies how many &#x201C;noise words&#x201D; should be drawn (usually between 5-20). If set to 0, no negative sampling is used.</li>
</ul>
</blockquote>
<h2 id="Checking-Out-the-Results"><a href="#Checking-Out-the-Results" class="headerlink" title="Checking Out the Results"></a>Checking Out the Results</h2><p>The goal of this exercise to find out what people talk about when they analyze inflation expectation (basically what currently drives the stock market, TIPS and metals to roar higher and higher) and gold (my favorite and probably the most promising asset in the next 10 years).</p>
<p>Word2vev model can find words that have been used in a similar context with the word we care about. Here we employ a method <em>most_similar</em> to reveal them.</p>
<p>Let&#x2019;s first check out the words people usually mention when they talk about inflation. I print out the top 20 words. The meaningful ones are,</p>
<ul>
<li>&#x201C;target&#x201D;, &#x201C;bank&#x2019;s&#x201D;, &#x201C;bank&#x201D; - of course central banks target on inflation and their monetary policies shape expectations.</li>
<li>&#x201C;nominal&#x201D;, &#x201C;negative&#x201D;, &#x201C;yields&#x201D; - words usually describing rates, make sense.</li>
<li>&#x201C;food&#x201D; - measure of inflation.</li>
<li>&#x201C;employment&#x201D; - make a lot sense if you remember the Phillips curve.</li>
</ul>
<p>The results are actually very good, authors did understand and explain inflations well.</p>
<figure class="highlight bash hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">In: model.wv.most_similar(<span class="hljs-string">&apos;inflation&apos;</span>,topn=20)</span><br><span class="line">Out:</span><br><span class="line">[(<span class="hljs-string">&apos;target&apos;</span>, 0.25702178478240967),</span><br><span class="line"> (<span class="hljs-string">&apos;strip&apos;</span>, 0.18441016972064972),</span><br><span class="line"> (<span class="hljs-string">&apos;securities&apos;</span>, 0.18246109783649445),</span><br><span class="line"> (<span class="hljs-string">&apos;full&apos;</span>, 0.16774789988994598),</span><br><span class="line"> (<span class="hljs-string">&apos;nominal&apos;</span>, 0.16606125235557556),</span><br><span class="line"> (<span class="hljs-string">&apos;bank&#x2019;s&apos;</span>, 0.162990003824234),</span><br><span class="line"> (<span class="hljs-string">&apos;yields&apos;</span>, 0.15935908257961273),</span><br><span class="line"> (<span class="hljs-string">&apos;negative&apos;</span>, 0.15806841850280762),</span><br><span class="line"> (<span class="hljs-string">&apos;remain&apos;</span>, 0.15371079742908478),</span><br><span class="line"> (<span class="hljs-string">&apos;levels&apos;</span>, 0.14426037669181824),</span><br><span class="line"> (<span class="hljs-string">&apos;well&apos;</span>, 0.1438026875257492),</span><br><span class="line"> (<span class="hljs-string">&apos;current&apos;</span>, 0.13828279078006744),</span><br><span class="line"> (<span class="hljs-string">&apos;attractive&apos;</span>, 0.13690069317817688),</span><br><span class="line"> (<span class="hljs-string">&apos;showed&apos;</span>, 0.1277044713497162),</span><br><span class="line"> (<span class="hljs-string">&apos;bank&apos;</span>, 0.12733221054077148),</span><br><span class="line"> (<span class="hljs-string">&apos;food&apos;</span>, 0.12377716600894928),</span><br><span class="line"> (<span class="hljs-string">&apos;similar&apos;</span>, 0.11955425888299942),</span><br><span class="line"> (<span class="hljs-string">&apos;the&apos;</span>, 0.11911500245332718),</span><br><span class="line"> (<span class="hljs-string">&apos;employment&apos;</span>, 0.11866464465856552),</span><br><span class="line"> (<span class="hljs-string">&apos;keep&apos;</span>, 0.11818670481443405)]</span><br></pre></td></tr></tbody></table></figure>

<p>Then let&#x2019;s try it for gold. It&#x2019;s good to see silver rank top, the two metals do hold a strong correlation despite the natures of them are very different, gold generally behaves as a bond of real rates, while silver should be viewed more as a commodity. (People make mistakes on this, see <a target="_blank" rel="noopener" href="https://www.investopedia.com/articles/optioninvestor/09/silver-thursday-hunt-brothers.asp">the story of Hunt Brothers</a>)</p>
<p>Other words are just descriptive, focusing on telling readers what&#x2019;s going on in the markets. While this is not helpful, Bloomberg! People want to know why, you may want to talk more about real rates, inflation expectations, debt and global growth in the future.</p>
<figure class="highlight bash hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">In: model.wv.most_similar(<span class="hljs-string">&apos;gold&apos;</span>,topn=30)</span><br><span class="line">Out:</span><br><span class="line">[(<span class="hljs-string">&apos;gold&#x2019;s&apos;</span>, 0.26054084300994873),</span><br><span class="line"> (<span class="hljs-string">&apos;silver&apos;</span>, 0.25277888774871826),</span><br><span class="line"> (<span class="hljs-string">&apos;ounce&apos;</span>, 0.2523342967033386),</span><br><span class="line"> (<span class="hljs-string">&apos;bullion&apos;</span>, 0.2294640988111496),</span><br><span class="line"> (<span class="hljs-string">&apos;2300&apos;</span>, 0.22592982649803162),</span><br><span class="line"> (<span class="hljs-string">&apos;spot&apos;</span>, 0.2171008288860321),</span><br><span class="line"> (<span class="hljs-string">&apos;climbing&apos;</span>, 0.19733953475952148),</span><br><span class="line"> (<span class="hljs-string">&apos;metal&apos;</span>, 0.1939847469329834),</span><br><span class="line"> (<span class="hljs-string">&apos;comex&apos;</span>, 0.19371576607227325),</span><br><span class="line"> (<span class="hljs-string">&apos;rally&apos;</span>, 0.18643531203269958),</span><br><span class="line"> (<span class="hljs-string">&apos;exchangetraded&apos;</span>, 0.1859540343284607),</span><br><span class="line"> (<span class="hljs-string">&apos;a&apos;</span>, 0.18539577722549438),</span><br><span class="line"> (<span class="hljs-string">&apos;delivery&apos;</span>, 0.17595867812633514),</span><br><span class="line"> (<span class="hljs-string">&apos;reaching&apos;</span>, 0.1702871322631836),</span><br><span class="line"> (<span class="hljs-string">&apos;strip&apos;</span>, 0.16905872523784637),</span><br><span class="line"> (<span class="hljs-string">&apos;posted&apos;</span>, 0.16247007250785828),</span><br><span class="line"> (<span class="hljs-string">&apos;lows&apos;</span>, 0.16047437489032745),</span><br><span class="line"> (<span class="hljs-string">&apos;as&apos;</span>, 0.1585429608821869),</span><br><span class="line"> (<span class="hljs-string">&apos;analysis&apos;</span>, 0.1577225774526596),</span><br><span class="line"> (<span class="hljs-string">&apos;2011&apos;</span>, 0.15711694955825806),</span><br><span class="line"> (<span class="hljs-string">&apos;precious&apos;</span>, 0.15656355023384094),</span><br><span class="line"> (<span class="hljs-string">&apos;threat&apos;</span>, 0.1542907953262329),</span><br><span class="line"> (<span class="hljs-string">&apos;more&apos;</span>, 0.1541929841041565),</span><br><span class="line"> (<span class="hljs-string">&apos;drive&apos;</span>, 0.15310749411582947),</span><br><span class="line"> (<span class="hljs-string">&apos;every&apos;</span>, 0.1524747610092163),</span><br><span class="line"> (<span class="hljs-string">&apos;analyst&apos;</span>, 0.1517343521118164),</span><br><span class="line"> (<span class="hljs-string">&apos;managing&apos;</span>, 0.14977654814720154),</span><br><span class="line"> (<span class="hljs-string">&apos;price&apos;</span>, 0.1497490406036377),</span><br><span class="line"> (<span class="hljs-string">&apos;amounts&apos;</span>, 0.14969849586486816),</span><br><span class="line"> (<span class="hljs-string">&apos;backed&apos;</span>, 0.1450338065624237)]</span><br></pre></td></tr></tbody></table></figure></body></html>
    
    </div>
    
    
</article>




    
    
    </div>
</section>
    <footer class="footer">
    <div class="container">
        <div class="columns content">
            <div class="column is-narrow has-text-centered">
                &copy; 2021 Yumeng Li&nbsp;
                Powered by <a href="http://hexo.io/" target="_blank">Hexo</a> & <a
                        target="_blank" rel="noopener" href="http://github.com/ppoffice/hexo-theme-minos">Minos</a>
            </div>
            <div class="column is-hidden-mobile"></div>

            
            <div class="column is-narrow">
                <div class="columns is-mobile is-multiline is-centered">
                
                    
                <a class="column is-narrow has-text-black" title="GitHub" target="_blank" rel="noopener" href="https://github.com/ppoffice/hexo-theme-minos">
                    
                    GitHub
                    
                </a>
                
                </div>
            </div>
            
            
        </div>
    </div>
</footer>
    <script src="//cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/moment.js/2.22.2/moment-with-locales.min.js"></script>

<!-- test if the browser is outdated -->
<div id="outdated">
    <h6>Your browser is out-of-date!</h6>
    <p>Update your browser to view this website correctly. <a id="btnUpdateBrowser" target="_blank" rel="noopener" href="http://outdatedbrowser.com/">Update my browser now </a></p>
    <p class="last"><a href="#" id="btnCloseUpdateBrowser" title="Close">&times;</a></p>
</div>
<script src="//cdnjs.cloudflare.com/ajax/libs/outdated-browser/1.1.5/outdatedbrowser.min.js"></script>
<script>
    $(document).ready(function () {
        // plugin function, place inside DOM ready function
        outdatedBrowser({
            bgColor: '#f25648',
            color: '#ffffff',
            lowerThan: 'flex'
        })
    });
</script>

<script>
    window.FontAwesomeConfig = {
        searchPseudoElements: true
    }
    moment.locale("en-AU");
</script>


    
    
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<script>
    MathJax.Hub.Config({
        "HTML-CSS": {
            matchFontHeight: false
        },
        SVG: {
            matchFontHeight: false
        },
        CommonHTML: {
            matchFontHeight: false
        },
        tex2jax: {
            inlineMath: [
                ['$','$'],
                ['\\(','\\)']
            ]
        }
    });
</script>

    
    
    
    
<script src="//cdnjs.cloudflare.com/ajax/libs/lightgallery/1.6.8/js/lightgallery-all.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/justifiedGallery/3.6.5/js/jquery.justifiedGallery.min.js"></script>
<script>
    (function ($) {
        $(document).ready(function () {
            if (typeof($.fn.lightGallery) === 'function') {
                $('.article.gallery').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof($.fn.justifiedGallery) === 'function') {
                $('.justified-gallery').justifiedGallery();
            }
        });
    })(jQuery);
</script>

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.0/clipboard.min.js"></script>
    <style>
        .hljs {
            position: relative;
        }

        .hljs .clipboard-btn {
            float: right;
            color: #9a9a9a;
            background: none;
            border: none;
            cursor: pointer;
        }

        .hljs .clipboard-btn:hover {
          color: #8a8a8a;
        }

        .hljs > .clipboard-btn {
            display: none;
            position: absolute;
            right: 4px;
            top: 4px;
        }

        .hljs:hover > .clipboard-btn {
            display: inline;
        }

        .hljs > figcaption > .clipboard-btn {
            margin-right: 4px;
        }
    </style>
    <script>
      $(document).ready(function () {
        $('figure.hljs').each(function(i, figure) {
          var codeId = 'code-' + i;
          var code = figure.querySelector('.code');
          var copyButton = $('<button>Copy <i class="far fa-clipboard"></i></button>');
          code.id = codeId;
          copyButton.addClass('clipboard-btn');
          copyButton.attr('data-clipboard-target-id', codeId);

          var figcaption = figure.querySelector('figcaption');

          if (figcaption) {
            figcaption.append(copyButton[0]);
          } else {
            figure.prepend(copyButton[0]);
          }
        })

        var clipboard = new ClipboardJS('.clipboard-btn', {
          target: function(trigger) {
            return document.getElementById(trigger.getAttribute('data-clipboard-target-id'));
          }
        });
        clipboard.on('success', function(e) {
          e.clearSelection();
        })
      })
    </script>

    
    

    



<script src="/js/script.js"></script>


    
    <div class="searchbox ins-search">
    <div class="searchbox-mask"></div>
    <div class="searchbox-container ins-search-container">
        <div class="searchbox-input-wrapper">
            <input type="text" class="searchbox-input ins-search-input" placeholder="Type something..." />
            <span class="searchbox-close ins-close ins-selectable"><i class="fa fa-times-circle"></i></span>
        </div>
        <div class="searchbox-result-wrapper ins-section-wrapper">
            <div class="ins-section-container"></div>
        </div>
    </div>
</div>
<script>
    (function (window) {
        var INSIGHT_CONFIG = {
            TRANSLATION: {
                POSTS: 'Posts',
                PAGES: 'Pages',
                CATEGORIES: 'Categories',
                TAGS: 'Tags',
                UNTITLED: '(Untitled)',
            },
            CONTENT_URL: '/content.json',
        };
        window.INSIGHT_CONFIG = INSIGHT_CONFIG;
    })(window);
</script>

<script src="/js/insight.js"></script>

    
</body>
</html>