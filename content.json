{"pages":[{"title":"categories","text":"","link":"/categories/index.html"}],"posts":[{"title":"Training Word Embedding on Bloomberg News","text":"While working from home (stuck at home indeed), I get more time, so last month revisited some deep learning courses, and thought it would be awesome if I can come up with some projects to play with, as we all know knowledge would go rusty if lack of use! Initially, I was thinking about a machine translator using Attention Models, because I was once asked about it in an interview. The fund showed a particular interest in developing such a tool for purpose of distributing their research reports to non-English-speaking countries. However, the thousands-of-dollar worth dataset set me back. While if labeled dataset is hard to get, why not run some unsurprised learning? Training word2vec is not a bad idea as I’ve been writing views on FICC (fixed income, currencies, and commodities) on my other non-geeky-at-all blog. Won’t it be fun to run word embeddings on Bloomberg views, see if people talk about the right things when they analyze inflation and gold prices? Now let’s rock! First it’s a bit introduction of word embedding. The term refers to a set of techniques in natural language processing that can map words into high dimensional vectors of real numbers. The vectors or embeddings can be seen as featurized representations of words, which preserve semantic properties and hence are commonly used as inputs in NPL tasks. Here is an example, While in real cases, the dimension of vectors is usually a lot higher. We will use a tool called word2vec to convert words into embeddings, gensim has it implemented in an easy-to-use way. Wiki Word2vec is a technique for natural language processing. The word2vec algorithm uses a neural network model to learn word associations from a large corpus of text. Once trained, such a model can detect synonymous words or suggest additional words for a partial sentence. More explanation on word2vec can be found here Create CorpusScarping Links to Bloomberg ArticlesI would like to run the embeddings only on the most recent articles, so first need to grab their urls through a search page. Import the libraries, 1234567import reimport requestsfrom bs4 import BeautifulSoupimport stringimport gensim.models.word2vec as word2vecfrom time import sleepimport numpy as np A function that can scrape the urls of articles that show up in the top p pages related to a specific topic, 123456789101112def scrape_bloomberg_urls(subject, maxPage, headers): urls = [] for p in range(1,maxPage): searchUrl = &apos;https://www.bloomberg.com/search?query=&apos; + subject + &apos;&amp;sort=relevance:desc&apos; + &apos;&amp;page=&apos; + str(p) response = requests.get(searchUrl, headers=headers) soup = BeautifulSoup(response.content, &apos;html.parser&apos;) regex = re.compile(&apos;.*headline.*&apos;) for tag in soup.findAll(&apos;a&apos;, {&quot;class&quot; : regex}, href = True): href = tag.attrs[&apos;href&apos;] if &apos;/news/articles/&apos; in href and href not in urls: urls.append(href) return urls Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","link":"/2020/08/02/hello-world/"}],"tags":[],"categories":[{"name":"NLP","slug":"NLP","link":"/categories/NLP/"}]}