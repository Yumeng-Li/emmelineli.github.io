{"pages":[{"title":"categories","text":"","link":"/categories/index.html"}],"posts":[{"title":"Training Word Embedding on Bloomberg News","text":"While working from home (stuck at home indeed), I get a bit more time, so revisited some old deep learning notes last month. Would it be awesome if I can come up with a project to play with? As we all know knowledge would go rusty if lack of use! Initially, I was thinking about a machine translator using Attention Models, because I was once asked in an interview. The fund showed a particular interest in developing such a tool for the purpose of distributing research reports to non-English-speaking countries. But I was completely set back by the data sets asking for literally thousands of dollars… While if labeled dataset is that expensive, why not run some unsurprised learning? Training word2vec doesn’t sound like a bad idea. I’ve been writing some views on FICC (fixed income, currencies, and commodities) on my other non-geeky-at-all blog. Won’t it be fun to run word embeddings on Bloomberg news, see if people talk about the right things when they analyze inflation and gold prices? So here we go, rock and roll! First it’s a bit introduction to word embedding. The term refers to a set of techniques in natural language processing that can map words into high dimensional vectors of real numbers. The vectors/embeddings can be seen as featurized representations of words, which preserve semantic properties, hence are commonly used as inputs in NLP tasks. Here is an example, While in real cases, the dimension of vectors is usually a lot higher. We will use a tool called word2vec to convert words into embeddings, gensim has it implemented in a very easy-to-use way. Word2vec is a technique for natural language processing. The word2vec algorithm uses a neural network model to learn word associations from a large corpus of text. Once trained, such a model can detect synonymous words or suggest additional words for a partial sentence. More explanation on word2vec can be found here. Creating CorpusScarping Addresses of Bloomberg ArticlesI would like to run the embeddings only on the most recent posts, so first need to grab their urls through a search page. Import the libraries, 1234567import reimport requestsimport stringimport numpy as npimport gensim.models.word2vec as word2vecfrom bs4 import BeautifulSoupfrom time import sleep A function that can scrape the urls of articles that show up in the top p pages related to a specific topic, 123456789101112def scrape_bloomberg_urls(subject, maxPage, headers): urls = [] for p in range(1,maxPage): searchUrl = &apos;https://www.bloomberg.com/search?query=&apos; + subject + &apos;&amp;sort=relevance:desc&apos; + &apos;&amp;page=&apos; + str(p) response = requests.get(searchUrl, headers=headers) soup = BeautifulSoup(response.content, &apos;html.parser&apos;) regex = re.compile(&apos;.*headline.*&apos;) for tag in soup.findAll(&apos;a&apos;, {&quot;class&quot; : regex}, href = True): href = tag.attrs[&apos;href&apos;] if &apos;/news/articles/&apos; in href and href not in urls: urls.append(href) return urls Set up headers to pass to the request, websites have gone crazy about blocking robots, 1234567headers = { &apos;user-agent&apos;: &apos;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36&apos;, &apos;referrer&apos;: &apos;https://google.com&apos;, &apos;Accept&apos;: &apos;text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8&apos;, &apos;Accept-Encoding&apos;: &apos;gzip, deflate, br&apos;, &apos;Accept-Language&apos;: &apos;en-US,en;q=0.9&apos;, &apos;Pragma&apos;: &apos;no-cache&apos;} We get 155 articles here, not a large dataset so it may give us weird results, but should be good enough for an exercise, 12In: len(urls)Out: 155 1234567In: urls[:5]Out: [&apos;https://www.bloomberg.com/news/articles/2020-08-07/inflation-trend-a-friend-in-real-yield-contest-seasia-rates&apos;, &apos;https://www.bloomberg.com/news/articles/2020-08-06/blackrock-joins-crescendo-of-inflation-warnings-amid-virus-fight&apos;, &apos;https://www.bloomberg.com/news/articles/2020-08-06/china-inflation-rate-headed-for-0-on-cooling-food-prices-chart&apos;, &apos;https://www.bloomberg.com/news/articles/2020-08-06/inflation-binds-czechs-after-fast-rate-cuts-decision-day-guide&apos;, &apos;https://www.bloomberg.com/news/articles/2020-08-07/jpmorgan-rejects-threat-to-dollar-status-flagged-by-goldman&apos;] Parsing ArticlesThis step is to parse articles through the urls and save the corpus into a local text file. 123456789101112def parse_article(urls, headers): corpus = [] for url in urls: response = requests.get(url, headers=headers) soup = BeautifulSoup(response.content,&apos;lxml&apos;) for tag in soup.find_all(&apos;p&apos;): content = tag.get_text() cleanedContent = content.tra nslate(str.maketrans(&apos;&apos;, &apos;&apos;, string.punctuation)).lower() corpus.append(cleanedContent) seconds = np.random.randint(low=5, high=20) sleep(seconds) return corpus A function serves as a writer, 12345def write_to_txt(outdir, subject, contentList): outputfile = f&apos;{outdir}/{subject}.txt&apos; with open(outputfile, &apos;a&apos;) as file: for i in contentList: file.write(f&apos;{i}\\n&apos;) Run the functions, 12corpus = parse_article(urls, headers)write_to_txt(&apos;Documents/Blog/&apos;, &apos;corpus&apos;, corpus) Training the ModelNow that we have created the corpus, let’s train the model using word2vec from gensim. Gensim has a cool function LineSentence that can directly read sentences from a text file with one sentence a line. Now you probably get why I had the corpus saved this way. 12sentences = word2vec.LineSentence(&apos;corpus.txt&apos;)model = word2vec.Word2Vec(sentences, min_count=10, workers=8, iter=500, window=15, size=300, negative=50) The model I choose is using the Skip-Gram algorithm with Negative Sampling. Hyper-parameters above are what I find work well, see below for their definitions, just in case you would like to tune them yourself, size – Dimensionality of the word vectors. window – Maximum distance between the current and predicted word within a sentence. min_count – Ignores all words with total frequency lower than this. workers – Use these many worker threads to train the model (=faster training with multicore machines). negative – If &gt; 0, negative sampling will be used, the int for negative specifies how many “noise words” should be drawn (usually between 5-20). If set to 0, no negative sampling is used. Checking Out the ResultsThe goal of this exercise to find out what people talk about when they analyze inflation expectation (basically what currently drives the stock market, TIPS and metals to roar higher and higher) and gold (my favorite and probably the most promising asset in the next 10 years). Word2vev model can find words that have been used in a similar context with the word we care about. Here we employ a method most_similar to reveal them. Let’s first check out the words people usually mention when they talk about inflation. I print out the top 20 words. The meaningful ones are, “target”, “bank’s”, “bank” - of course central banks target on inflation and their monetary policies shape expectations. “nominal”, “negative”, “yields” - words usually describing rates, make sense. “food” - measure of inflation. “employment” - make a lot sense if you remember the Phillips curve. The results are actually very good, authors did understand and explain inflations well. 12345678910111213141516171819202122In: model.wv.most_similar(&apos;inflation&apos;,topn=20)Out:[(&apos;target&apos;, 0.25702178478240967), (&apos;strip&apos;, 0.18441016972064972), (&apos;securities&apos;, 0.18246109783649445), (&apos;full&apos;, 0.16774789988994598), (&apos;nominal&apos;, 0.16606125235557556), (&apos;bank’s&apos;, 0.162990003824234), (&apos;yields&apos;, 0.15935908257961273), (&apos;negative&apos;, 0.15806841850280762), (&apos;remain&apos;, 0.15371079742908478), (&apos;levels&apos;, 0.14426037669181824), (&apos;well&apos;, 0.1438026875257492), (&apos;current&apos;, 0.13828279078006744), (&apos;attractive&apos;, 0.13690069317817688), (&apos;showed&apos;, 0.1277044713497162), (&apos;bank&apos;, 0.12733221054077148), (&apos;food&apos;, 0.12377716600894928), (&apos;similar&apos;, 0.11955425888299942), (&apos;the&apos;, 0.11911500245332718), (&apos;employment&apos;, 0.11866464465856552), (&apos;keep&apos;, 0.11818670481443405)] Then let’s try it for gold. It’s good to see silver rank top, the two metals do hold a strong correlation despite the natures of them are very different, gold generally behaves as a bond of real rates, while silver should be viewed more as a commodity. (People make mistakes on this, see the story of Hunt Brothers) Other words are just descriptive, focusing on telling readers what’s going on in the markets. While this is not helpful, Bloomberg! People want to know why, you may want to talk more about real rates, inflation expectations, debt and global growth in the future. 1234567891011121314151617181920212223242526272829303132In: model.wv.most_similar(&apos;gold&apos;,topn=30)Out:[(&apos;gold’s&apos;, 0.26054084300994873), (&apos;silver&apos;, 0.25277888774871826), (&apos;ounce&apos;, 0.2523342967033386), (&apos;bullion&apos;, 0.2294640988111496), (&apos;2300&apos;, 0.22592982649803162), (&apos;spot&apos;, 0.2171008288860321), (&apos;climbing&apos;, 0.19733953475952148), (&apos;metal&apos;, 0.1939847469329834), (&apos;comex&apos;, 0.19371576607227325), (&apos;rally&apos;, 0.18643531203269958), (&apos;exchangetraded&apos;, 0.1859540343284607), (&apos;a&apos;, 0.18539577722549438), (&apos;delivery&apos;, 0.17595867812633514), (&apos;reaching&apos;, 0.1702871322631836), (&apos;strip&apos;, 0.16905872523784637), (&apos;posted&apos;, 0.16247007250785828), (&apos;lows&apos;, 0.16047437489032745), (&apos;as&apos;, 0.1585429608821869), (&apos;analysis&apos;, 0.1577225774526596), (&apos;2011&apos;, 0.15711694955825806), (&apos;precious&apos;, 0.15656355023384094), (&apos;threat&apos;, 0.1542907953262329), (&apos;more&apos;, 0.1541929841041565), (&apos;drive&apos;, 0.15310749411582947), (&apos;every&apos;, 0.1524747610092163), (&apos;analyst&apos;, 0.1517343521118164), (&apos;managing&apos;, 0.14977654814720154), (&apos;price&apos;, 0.1497490406036377), (&apos;amounts&apos;, 0.14969849586486816), (&apos;backed&apos;, 0.1450338065624237)]","link":"/2020/08/02/word2vec-on-bbg/"},{"title":"Project Covid Admissions Based on Vaccine Progress, Part 1","text":"Lately I’ve been working on a project projecting credit loss for mortgages with a bunch of loan-level attributes. To be honest, the data wasn’t rich. In pursuit of good predictive power, we had to try from the simple multi-period regression, to generalized linear model with distributions beyond the Gaussian family, then mixed effect models, and even used some curve fitting techniques like splines at last to fix the oversimple (or wrong) assumptions we made at the beginning. Overall, I think the journey was inspiring as when it comes to predictive modeling, people always turn to linear regressions first, which isn’t wrong, just so you know the linear regression models and the OLS make a number of assumptions about the predictor variables, the response variable, and their relationship. Violating these assumptions results in biased models or imprecise coefficient, in short cause your model to be less predictive.Well, the good news is that there are numerous extensions have been developed that allow each of these assumptions to be relaxed, and in some cases eliminated entirely, but only if you know what they are and when to use them. More commonly, I see people get stuck and choose to live with a just-fine fit model without knowing that they can easily fix it employing a variation of the regression models. Now let’s see how powerful regressions can get through a very trendy modeling case - project covid admissions based on the vaccinate progress. Regression DataThe data I use includes hospital admissions and vaccinations from the CDC website, and the most up-to-date populations by state from the US Census Bureau. Then let’s create the train/test set to use for all the models below. 1234567891011121314151617def define_variables(data,date0): N=len(data) T=(data[&quot;date&quot;]-date0).dt.days X=data[&quot;vaccinated&quot;].copy().to_frame() X[&quot;T&quot;]=T X[&quot;T2&quot;]=T**2 P=data[&quot;population&quot;] X[&quot;const&quot;]=np.ones(N) return X,Pstart_train=&quot;2021-07-01&quot;end_train=&quot;2021-07-31&quot;event=&quot;admissions&quot;mask = (data[&apos;date&apos;] &gt;= start_train) &amp; (data[&apos;date&apos;] &lt;= end_train)train_data=data[mask].copy()X_train,P_train=covid.define_variables(train_data,date0)Y_train=train_data[event] Weighted Multi-period Regression \\begin{eqnarray} &amp;\\eta_i &amp;=c + \\beta_1 x_i + \\beta_2 t_i + \\beta_3 t_i^2 \\\\ \\mathbb{E}(y_i|x_i) =&amp; \\hat{y}_i &amp;= \\eta_i \\\\ &amp;p(y_i|x_i) &amp; = N(\\hat{y_i},\\sigma^2) \\end{eqnarray} where where - Each observation $i$ represents a single state at one particular date. - $y_i$ is the number of events on that state. - $\\hat{y}_i$ is the *predicted* number of events on that state. - $x_i$ is the percentage of the state population fully vaccinated on that state. - $t_i$ is the number of days elapsed since the beginning of the training period. - $\\sigma^2$ is the level of *noise* of the observations (the variance of the residuals). - the parameters $c$ and $\\beta_1,\\beta_2,\\beta_3$ will be fitted to the observed data to maximize agreement with the model","link":"/2021/09/20/Project-Covid-Admissions-Based-on-Vaccine-Progress-Part-1/"},{"title":"","text":"Covid: Weighted Linear Regression ModelPreliminaries1cd /Users/yumengli/Documents/Python/covid_regressions/module /Users/yumengli/Documents/Python/covid_regressions/module 12345678910import numpy as npimport pandas as pdimport matplotlib.pyplot as pltimport matplotlib.colorsimport seaborn as snsimport statsmodels.api as smimport sysimport covid_analysis as covid Select EventSelect one of cases deaths admissions This will set up the analysis for that particular variable 1event=&quot;admissions&quot; Data1data_dir=&quot;../data&quot; Cases and Deaths12cases=pd.read_csv(f&quot;{data_dir}/covid_cases.csv&quot;,parse_dates=[&quot;date&quot;])cases.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } date state cases deaths 0 2020-02-08 AL 1 0 1 2021-02-02 IL 2304 63 2 2020-07-30 ME 22 2 3 2020-03-29 AL 129 5 4 2020-05-03 NH 89 2 Hospital Admissions12hospitalizations=pd.read_csv(f&quot;{data_dir}/covid_hospitalizations.csv&quot;,parse_dates=[&quot;date&quot;])hospitalizations.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } date state used_beds admissions 0 2020-07-27 PR 514.0 NaN 1 2020-07-24 PR 453.0 NaN 2 2020-07-23 PR 448.0 NaN 3 2020-07-22 IA 0.0 33.0 4 2020-07-22 PR 419.0 0.0 Vaccinations12vaccinations=pd.read_csv(f&quot;{data_dir}/covid_vaccinations.csv&quot;,parse_dates=[&quot;date&quot;])vaccinations.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } date state vaccinated 0 2021-08-24 HI 774774 1 2021-08-24 KY 2128895 2 2021-08-24 VA 4820547 3 2021-08-24 NV 1444294 4 2021-08-24 KS 1381260 Population12population=pd.read_csv(f&quot;{data_dir}/population.csv&quot;)population.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } state population 0 AL 4921532 1 AK 731158 2 AZ 7421401 3 AR 3030522 4 CA 39368078 Used Dataset1234if event==&quot;admissions&quot;: data=hospitalizationselse: data=cases 12data=data.merge(vaccinations,on=[&quot;date&quot;,&quot;state&quot;])data.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } date state used_beds admissions vaccinated 0 2021-06-14 VI 9.0 0.0 33112 1 2021-06-10 MT 66.0 16.0 429634 2 2021-05-12 HI 62.0 4.0 583992 3 2021-05-17 CO 589.0 74.0 2380766 4 2021-07-15 CA 2099.0 302.0 20309600 12data=data.merge(population,on=&quot;state&quot;)data.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } date state used_beds admissions vaccinated population 0 2021-06-10 MT 66.0 16.0 429634 1080577 1 2021-06-21 MT 57.0 13.0 444781 1080577 2 2021-08-05 MT 149.0 23.0 475619 1080577 3 2021-07-15 MT 62.0 9.0 466509 1080577 4 2021-08-21 MT 224.0 59.0 485391 1080577 12data[&quot;vaccinated&quot;]=data[&quot;vaccinated&quot;]/data[&quot;population&quot;]data.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } date state used_beds admissions vaccinated population 0 2021-06-10 MT 66.0 16.0 0.397597 1080577 1 2021-06-21 MT 57.0 13.0 0.411614 1080577 2 2021-08-05 MT 149.0 23.0 0.440153 1080577 3 2021-07-15 MT 62.0 9.0 0.431722 1080577 4 2021-08-21 MT 224.0 59.0 0.449196 1080577 Single Period RegressionModel DefinitionThe weighted linear regression model is defined as:\\begin{eqnarray} \\hat{y}_i &amp;= c + \\beta x_i \\ y_i &amp;=\\hat{y}_i+ \\epsilon_i \\\\end{eqnarray}and we minimize the weighted least squares error :\\begin{equation} E_w = \\sum_i P_i(y - \\hat{y}_i)^2\\end{equation}where Each observation $i$ represents a single state. $\\hat{y}_i$ is the predicted probability of an event (cases, deaths or hospital admissions) per persons on that state. $y_i$ is the actual number of events per person observed. $x_i$ is the percentage of the state population fully vaccinated. $P_i$ is the population of the state. $\\epsilon_i$ is a Gaussian uncorrelated noise with constant variance $\\sigma$ the parameters $c$ and $\\beta$ will be fitted to the observed data to maximize agreement with the model We first fit the regression model to single day of state hospitalization data. 1offset=9 # if we do not want to look at the last day, substract here. 12last_date=data[&quot;date&quot;].max()-pd.offsets.Day(offset)last_date Timestamp(&apos;2021-08-15 00:00:00&apos;) 12last_data=data[data[&quot;date&quot;]==last_date]last_data.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } date state used_beds admissions vaccinated population 75 2021-08-15 MT 194.0 35.0 0.444864 1080577 241 2021-08-15 HI 318.0 42.0 0.546276 1407006 348 2021-08-15 CO 646.0 112.0 0.550732 5807719 658 2021-08-15 CA 7636.0 859.0 0.544115 39368078 813 2021-08-15 MA 416.0 59.0 0.647362 6893574 Regression Data12X=sm.add_constant(last_data[&quot;vaccinated&quot;])X.head() /opt/anaconda3/lib/python3.7/site-packages/statsmodels/tsa/tsatools.py:142: FutureWarning: In a future version of pandas all arguments of concat except for the argument &apos;objs&apos; will be keyword-only x = pd.concat(x[::order], 1) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } const vaccinated 75 1.0 0.444864 241 1.0 0.546276 348 1.0 0.550732 658 1.0 0.544115 813 1.0 0.647362 1P=last_data[&quot;population&quot;] 1P.head() 75 1080577 241 1407006 348 5807719 658 39368078 813 6893574 Name: population, dtype: int64 12Y=last_data[event]/last_data[&quot;population&quot;]Y.head() 75 0.000032 241 0.000030 348 0.000019 658 0.000022 813 0.000009 dtype: float64 statmodels Weighted Linear RegressionWe first use the Weighted Linear Regression model from statmodels. The model is called WLS for Weighted Least Squares. 123mod=sm.WLS(Y,X,P)res=mod.fit()res.summary() WLS Regression Results Dep. Variable: y R-squared: 0.327 Model: WLS Adj. R-squared: 0.313 Method: Least Squares F-statistic: 23.77 Date: Mon, 20 Sep 2021 Prob (F-statistic): 1.18e-05 Time: 22:34:40 Log-Likelihood: 468.45 No. Observations: 51 AIC: -932.9 Df Residuals: 49 BIC: -929.0 Df Model: 1 Covariance Type: nonrobust coef std err t P&gt;|t| [0.025 0.975] const 0.0001 1.97e-05 6.446 0.000 8.73e-05 0.000 vaccinated -0.0002 3.89e-05 -4.875 0.000 -0.000 -0.000 Omnibus: 73.676 Durbin-Watson: 2.094 Prob(Omnibus): 0.000 Jarque-Bera (JB): 909.829 Skew: 3.770 Prob(JB): 2.71e-198 Kurtosis: 22.269 Cond. No. 17.7 Notes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified. 1res.params const 0.000127 vaccinated -0.000190 dtype: float64 The average vaccination rate is 12vac_mean=np.sum(P*last_data[&quot;vaccinated&quot;])/np.sum(P)vac_mean 0.5005361699932351 The expected number of events per 100k person’s assuming an state with the average vaccination rate is: 12even_mean=(res.params[&quot;const&quot;]+res.params[&quot;vaccinated&quot;]*vac_mean)even_mean*100_000 3.186192980837503 A 10% change in the vaccination rate will affect events by a percentage of 12vac_effect=res.params[&quot;vaccinated&quot;]*0.1/even_meanprint(f&quot;{vac_effect:.0%}&quot;) -60% The R-square is defined as ratio of square loss to target variance$$ R^2 = 1 - \\frac{\\sum_{i=1}^N P_i(y_i-\\hat{y}i)^2}{\\sum{i=1}^N P_i(y_i-\\bar{y})^2}$$ 123def square_error(y,y_hat,P): err=y-y_hat return np.sum(P*err**2) 1234Y_pred=res.predict(X)Y_bar=(Y*P).sum()/P.sum()R2=1 - square_error(Y,Y_pred,P)/square_error(Y,Y_bar,P)R2 0.3266337795171833 statmodels Generalized Linear ModelExactly the same model can be written in the more general language of Generalized Linear models as Model DefinitionThe linear regression model is defined as:\\begin{eqnarray} &amp;\\eta_i &amp;=c + \\beta*x_i \\ \\mathbb{E}(y_i|x_i) =&amp; \\hat{y}_i &amp;= \\eta_i \\ &amp;p(y_i|x_i) &amp; = N(\\hat{y}_i,\\sigma)\\end{eqnarray} The fact that $\\eta_i=\\hat{y}_i$ means we are using the identity link between the Gaussian family and the linear model $\\eta$. 123glm_model=sm.GLM(Y,X,family=sm.families.Gaussian(sm.families.links.identity()),var_weights=P)glm_res=glm_model.fit()glm_res.summary() Generalized Linear Model Regression Results Dep. Variable: y No. Observations: 51 Model: GLM Df Residuals: 49 Model Family: Gaussian Df Model: 1 Link Function: identity Scale: 0.0024992 Method: IRLS Log-Likelihood: 468.45 Date: Mon, 20 Sep 2021 Deviance: 0.12246 Time: 22:34:41 Pearson chi2: 0.122 No. Iterations: 3 Covariance Type: nonrobust coef std err z P&gt;|z| [0.025 0.975] const 0.0001 1.97e-05 6.446 0.000 8.83e-05 0.000 vaccinated -0.0002 3.89e-05 -4.875 0.000 -0.000 -0.000 Results are the same but with the GLM language we will be able to perform regression many more kind of variables. The generalization of square error for a GLM model is called the deviance. This is the square error the Gaussian Family used in linear regression. 1glm_res.params const 0.000127 vaccinated -0.000190 dtype: float64 In Sample $R^2$1glm_res.deviance, square_error(Y,Y_pred,P) (0.12246143037040369, 0.12246143037040369) The resuls also include the deviance for the null model fitted to just a constant 1glm_res.null_deviance,square_error(Y,Y_bar,P) (0.1818645287591, 0.1818645287591) The ratio defines an $R^2$ value. 11-glm_res.deviance/glm_res.null_deviance 0.3266337795171833 Model Residuals1covid.plot_residuals(last_data,event,glm_res) We can see that Predicted events can be negative for large vaccination rates. residual variance is negatively correlated with vaccinations rate (smaller vaccination rates have larger magnitude residuals).A Poisson model can fix those issues. Multiple Period RegressionModel DefinitionWe have data from multiple dates, so it makes sense to incorporate Time as a extra variable. \\begin{eqnarray} &amp;\\eta_i &amp;=c + \\beta_1 x_i + \\beta_2 t_i + \\beta_3 t_i^2 \\ \\mathbb{E}(y_i|x_i) =&amp; \\hat{y}_i &amp;= \\eta_i \\ &amp;p(y_i|x_i) &amp; = N(\\hat{y_i},\\sigma^2)\\end{eqnarray}wherewhere Each observation $i$ represents a single state at one particular date. $y_i$ is the number of events on that state. $\\hat{y}_i$ is the predicted number of events on that state. $x_i$ is the percentage of the state population fully vaccinated on that state. $t_i$ is the number of days elapsed since the beginning of the training period. $\\sigma^2$ is the level of noise of the observations (the variance of the residuals). the parameters $c$ and $\\beta_1,\\beta_2,\\beta_3$ will be fitted to the observed data to maximize agreement with the model We including coefficients for $t$ and $t^2$ the dependence of the admission rate on time can have some curvature and does not need to be linear. This can be fitter as a linear model where the inputs are $x_i$, $t_i$ and $t_i^2$ Regression DataFirs we select the training period: 12start_train=&quot;2021-07-15&quot;end_train=&quot;2021-08-15&quot; 123#greater than the start date and smaller than the end datemask = (data[&apos;date&apos;] &gt;= start_train) &amp; (data[&apos;date&apos;] &lt;= end_train)train_data=data[mask].copy() 1date0=train_data[&quot;date&quot;].min() 1train_data.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } date state used_beds admissions vaccinated population 2 2021-08-05 MT 149.0 23.0 0.440153 1080577 3 2021-07-15 MT 62.0 9.0 0.431722 1080577 6 2021-07-25 MT 84.0 10.0 0.435852 1080577 8 2021-08-08 MT 152.0 27.0 0.441906 1080577 10 2021-07-30 MT 116.0 30.0 0.437877 1080577 We write a function to generate our design matrix: 12X_train,P_train=covid.define_variables(train_data,date0)X_train.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } vaccinated T T2 const 2 0.440153 21 441 1.0 3 0.431722 0 0 1.0 6 0.435852 10 100 1.0 8 0.441906 24 576 1.0 10 0.437877 15 225 1.0 1Y_train=train_data[event]/train_data[&quot;population&quot;] Weighted Linear Regression123mod=sm.WLS(Y_train,X_train,P_train)res=mod.fit()res.summary() WLS Regression Results Dep. Variable: y R-squared: 0.373 Model: WLS Adj. R-squared: 0.371 Method: Least Squares F-statistic: 322.3 Date: Mon, 20 Sep 2021 Prob (F-statistic): 3.12e-164 Time: 22:34:43 Log-Likelihood: 15151. No. Observations: 1632 AIC: -3.029e+04 Df Residuals: 1628 BIC: -3.027e+04 Df Model: 3 Covariance Type: nonrobust coef std err t P&gt;|t| [0.025 0.975] vaccinated -0.0002 6.13e-06 -24.647 0.000 -0.000 -0.000 T 1.038e-06 1.82e-07 5.686 0.000 6.8e-07 1.4e-06 T2 -1.18e-09 5.69e-09 -0.207 0.836 -1.23e-08 9.98e-09 const 8.1e-05 3.17e-06 25.544 0.000 7.48e-05 8.72e-05 Omnibus: 1418.486 Durbin-Watson: 0.372 Prob(Omnibus): 0.000 Jarque-Bera (JB): 43359.192 Skew: 4.022 Prob(JB): 0.00 Kurtosis: 26.936 Cond. No. 6.93e+03 Notes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.[2] The condition number is large, 6.93e+03. This might indicate that there arestrong multicollinearity or other numerical problems. 1res.params vaccinated -1.511215e-04 T 1.037547e-06 T2 -1.179850e-09 const 8.099511e-05 dtype: float64 123# weighted averageY_bar=np.sum(Y_train*P_train)/np.sum(P_train)Y_bar 2.2866902299871973e-05 1Y_pred=res.predict(X_train) 123deviance=square_error(Y_train,Y_pred,P_train)null_deviance=square_error(Y_train,Y_bar,P_train)deviance,null_deviance (3.2184382974951373, 5.12981850956498) 12R2=1-deviance/null_devianceR2 0.3726019173009557 Extrapolation of Time trends1covid.plot_time_extrapolation(train_data,res,event,date0) /opt/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/__init__.py:1402: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version. Convert to a numpy array before indexing instead. x[:, None] /opt/anaconda3/lib/python3.7/site-packages/matplotlib/axes/_base.py:278: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version. Convert to a numpy array before indexing instead. y = y[:, np.newaxis] In-sample model FitObservations are arranged by state and date. To visualize the quality of the fit on the training data, we present snapshots of the fit for a few selected dates on the training period. 1234567891011121314151617181920212223242526272829def plot_residuals(data,event,res): P=data[&quot;population&quot;] # Create two subplots and unpack the output array immediately f, (ax1, ax2) = plt.subplots(1, 2,sharex=True,figsize=(16,4)) norm=matplotlib.colors.LogNorm(vmin=100_000, vmax=P.max(), clip=False) m=ax1.scatter(data[&quot;vaccinated&quot;],data[event]/data[&quot;population&quot;]*100_000, c=P,norm=norm,cmap=&quot;Blues&quot;,label=event) plt.colorbar(m,label=&quot;State Population&quot;) x=np.linspace(0.3,0.75,201) x= pd.DataFrame({&apos;vaccinated&apos;:x, &apos;T&apos;:31, &apos;T2&apos;:31*31}) x[&quot;const&quot;]=np.ones(201) y_pred=res.predict(x) ax1.plot(x.vaccinated, y_pred*100_000,&quot;k--&quot;,label=&quot;predicted&quot;) idx = data.index ax2.scatter(data[&quot;vaccinated&quot;], res.resid[idx], c=P,norm=norm,cmap=&quot;Blues&quot;,label=event) ax2.set_ybound(lower=-0.00008,upper=0.00008) ax2.set_title(&quot;Residuals&quot;) ax2.set_xlabel(&quot;vaccination&quot;) ax2.set_ylabel(&quot;residual&quot;) # select outlier state outlier_idx=np.argpartition(np.array(res.resid[idx]), -2)[-2:] for i in outlier_idx: outlier=data.iloc[i] ax1.annotate(outlier[&quot;state&quot;],(outlier[&quot;vaccinated&quot;]+0.01,outlier[event]/outlier[&quot;population&quot;]*100_000)) ax2.annotate(outlier[&quot;state&quot;],(outlier[&quot;vaccinated&quot;]+0.01,res.resid[idx].iloc[i])) 1plot_residuals(last_data,event,res) 1covid.plot_time_facets(train_data,res,event,date0) Model PredictionWe will now use the model without recalibrating to make predictions about admission rates on new data. This is out of sample evaluation. It is the only way to make sure the model really works. First, we select a few new dates: 1test_period=7 # days 1test_end=pd.Timestamp(end_train)+pd.DateOffset(days=test_period) 12test_data=data[(data[&quot;date&quot;]&gt;end_train) &amp; (data[&quot;date&quot;]&lt;=test_end)].copy()test_data[&quot;date&quot;].unique() array([&apos;2021-08-21T00:00:00.000000000&apos;, &apos;2021-08-16T00:00:00.000000000&apos;, &apos;2021-08-17T00:00:00.000000000&apos;, &apos;2021-08-19T00:00:00.000000000&apos;, &apos;2021-08-18T00:00:00.000000000&apos;, &apos;2021-08-20T00:00:00.000000000&apos;, &apos;2021-08-22T00:00:00.000000000&apos;], dtype=&apos;datetime64[ns]&apos;) We generate the regression variables for the new data 12X_test,P_test=covid.define_variables(test_data,date0)X_test.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } vaccinated T T2 const 4 0.449196 37 1369 1.0 16 0.444887 32 1024 1.0 66 0.444963 33 1089 1.0 79 0.446743 35 1225 1.0 101 0.446681 34 1156 1.0 1Y_test=test_data[event]/test_data[&quot;population&quot;] Out of Sample $R^2$First we fit the single constant admission rate for the test data to define the null model 1Y_bar=np.sum(Y_test*P_test)/np.sum(P_test) 12null_deviance_test=square_error(Y_test,Y_bar,P_test)null_deviance_test 1.5894793995786725 1Y_pred=res.predict(X_test) 12deviance_test=square_error(Y_test,Y_pred,P_test)deviance_test 1.1348447392311178 11-deviance_test/null_deviance_test 0.2860274002091917 Out of sample $R^2$ is worse than for the in sample (\\approx 39%). Predicting the future is hard, and all that… Out of Sample Model Fit1 1covid.plot_time_facets(test_data,res,event,date0) It seems we are over estimating how fast the admission rate is increasing. Day of the Week EffectsData reporting has day of the week effects: 123aggregate=data.groupby([&quot;date&quot;])[event].sum()aggregate=aggregate.to_frame().reset_index()aggregate.plot(x=&quot;date&quot;,y=event) &lt;matplotlib.axes._subplots.AxesSubplot at 0x7feff0dcc850&gt; 12X_train,P_train=covid.define_variables2(train_data,date0)X_train.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } vaccinated T T2 const Monday Saturday Sunday Thursday Tuesday Wednesday 2 0.440153 21 441 1.0 0 0 0 1 0 0 3 0.431722 0 0 1.0 0 0 0 1 0 0 6 0.435852 10 100 1.0 0 0 1 0 0 0 8 0.441906 24 576 1.0 0 0 1 0 0 0 10 0.437877 15 225 1.0 0 0 0 0 0 0 123mod=sm.WLS(Y_train,X_train,P_train)res=mod.fit()res.summary() WLS Regression Results Dep. Variable: y R-squared: 0.376 Model: WLS Adj. R-squared: 0.372 Method: Least Squares F-statistic: 108.5 Date: Mon, 20 Sep 2021 Prob (F-statistic): 4.82e-159 Time: 22:36:26 Log-Likelihood: 15155. No. Observations: 1632 AIC: -3.029e+04 Df Residuals: 1622 BIC: -3.024e+04 Df Model: 9 Covariance Type: nonrobust coef std err t P&gt;|t| [0.025 0.975] vaccinated -0.0002 6.13e-06 -24.658 0.000 -0.000 -0.000 T 1.028e-06 1.87e-07 5.484 0.000 6.6e-07 1.4e-06 T2 -6.639e-10 5.85e-09 -0.113 0.910 -1.21e-08 1.08e-08 const 8.186e-05 3.31e-06 24.752 0.000 7.54e-05 8.83e-05 Monday -2.236e-06 1.66e-06 -1.350 0.177 -5.48e-06 1.01e-06 Saturday -8.319e-07 1.55e-06 -0.537 0.591 -3.87e-06 2.21e-06 Sunday -3.189e-06 1.55e-06 -2.056 0.040 -6.23e-06 -1.47e-07 Thursday -2.234e-08 1.55e-06 -0.014 0.988 -3.06e-06 3.02e-06 Tuesday 3.233e-07 1.66e-06 0.195 0.845 -2.93e-06 3.57e-06 Wednesday -2.672e-07 1.66e-06 -0.161 0.872 -3.52e-06 2.98e-06 Omnibus: 1416.223 Durbin-Watson: 0.364 Prob(Omnibus): 0.000 Jarque-Bera (JB): 42872.585 Skew: 4.016 Prob(JB): 0.00 Kurtosis: 26.790 Cond. No. 6.95e+03 Notes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.[2] The condition number is large, 6.95e+03. This might indicate that there arestrong multicollinearity or other numerical problems. Day of the week provides a small improvement on $R^2$ compared to original model. 1 1","link":"/2021/09/20/Covid_WeightedLinearRegression/"}],"tags":[],"categories":[{"name":"NLP","slug":"NLP","link":"/categories/NLP/"}]}