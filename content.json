{"pages":[{"title":"categories","text":"","link":"/categories/index.html"}],"posts":[{"title":"Training Word Embedding on Bloomberg News","text":"While working from home (stuck at home indeed), I get a bit more time, so revisited some old deep learning notes last month. Would it be awesome if I can come up with a project to play with? As we all know knowledge would go rusty if lack of use! Initially, I was thinking about a machine translator using Attention Models, because I was once asked in an interview. The fund showed a particular interest in developing such a tool for the purpose of distributing research reports to non-English-speaking countries. But I was completely set back by the data sets asking for literally thousands of dollars… While if labeled dataset is that expensive, why not run some unsurprised learning? Training word2vec doesn’t sound like a bad idea. I’ve been writing some views on FICC (fixed income, currencies, and commodities) on my other non-geeky-at-all blog. Won’t it be fun to run word embeddings on Bloomberg news, see if people talk about the right things when they analyze inflation and gold prices? So here we go, rock and roll! First it’s a bit introduction to word embedding. The term refers to a set of techniques in natural language processing that can map words into high dimensional vectors of real numbers. The vectors/embeddings can be seen as featurized representations of words, which preserve semantic properties, hence are commonly used as inputs in NLP tasks. Here is an example, While in real cases, the dimension of vectors is usually a lot higher. We will use a tool called word2vec to convert words into embeddings, gensim has it implemented in a very easy-to-use way. Word2vec is a technique for natural language processing. The word2vec algorithm uses a neural network model to learn word associations from a large corpus of text. Once trained, such a model can detect synonymous words or suggest additional words for a partial sentence. More explanation on word2vec can be found here. Creating CorpusScarping Addresses of Bloomberg ArticlesI would like to run the embeddings only on the most recent posts, so first need to grab their urls through a search page. Import the libraries, 1234567import reimport requestsimport stringimport numpy as npimport gensim.models.word2vec as word2vecfrom bs4 import BeautifulSoupfrom time import sleep A function that can scrape the urls of articles that show up in the top p pages related to a specific topic, 123456789101112def scrape_bloomberg_urls(subject, maxPage, headers): urls = [] for p in range(1,maxPage): searchUrl = &apos;https://www.bloomberg.com/search?query=&apos; + subject + &apos;&amp;sort=relevance:desc&apos; + &apos;&amp;page=&apos; + str(p) response = requests.get(searchUrl, headers=headers) soup = BeautifulSoup(response.content, &apos;html.parser&apos;) regex = re.compile(&apos;.*headline.*&apos;) for tag in soup.findAll(&apos;a&apos;, {&quot;class&quot; : regex}, href = True): href = tag.attrs[&apos;href&apos;] if &apos;/news/articles/&apos; in href and href not in urls: urls.append(href) return urls Set up headers to pass to the request, websites have gone crazy about blocking robots, 1234567headers = { &apos;user-agent&apos;: &apos;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36&apos;, &apos;referrer&apos;: &apos;https://google.com&apos;, &apos;Accept&apos;: &apos;text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8&apos;, &apos;Accept-Encoding&apos;: &apos;gzip, deflate, br&apos;, &apos;Accept-Language&apos;: &apos;en-US,en;q=0.9&apos;, &apos;Pragma&apos;: &apos;no-cache&apos;} We get 155 articles here, not a large dataset so it may give us weird results, but should be good enough for an exercise, 12In: len(urls)Out: 155 1234567In: urls[:5]Out: [&apos;https://www.bloomberg.com/news/articles/2020-08-07/inflation-trend-a-friend-in-real-yield-contest-seasia-rates&apos;, &apos;https://www.bloomberg.com/news/articles/2020-08-06/blackrock-joins-crescendo-of-inflation-warnings-amid-virus-fight&apos;, &apos;https://www.bloomberg.com/news/articles/2020-08-06/china-inflation-rate-headed-for-0-on-cooling-food-prices-chart&apos;, &apos;https://www.bloomberg.com/news/articles/2020-08-06/inflation-binds-czechs-after-fast-rate-cuts-decision-day-guide&apos;, &apos;https://www.bloomberg.com/news/articles/2020-08-07/jpmorgan-rejects-threat-to-dollar-status-flagged-by-goldman&apos;] Parsing ArticlesThis step is to parse articles through the urls and save the corpus into a local text file. 123456789101112def parse_article(urls, headers): corpus = [] for url in urls: response = requests.get(url, headers=headers) soup = BeautifulSoup(response.content,&apos;lxml&apos;) for tag in soup.find_all(&apos;p&apos;): content = tag.get_text() cleanedContent = content.tra nslate(str.maketrans(&apos;&apos;, &apos;&apos;, string.punctuation)).lower() corpus.append(cleanedContent) seconds = np.random.randint(low=5, high=20) sleep(seconds) return corpus A function serves as a writer, 12345def write_to_txt(outdir, subject, contentList): outputfile = f&apos;{outdir}/{subject}.txt&apos; with open(outputfile, &apos;a&apos;) as file: for i in contentList: file.write(f&apos;{i}\\n&apos;) Run the functions, 12corpus = parse_article(urls, headers)write_to_txt(&apos;Documents/Blog/&apos;, &apos;corpus&apos;, corpus) Training the ModelNow that we have created the corpus, let’s train the model using word2vec from gensim. Gensim has a cool function LineSentence that can directly read sentences from a text file with one sentence a line. Now you probably get why I had the corpus saved this way. 12sentences = word2vec.LineSentence(&apos;corpus.txt&apos;)model = word2vec.Word2Vec(sentences, min_count=10, workers=8, iter=500, window=15, size=300, negative=50) The model I choose is using the Skip-Gram algorithm with Negative Sampling. Hyper-parameters above are what I find work well, see below for their definitions, just in case you would like to tune them yourself, size – Dimensionality of the word vectors. window – Maximum distance between the current and predicted word within a sentence. min_count – Ignores all words with total frequency lower than this. workers – Use these many worker threads to train the model (=faster training with multicore machines). negative – If &gt; 0, negative sampling will be used, the int for negative specifies how many “noise words” should be drawn (usually between 5-20). If set to 0, no negative sampling is used. Checking Out the ResultsThe goal of this exercise to find out what people talk about when they analyze inflation expectation (basically what currently drives the stock market, TIPS and metals to roar higher and higher) and gold (my favorite and probably the most promising asset in the next 10 years). Word2vev model can find words that have been used in a similar context with the word we care about. Here we employ a method most_similar to reveal them. Let’s first check out the words people usually mention when they talk about inflation. I print out the top 20 words. The meaningful ones are, “target”, “bank’s”, “bank” - of course central banks target on inflation and their monetary policies shape expectations. “nominal”, “negative”, “yields” - words usually describing rates, make sense. “food” - measure of inflation. “employment” - make a lot sense if you remember the Phillips curve. The results are actually very good, authors did understand and explain inflations well. 12345678910111213141516171819202122In: model.wv.most_similar(&apos;inflation&apos;,topn=20)Out:[(&apos;target&apos;, 0.25702178478240967), (&apos;strip&apos;, 0.18441016972064972), (&apos;securities&apos;, 0.18246109783649445), (&apos;full&apos;, 0.16774789988994598), (&apos;nominal&apos;, 0.16606125235557556), (&apos;bank’s&apos;, 0.162990003824234), (&apos;yields&apos;, 0.15935908257961273), (&apos;negative&apos;, 0.15806841850280762), (&apos;remain&apos;, 0.15371079742908478), (&apos;levels&apos;, 0.14426037669181824), (&apos;well&apos;, 0.1438026875257492), (&apos;current&apos;, 0.13828279078006744), (&apos;attractive&apos;, 0.13690069317817688), (&apos;showed&apos;, 0.1277044713497162), (&apos;bank&apos;, 0.12733221054077148), (&apos;food&apos;, 0.12377716600894928), (&apos;similar&apos;, 0.11955425888299942), (&apos;the&apos;, 0.11911500245332718), (&apos;employment&apos;, 0.11866464465856552), (&apos;keep&apos;, 0.11818670481443405)] Then let’s try it for gold. It’s good to see silver rank top, the two metals do hold a strong correlation despite the natures of them are very different, gold generally behaves as a bond of real rates, while silver should be viewed more as a commodity. (People make mistakes on this, see the story of Hunt Brothers) Other words are just descriptive, focusing on telling readers what’s going on in the markets. While this is not helpful, Bloomberg! People want to know why, you may want to talk more about real rates, inflation expectations, debt and global growth in the future. 1234567891011121314151617181920212223242526272829303132In: model.wv.most_similar(&apos;gold&apos;,topn=30)Out:[(&apos;gold’s&apos;, 0.26054084300994873), (&apos;silver&apos;, 0.25277888774871826), (&apos;ounce&apos;, 0.2523342967033386), (&apos;bullion&apos;, 0.2294640988111496), (&apos;2300&apos;, 0.22592982649803162), (&apos;spot&apos;, 0.2171008288860321), (&apos;climbing&apos;, 0.19733953475952148), (&apos;metal&apos;, 0.1939847469329834), (&apos;comex&apos;, 0.19371576607227325), (&apos;rally&apos;, 0.18643531203269958), (&apos;exchangetraded&apos;, 0.1859540343284607), (&apos;a&apos;, 0.18539577722549438), (&apos;delivery&apos;, 0.17595867812633514), (&apos;reaching&apos;, 0.1702871322631836), (&apos;strip&apos;, 0.16905872523784637), (&apos;posted&apos;, 0.16247007250785828), (&apos;lows&apos;, 0.16047437489032745), (&apos;as&apos;, 0.1585429608821869), (&apos;analysis&apos;, 0.1577225774526596), (&apos;2011&apos;, 0.15711694955825806), (&apos;precious&apos;, 0.15656355023384094), (&apos;threat&apos;, 0.1542907953262329), (&apos;more&apos;, 0.1541929841041565), (&apos;drive&apos;, 0.15310749411582947), (&apos;every&apos;, 0.1524747610092163), (&apos;analyst&apos;, 0.1517343521118164), (&apos;managing&apos;, 0.14977654814720154), (&apos;price&apos;, 0.1497490406036377), (&apos;amounts&apos;, 0.14969849586486816), (&apos;backed&apos;, 0.1450338065624237)]","link":"/2020/08/02/word2vec-on-bbg/"},{"title":"Project Covid Admissions Based on Vaccine Progress, Part 1 -- From Simple Linear Regression to Weighted Time Series Regression","text":"Lately I’ve been working on a project predicting credit losses for mortgages with a bunch of loan-level attributes. To be honest, the data wasn’t rich. In pursuit of good predictive power, I had to try from the simple multi-period regression, to generalized linear model with distributions beyond the Gaussian family, then mixed effect models, and even used some curve fitting techniques like splines at last to fix the oversimple (or wrong) assumptions I made at the beginning. Overall, I think the journey was inspiring as it basically shows that one can almost crack any problem with regression analysis, more importantly, in an elegant way. When it comes to predictive modeling, people always turn to linear regressions, which isn’t wrong, just so you know that linear regression models and OLS make a number of assumptions about the predictor variables, the response variable, and their relationship. Violating these assumptions can result in biased predictions or imprecise coefficients, in short cause your model to be less predictive. Well, the good news is that there are numerous extensions have been developed that allow each of these assumptions to be relaxed, and in some cases eliminated entirely, but only if you know what they are and when to use them. More commonly, I see people get stuck and choose to live with a just-fine-fit model without knowing that they can easily fix it by employing a more generalized regression models. Now let’s see how powerful regressions can get through a trendy modeling case - project covid admissions based on the vaccinate progress. Preliminaries12345678import numpy as npimport pandas as pdimport matplotlib.pyplot as pltimport matplotlib.colorsimport seaborn as snsimport statsmodels.api as smimport sysimport covid_analysis as covid # some functions developed for this blog The data I use includes hospital admissions and vaccinations from the CDC website, and the most up-to-date populations by state from the US Census Bureau. 1data_dir=&quot;../data&quot; 1234567891011hospitalizations=pd.read_csv(f&quot;{data_dir}/covid_hospitalizations.csv&quot;,parse_dates=[&quot;date&quot;])vaccinations=pd.read_csv(f&quot;{data_dir}/covid_vaccinations.csv&quot;,parse_dates=[&quot;date&quot;])population=pd.read_csv(f&quot;{data_dir}/population.csv&quot;)event=&quot;admissions&quot;data=hospitalizationsdata=data.merge(vaccinations,on=[&quot;date&quot;,&quot;state&quot;])data=data.merge(population,on=&quot;state&quot;)data[&quot;vaccinated&quot;]=data[&quot;vaccinated&quot;]/data[&quot;population&quot;]data.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } date state used_beds admissions vaccinated population 0 2021-06-10 MT 66.0 16.0 0.397597 1080577 1 2021-06-21 MT 57.0 13.0 0.411614 1080577 2 2021-08-05 MT 149.0 23.0 0.440153 1080577 3 2021-07-15 MT 62.0 9.0 0.431722 1080577 4 2021-08-21 MT 224.0 59.0 0.449196 1080577 Now we are good to go. As always, let’s first check on our old friend, simple linear regression with one single explanatory variable, as a warm-up. Here specifically, I am using weighted least squares rather than ordinary least squares to incorporate the knowledge of state populations. WLS is a generalization of OLS and a specialization of generalized least squares, which can relax the assumptions of constant variance, allowing the variances of the observations to be unequal i.e., heteroscedasticity (btw one of my favorite words in statistics, the pronunciation’s fun). Single Period RegressionModel DefinitionThe weighted linear regression model is defined as:\\begin{eqnarray} \\hat{y}_i &amp;= c + \\beta x_i \\newline y_i &amp;=\\hat{y}_i+ \\epsilon_i \\newline\\end{eqnarray}and we minimize the weighted least squares error :\\begin{equation} E_w = \\sum_i P_i(y - \\hat{y}_i)^2\\end{equation}where Each observation $i$ represents a single state. $\\hat{y}_i$ is the predicted probability of hospital admissions per persons on that state. $y_i$ is the actual number of events per person observed. $x_i$ is the percentage of the state population fully vaccinated. $P_i$ is the population of the state. $\\epsilon_i$ is a Gaussian uncorrelated noise with constant variance $\\sigma$ the parameters $c$ and $\\beta$ will be fitted to the observed data to maximize agreement with the model We first fit the regression model to single day of state hospitalization data. 1offset=9 # if we do not want to look at the last day, subtract here. Let’s take Aug.15, 12last_date=data[&quot;date&quot;].max()-pd.offsets.Day(offset)last_data=data[data[&quot;date&quot;]==last_date] 123X=sm.add_constant(last_data[&quot;vaccinated&quot;])P=last_data[&quot;population&quot;]Y=last_data[event]/last_data[&quot;population&quot;] Use statmodels for Weighted Linear RegressionLet’s first use the Weighted Linear Regression model from statmodels.The exact same estimation can be done by a different way, which I will show later. 123mod=sm.WLS(Y,X,P)res=mod.fit()res.params const 0.000127 vaccinated -0.000190 dtype: float64 So here’s our estimation of the coefficients and some statistics that are helpful for diagnosis. It’s easy to read that the R-squared is 0.327. But we can also calculate it ourselves. The R-square is defined as ratio of square loss to target variance, 123def square_error(y,y_hat,P): err=y-y_hat return np.sum(P*err**2) 1234Y_pred=res.predict(X)Y_bar=(Y*P).sum()/P.sum()R2=1 - square_error(Y,Y_pred,P)/square_error(Y,Y_bar,P)R2 0.3266337795171833 Use statmodels GLM for Weighted Linear RegressionExactly the same model can be written in the more general language of Generalized Linear models as Model DefinitionThe linear regression model is defined as:\\begin{eqnarray} &amp;\\eta_i &amp;=c + \\beta*x_i \\newline \\mathbb{E}(y_i|x_i) =&amp; \\hat{y}_i &amp;= \\eta_i \\newline &amp;p(y_i|x_i) &amp; = N(\\hat{y}_i,\\sigma)\\end{eqnarray} The fact that $\\eta_i=\\hat{y}_i$ means we are using the identity link between the Gaussian family and the linear model $\\eta$. One should be aware here that linear model is just a specical case of GLM. 123glm_model=sm.GLM(Y,X,family=sm.families.Gaussian(sm.families.links.identity()),var_weights=P)glm_res=glm_model.fit()glm_res.params const 0.000127 vaccinated -0.000190 dtype: float64 Results are the same. But with the GLM language we will be able to perform regression many more kinds of variables. The generalization of square error for a GLM model is called the deviance. This is the square error the Gaussian Family used in linear regression. Let’s calculate the in-sample R-squared in the GLM way and compare it with the previous. 1glm_res.deviance, square_error(Y,Y_pred,P) (0.12246143037040369, 0.12246143037040369) The resuls also include the deviance for the null model fitted to just a constant 1glm_res.null_deviance,square_error(Y,Y_bar,P) (0.1818645287591, 0.1818645287591) The ratio defines an $R^2$ value. Not surprisingly, the results are exactly the same. Overall, I admit a 32.7% R-squared model can barely be said to be a good model. But no worries, all can be fixed. 11-glm_res.deviance/glm_res.null_deviance 0.3266337795171833 Now, as we have a better understanding of the data and the modeling methodologies, let’s move on to model the time series. The data we have here is time series data, indeed panel data, as time series and cross-sectional data are both special cases of panel data that are in one dimension only. But we will deal with the multi-subjects matter in the next blog(probably should have a spoiler tag oops). Anyways, let’s first start with the multi-period regression, here I have trend variables added to the model to represent time. Multiple Period RegressionModel Definition\\begin{eqnarray} &amp;\\eta_i &amp;=c + \\beta_1 x_i + \\beta_2 t_i + \\beta_3 t_i^2 \\newline \\mathbb{E}(y_i|x_i) =&amp; \\hat{y}_i &amp;= \\eta_i \\newline &amp;p(y_i|x_i) &amp; = N(\\hat{y_i},\\sigma^2)\\end{eqnarray}where Each observation $i$ represents a single state at one particular date. $y_i$ is the number of events on that state. $\\hat{y}_i$ is the predicted number of events on that state. $x_i$ is the percentage of the state population fully vaccinated on that state. $t_i$ is the number of days elapsed since the beginning of the training period. $\\sigma^2$ is the level of noise of the observations (the variance of the residuals). the parameters $c$ and $\\beta_1,\\beta_2,\\beta_3$ will be fitted to the observed data to maximize agreement with the model This can be fitter as a linear model where the inputs are $x_i$, $t_i$ and $t_i^2$ First we select the training period: 1234start_train=&quot;2021-07-15&quot;end_train=&quot;2021-08-15&quot;train_data=data[(data[&apos;date&apos;] &gt;= start_train) &amp; (data[&apos;date&apos;] &lt;= end_train)].copy()date0=train_data[&quot;date&quot;].min() Then a 7-day testing period: 123test_period=7 # daystest_end=pd.Timestamp(end_train)+pd.DateOffset(days=test_period)test_data=data[(data[&quot;date&quot;]&gt;end_train) &amp; (data[&quot;date&quot;]&lt;=test_end)].copy() I write a function to generate the desired matrix, 123456789def define_variables(data,date0): N=len(data) T=(data[&quot;date&quot;]-date0).dt.days X=data[&quot;vaccinated&quot;].copy().to_frame() X[&quot;T&quot;]=T X[&quot;T2&quot;]=T**2 P=data[&quot;population&quot;] X[&quot;const&quot;]=np.ones(N) return X,P 1234X_train,P_train=covid.define_variables(train_data,date0)Y_train=train_data[event]/train_data[&quot;population&quot;]X_test,P_test=covid.define_variables(test_data,date0)Y_test=test_data[event]/test_data[&quot;population&quot;] Weighted Multi-period Linear Regression123mod=sm.WLS(Y_train,X_train,P_train)res=mod.fit()res.summary() WLS Regression Results Dep. Variable: y R-squared: 0.373 Model: WLS Adj. R-squared: 0.371 Method: Least Squares F-statistic: 322.3 Date: Tue, 21 Sep 2021 Prob (F-statistic): 3.12e-164 Time: 15:16:26 Log-Likelihood: 15151. No. Observations: 1632 AIC: -3.029e+04 Df Residuals: 1628 BIC: -3.027e+04 Df Model: 3 Covariance Type: nonrobust coef std err t P&gt;|t| [0.025 0.975] vaccinated -0.0002 6.13e-06 -24.647 0.000 -0.000 -0.000 T 1.038e-06 1.82e-07 5.686 0.000 6.8e-07 1.4e-06 T2 -1.18e-09 5.69e-09 -0.207 0.836 -1.23e-08 9.98e-09 const 8.1e-05 3.17e-06 25.544 0.000 7.48e-05 8.72e-05 Omnibus: 1418.486 Durbin-Watson: 0.372 Prob(Omnibus): 0.000 Jarque-Bera (JB): 43359.192 Skew: 4.022 Prob(JB): 0.00 Kurtosis: 26.936 Cond. No. 6.93e+03 Notes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.[2] The condition number is large, 6.93e+03. This might indicate that there arestrong multicollinearity or other numerical problems. Calculate the R-squared again, still, not good enough.So let&apos;s take a deep dive and see where can be improved. 1234567Y_bar=np.sum(Y_train*P_train)/np.sum(P_train) # weighted averageY_pred=res.predict(X_train)deviance=square_error(Y_train,Y_pred,P_train)null_deviance=square_error(Y_train,Y_bar,P_train)R2=1-deviance/null_devianceR2 0.3726019173009557 Extrapolation of Time trendsThe linear model projects the admission to be increasing steadily if assuming current vaccination rate. 1covid.plot_time_extrapolation(train_data,res,event,date0) In-sample model FitTo better evaluate the model fit, we visualize the real observations and model prodictions. Here’s some snapshots of the fit for a few selected dates on the training period. Observations are arranged by state and date. Don’t have to say much, you can tell the weighted linear model doesn’t fit well as the corelation between vaccinations and admissions looks more like a curve than a straight line. 1covid.plot_time_facets(train_data,res,event,date0) We can further plot out the residuals. I choose one date from the dates above, Aug.15 and apply the following function to generate its residual plot. 1234567891011121314151617181920212223242526272829def plot_residuals(data,event,res): P=data[&quot;population&quot;] # Create two subplots and unpack the output array immediately f, (ax1, ax2) = plt.subplots(1, 2,sharex=True,figsize=(16,4)) norm=matplotlib.colors.LogNorm(vmin=100_000, vmax=P.max(), clip=False) m=ax1.scatter(data[&quot;vaccinated&quot;],data[event]/data[&quot;population&quot;]*100_000, c=P,norm=norm,cmap=&quot;Blues&quot;,label=event) plt.colorbar(m,label=&quot;State Population&quot;) x=np.linspace(0.3,0.75,201) x= pd.DataFrame({&apos;vaccinated&apos;:x, &apos;T&apos;:31, &apos;T2&apos;:31*31}) x[&quot;const&quot;]=np.ones(201) y_pred=res.predict(x) ax1.plot(x.vaccinated, y_pred*100_000,&quot;k--&quot;,label=&quot;predicted&quot;) idx = data.index ax2.scatter(data[&quot;vaccinated&quot;], res.resid[idx], c=P,norm=norm,cmap=&quot;Blues&quot;,label=event) ax2.set_ybound(lower=-0.00008,upper=0.00008) ax2.set_title(&quot;Residuals&quot;) ax2.set_xlabel(&quot;vaccination&quot;) ax2.set_ylabel(&quot;residual&quot;) # select outlier state outlier_idx=np.argpartition(np.array(res.resid[idx]), -2)[-2:] for i in outlier_idx: outlier=data.iloc[i] ax1.annotate(outlier[&quot;state&quot;],(outlier[&quot;vaccinated&quot;]+0.01,outlier[event]/outlier[&quot;population&quot;]*100_000)) ax2.annotate(outlier[&quot;state&quot;],(outlier[&quot;vaccinated&quot;]+0.01,res.resid[idx].iloc[i])) We can see that Predicted events can be negative for large vaccination rates, saying we are over estimating how fast the admission rate is increasing. residual variance is negatively correlated with vaccinations rate i.e., smaller vaccination rates have larger magnitude residuals. A Poisson model can fix those issues, which will be the topic of our next blog. Some states like FL and KY look like outliers, indicating that they may have different intercepts than the others, which is not surprising as states are different subjects and each may have unique fundamentals. A mixed model would be a good solution for this. 12plot_residuals(last_data,event,res) Model PredictionWe will now use the model without recalibrating to make predictions about admission rates on new data. This is out of sample evaluation. It is the only way to make sure the model really works. Out of Sample Model FitBesides greater residuals, the out of sample evaluation is basically telling us a same story. 1covid.plot_time_facets(test_data,res,event,date0) Day of the Week EffectsBefore we move on to GLM, there’s another trick to play with. The data reporting obviously has day of the week effects, 123aggregate=data.groupby([&quot;date&quot;])[event].sum()aggregate=aggregate.to_frame().reset_index()aggregate.plot(x=&quot;date&quot;,y=event) We use a new function to generate the train data, more dummy variables are added to represent days of a week, 1234567891011def define_variables2(data,date0): N=len(data) T=(data[&quot;date&quot;]-date0).dt.days X=data[&quot;vaccinated&quot;].copy().to_frame() X[&quot;T&quot;]=T X[&quot;T2&quot;]=T**2 X[&quot;const&quot;]=np.ones(N) Z=pd.get_dummies(data[&quot;date&quot;].dt.day_name(),drop_first=True) X=pd.concat([X,Z],axis=1) P=data[&quot;population&quot;] return X,P 12X_train,P_train=define_variables2(train_data,date0)X_train.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } vaccinated T T2 const Monday Saturday Sunday Thursday Tuesday Wednesday 2 0.440153 21 441 1.0 0 0 0 1 0 0 3 0.431722 0 0 1.0 0 0 0 1 0 0 6 0.435852 10 100 1.0 0 0 1 0 0 0 8 0.441906 24 576 1.0 0 0 1 0 0 0 10 0.437877 15 225 1.0 0 0 0 0 0 0 123mod=sm.WLS(Y_train,X_train,P_train)res=mod.fit()res.params vaccinated -1.510900e-04 T 1.028112e-06 T2 -6.639196e-10 const 8.186223e-05 Monday -2.235516e-06 Saturday -8.318830e-07 Sunday -3.189080e-06 Thursday -2.233662e-08 Tuesday 3.232847e-07 Wednesday -2.672040e-07 dtype: float64 1res.rsquared 0.37572844250241666 $R^2$ is 0.376, higher than the 0.373 we previously have. Day of the week provides a small improvement on $R^2$ compared to original model.","link":"/2021/09/21/projectcovid/"},{"title":"Project Covid Admissions Based on Vaccine Progress, Part 2 -- Generalized Linear Models, From Poisson Regression to Negative Binomial Regression","text":"In the last blog, we try to use linear regressions to model hospital admissions and vaccinations. The findings were the linear model tend to over-estimating how fast the admission rate is increasing on the higher range of vaccination rates, and residual variance is negatively correlated with vaccinations rates, suggesting heteroskedasticity. Generalized linear models which allow for the response variable to have an error distribution other than the normal distribution can fix those issues. So this will be our topic today. Preliminaries12345678910import numpy as npimport pandas as pdimport matplotlib.pyplot as pltimport matplotlib.colorsimport seaborn as snsimport statsmodels.api as smfrom statsmodels.discrete.discrete_model import Poissonfrom statsmodels.discrete.discrete_model import NegativeBinomialimport sysimport covid_analysis as covid The data I use includes hospital admissions and vaccinations from the CDC website, and the most up-to-date populations by state from the US Census Bureau. 1data_dir=&quot;../data&quot; 1234567891011hospitalizations=pd.read_csv(f&quot;{data_dir}/covid_hospitalizations.csv&quot;,parse_dates=[&quot;date&quot;])vaccinations=pd.read_csv(f&quot;{data_dir}/covid_vaccinations.csv&quot;,parse_dates=[&quot;date&quot;])population=pd.read_csv(f&quot;{data_dir}/population.csv&quot;)event=&quot;admissions&quot;data=hospitalizationsdata=data.merge(vaccinations,on=[&quot;date&quot;,&quot;state&quot;])data=data.merge(population,on=&quot;state&quot;)data[&quot;vaccinated&quot;]=data[&quot;vaccinated&quot;]/data[&quot;population&quot;]data.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } date state used_beds admissions vaccinated population 0 2021-06-10 MT 66.0 16.0 0.397597 1080577 1 2021-06-21 MT 57.0 13.0 0.411614 1080577 2 2021-08-05 MT 149.0 23.0 0.440153 1080577 3 2021-07-15 MT 62.0 9.0 0.431722 1080577 4 2021-08-21 MT 224.0 59.0 0.449196 1080577 Poisson RegressionLike what we did with the linear regression, we can choose either to use the Poisson Regression model from statsmodels, or the GLM from the same lib. The two will give us the exact same estimations and statistics. Model DefinitionWe have data from multiple dates, so it makes sense to incorporate Time as a extra variable. \\begin{eqnarray} &amp;\\eta_i &amp;=c + \\beta_1 x_i + \\beta_2 t_i + \\beta_3 t_i^2 \\newline \\mathbb{E}(y_i|x_i) =&amp; \\lambda_iP_i &amp;= e^{\\eta_i }P_i \\newline &amp;p(y_i|x_i) &amp; = \\text{Poisson}(y;\\lambda_i P_i)\\end{eqnarray}where Each observation $i$ represents a single state at one particular date. $y_i$ is the number of hospital admissions on that state. $x_i$ is the percentage of the state population fully vaccinated that state. $t_i$ is the number of days elapsed since the beginning of the training period. $P_i$ is the population of the state. the parameters $c$ and $\\beta_1,\\beta_2,\\beta_3$ will be fitted to the observed data to maximize agreement with the model We including coefficients for $t$ and $t^2$ the dependence of the admission rate on time can have some curvature and does not need to be linear. This can be fitter as a linear model where the inputs are $x_i$, $t_i$ and $t_i^2$ First we select the training period: 1234start_train=&quot;2021-07-15&quot;end_train=&quot;2021-08-15&quot;train_data=data[(data[&apos;date&apos;] &gt;= start_train) &amp; (data[&apos;date&apos;] &lt;= end_train)].copy()date0=train_data[&quot;date&quot;].min() Then a 7-day testing period: 123test_period=7 # daystest_end=pd.Timestamp(end_train)+pd.DateOffset(days=test_period)test_data=data[(data[&quot;date&quot;]&gt;end_train) &amp; (data[&quot;date&quot;]&lt;=test_end)].copy() 1234X_train,P_train=covid.define_variables(train_data,date0)Y_train=train_data[event]X_test,P_test=covid.define_variables(test_data,date0)Y_test=test_data[event] Fit the model to the train data, 123mod=Poisson(Y_train,X_train,exposure=P_train.values)res=mod.fit()res.summary() Optimization terminated successfully. Current function value: 32.115992 Iterations 6 Poisson Regression Results Dep. Variable: admissions No. Observations: 1632 Model: Poisson Df Residuals: 1628 Method: MLE Df Model: 3 Date: Sun, 26 Sep 2021 Pseudo R-squ.: 0.4493 Time: 19:44:49 Log-Likelihood: -52413. converged: True LL-Null: -95175. Covariance Type: nonrobust LLR p-value: 0.000 coef std err z P&gt;|z| [0.025 0.975] vaccinated -6.7589 0.030 -225.710 0.000 -6.818 -6.700 T 0.0750 0.001 75.492 0.000 0.073 0.077 T2 -0.0009 2.84e-05 -31.349 0.000 -0.001 -0.001 const -8.4423 0.015 -547.566 0.000 -8.473 -8.412 123mod=sm.GLM(Y_train,X_train,exposure=P_train,family=sm.families.Poisson(sm.families.links.log()))glm_res=mod.fit()glm_res.summary() Generalized Linear Model Regression Results Dep. Variable: admissions No. Observations: 1632 Model: GLM Df Residuals: 1628 Model Family: Poisson Df Model: 3 Link Function: log Scale: 1.0000 Method: IRLS Log-Likelihood: -52413. Date: Sun, 26 Sep 2021 Deviance: 95443. Time: 19:44:50 Pearson chi2: 1.26e+05 No. Iterations: 6 Covariance Type: nonrobust coef std err z P&gt;|z| [0.025 0.975] vaccinated -6.7589 0.030 -225.710 0.000 -6.818 -6.700 T 0.0750 0.001 75.492 0.000 0.073 0.077 T2 -0.0009 2.84e-05 -31.349 0.000 -0.001 -0.001 const -8.4423 0.015 -547.566 0.000 -8.473 -8.412 Poisson regression is fit by maximum likelihood, there are several choices of Pseudo R-squareHere, what statsmodels implements for poisson regression is McFadden $R_{McF}^2$ that is defined as ratio of log likelihood for the fitted model and log likelihood of a model fitted to just a constant. 12# Pseudo R-squ McFres.prsquared 0.4492953669633617 1res.llf, res.llnull (-52413.29834953645, -95174.97258108124) 11-res.llf/res.llnull 0.4492953669633617 Certainly, we can implement other measures of the Pseudo R-square ourselves. Here I did $R_{L}^2$, which is deviance-based. It’s the most analogous index to the squared multiple correlations in linear regression. We will stick to it in our analysis from now on. 12def poisson_deviance(y,y_pred): return 2*np.sum(y*np.log(np.maximum(y,1e-12)/y_pred)-(y-y_pred)) 12345678mu_train=Y_train.sum()/P_train.sum()Y_pred=glm_res.predict(X_train,exposure=P_train)deviance=poisson_deviance(Y_train,Y_pred)null_deviance=poisson_deviance(Y_train,mu_train*P_train)R2=1-deviance/null_deviance# Pseudo R-squR2 0.4725915412837366 If you don’t want bother yourself with the mathematical formulas, here’s a simpler way (my favorite as the laziest person ever). Just fit the model with a constant, and take the log-likelihood/deviance as your null case. 1234mod=sm.GLM(Y_train,X_train[&apos;const&apos;],exposure=P_train,family=sm.families.Poisson(sm.families.links.log()))res_null=mod.fit()# deviance for a const fit modelres_null.deviance 180966.73552551522 12# null deviance from our math formulanull_deviance 180966.7355255152 After all, the R-squared we have for now is 0.473. Next let’s check on how the model predicts. Extrapolation of Time trendsRemember what we saw last time with the linear model? Its projection was a straight line. But the poisson model is telling us a different story, that the admissions will flatten out assuming the current vaccination rate. Well, it is onto something, isn’t it? 1covid.plot_time_extrapolation(train_data,glm_res,event,date0) In-sample model FitTo better evaluate the model fit, we visualize the real observations and model predictions. Here’s some snapshots of the fit for a few selected dates on the training period. As we can see, curves from the poisson regression model fits the data much better than the linear model, indicating the correlation between admissions and vaccinations are not linear. 1covid.plot_time_facets(train_data,glm_res,event,date0) But is this enough? Certainly not, let’s verify the deviance residuals by visualizing them, 12345678910111213141516171819202122232425262728def plot_deviance_residuals(data,event,res): P=data[&quot;population&quot;] # Create two subplots and unpack the output array immediately f, (ax1, ax2) = plt.subplots(1, 2,sharex=True,figsize=(16,4)) norm=matplotlib.colors.LogNorm(vmin=100_000, vmax=P.max(), clip=False) m=ax1.scatter(data[&quot;vaccinated&quot;],data[event]/data[&quot;population&quot;]*100_000, c=P,norm=norm,cmap=&quot;Blues&quot;,label=event) plt.colorbar(m,label=&quot;State Population&quot;) x=np.linspace(0.3,0.75,201) x= pd.DataFrame({&apos;vaccinated&apos;:x, &apos;T&apos;:31, &apos;T2&apos;:31*31}) x[&quot;const&quot;]=np.ones(201) y_pred=res.predict(x) ax1.plot(x.vaccinated, y_pred*100_000,&quot;k--&quot;,label=&quot;predicted&quot;) idx = data.index ax2.scatter(data[&quot;vaccinated&quot;], res.resid_deviance[idx], c=P,norm=norm,cmap=&quot;Blues&quot;,label=event) ax2.set_title(&quot;Residuals&quot;) ax2.set_xlabel(&quot;vaccination&quot;) ax2.set_ylabel(&quot;residual&quot;) # select outlier state outlier_idx=np.argpartition(np.array(res.resid_deviance[idx]), -2)[-2:] for i in outlier_idx: outlier=data.iloc[i] ax1.annotate(outlier[&quot;state&quot;],(outlier[&quot;vaccinated&quot;]+0.01,outlier[event]/outlier[&quot;population&quot;]*100_000)) ax2.annotate(outlier[&quot;state&quot;],(outlier[&quot;vaccinated&quot;]+0.01,res.resid_deviance[idx].iloc[i])) The date I choose is Aug.15, 123offset=9 interested_date=data[&quot;date&quot;].max()-pd.offsets.Day(offset)interested_data=data[data[&quot;date&quot;]==interested_date] 1plot_deviance_residuals(interested_data,event,glm_res) The deviance residuals are defined as the signed square roots of the unit deviances, representing the contributions of individual samples to the deviance. The deviance indicates the extent to which the likelihood of the saturated model exceeds the likelihood of the proposed model. If the proposed model has a good fit, the deviance will be small. If the proposed model has a bad fit, the deviance will be high. Thus, the deviance residuals are analogous to the conventional residuals: when they are squared, we obtain the sum of squares that we use for assessing the fit of the model. However, while the sum of squares is the residual sum of squares for linear models, for GLMs, this is the deviance. 123deviance=poisson_deviance(Y_train,Y_pred)sum_of_squared_deviance_residuals = sum(glm_res.resid_deviance**2)deviance, sum_of_squared_deviance_residuals (95443.38706242564, 95443.38706242583) In a properly specified model, the deviance is approximately chi-square distributed with n-k-1 degrees of freedom, and the deviance residuals would be independent, standard normal random variables, i.e., ~ norm(0,1). So we would expect the residuals to be mostly in range of -1 to 1, and rarely fall outside the ± 3 limits. However, the model residuals are 10 times larger than expected and we say there is overdispersion. If overdispersion is present in a dataset, the estimated standard errors and test statistics, the overall goodness-of-fit will be distorted and adjustments must be made. A more appropriate model will be a Negative Binomial regression. Model PredictionWe will now use the model without recalibrating to make predictions about admission rates on new data. This is out of sample evaluation. It is the only way to make sure the model really works. Out of Sample $R^2$1234567test_admission_mean=Y_test.sum()/P_test.sum()Y_pred=glm_res.predict(X_test,exposure=P_test)null_deviance_test=poisson_deviance(Y_test,P_test*test_admission_mean)deviance_test=poisson_deviance(Y_test,Y_pred)R2 = 1-deviance_test/null_deviance_testR2 0.371917369051069 Out of sample $R^2$ is worse than for the in sample (approx. 0.473). I think we’ll all agree, predicting the future is hard. Negative Binomial RegressionModel DefinitionWe have data from multiple dates, so it makes sense to incorporate Time as a extra variable. \\begin{eqnarray} &amp;\\eta_i &amp;=c + \\beta_1 x_i + \\beta_2 t_i + \\beta_3 t_i^2 \\newline \\mathbb{E}(y_i|x_i) =&amp; \\lambda_i P_i &amp;= e^{\\eta_i } P_i \\newline &amp;p(y_i|x_i) &amp; = \\text{NB}(y_i; \\lambda_i P_i, \\alpha)\\end{eqnarray}where Each observation $i$ represents a single state at one particular date. $y_i$ is the number of hospital admissions on that state. $x_i$ is the percentage of the state population fully vaccinated that state. $t_i$ is the number of days elapsed since the beginning of the training period. $P_i$ is the population of the state. the parameters $c$ and $\\beta_1,\\beta_2,\\beta_3$ will be fitted to the observed data to maximize agreement with the model We including coefficients for $t$ and $t^2$ the dependence of the admission rate on time can have some curvature and does not need to be linear. This can be fitter as a linear model where the inputs are $x_i$, $t_i$ and $t_i^2$ Variance Comparison to Poisson ModelIntroducing a free additional parameter $\\alpha$ give more accurate models than simple parametric models like the Poisson distribution by allowing the mean and variance to be different, unlike the Poisson. The negative binomial distribution has a variance $\\hat{y}+\\hat{y}^2\\alpha$ . This can make the distribution a useful overdispersed alternative to the Poisson distribution. As we can see, with an extra parameter $\\alpha$ to control the variance, the expected variance of observations is larger with the Negative Binomial Model than with the Poisson model. 1234567alpha = 0.5y_hat=np.linspace(0,10,201)plt.plot(y_hat,y_hat,&quot;k--&quot;,label=&quot;Poisson&quot;)plt.plot(y_hat,y_hat+alpha*y_hat**2,&quot;k-&quot;,linewidth=3,label=&quot;Negative Binomial&quot;)plt.xlabel(r&quot;$\\hat{y}$&quot;)plt.ylabel(r&quot;Var($y|\\hat{y}$)&quot;)plt.legend() Choosing $\\alpha$Negative Binomial is a GLM model with an extra parameter $\\alpha$ that we must choose by maximizing the log likelihood. The train/test data we fit to the Negative Binomial Regression is the same as what we create at start. 123456789101112lls=[]alphas=np.linspace(0.1,0.5,500)for alpha in alphas: glm_model=sm.GLM(Y_train,X_train,exposure=P_train,family=sm.families.NegativeBinomial(sm.families.links.log(),alpha=alpha)) glm_res=glm_model.fit() ll=glm_res.llf lls.append(ll)plt.semilogx(alphas,lls) plt.xlabel(r&quot;$\\alpha$&quot;)plt.ylabel(&quot;Log Likelihood&quot;)alpha=alphas[np.argmax(lls)]print(&quot;alpha&quot;,alpha) alpha 0.3004008016032064 The best alpha is 0.3004 given a run of 500 times. Or easier, we can go for the Negative Regression model from statsmodels, which can do the dirty work, optimizing alpha for us. 1234#mod=sm.GLM(Y_train,X_train,exposure=P_train,family=nb)mod=NegativeBinomial(Y_train,X_train,exposure=P_train)res=mod.fit(method=&quot;lbfgs&quot;)res.summary() /opt/anaconda3/lib/python3.7/site-packages/statsmodels/discrete/discrete_model.py:2642: RuntimeWarning: divide by zero encountered in log llf = coeff + size*np.log(prob) + endog*np.log(1-prob) /opt/anaconda3/lib/python3.7/site-packages/statsmodels/discrete/discrete_model.py:2642: RuntimeWarning: invalid value encountered in multiply llf = coeff + size*np.log(prob) + endog*np.log(1-prob) /opt/anaconda3/lib/python3.7/site-packages/statsmodels/base/model.py:568: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals ConvergenceWarning) NegativeBinomial Regression Results Dep. Variable: admissions No. Observations: 1632 Model: NegativeBinomial Df Residuals: 1628 Method: MLE Df Model: 3 Date: Sun, 26 Sep 2021 Pseudo R-squ.: 0.08843 Time: 20:24:21 Log-Likelihood: -7946.2 converged: False LL-Null: -8717.1 Covariance Type: nonrobust LLR p-value: 0.000 coef std err z P&gt;|z| [0.025 0.975] vaccinated -6.7968 0.179 -38.019 0.000 -7.147 -6.446 T 0.0525 0.006 8.645 0.000 0.041 0.064 T2 -0.0002 0.000 -1.262 0.207 -0.001 0.000 const -8.4877 0.092 -92.109 0.000 -8.668 -8.307 alpha 0.3025 0.011 26.621 0.000 0.280 0.325 The alpha values are close, 123p=res.paramsnb=sm.families.NegativeBinomial(alpha=p[&quot;alpha&quot;])nb.alpha 0.3024936677385029 Compared to Poisson:- The regression coefficients are very similar.- The confidence intervals are much wider.- The t statistics are smaller because we assume a larger variance of residuals. In summary, switching from Poisson to Negative Binominal yields stable coefficient estimates, but a higher chance for the null hepothesis to be rejected and more reliable confidence intervals. The statsmodel.family.NegativeBinomial object knows how to compute deviance, and we use it to calculate the R squared, 123456mu_train=Y_train.sum()/P_train.sum()Y_pred=res.predict(X_train,exposure=P_train)deviance=nb.deviance(Y_train,Y_pred)null_deviance=nb.deviance(Y_train,mu_train*P_train)R2=1-deviance/null_devianceR2 0.5937123540474427 In-sample model Fit1covid.plot_time_facets(train_data,res,event,date0) Then most excitingly, let’s check out the deviance residuals again. The residuals are now well behaved, randomly fall into the range of -1 to 1. Though FL, KY and a few other states seem to be outliers. But no worries, we will address them in our next blog, using a more advanced method - mixed models. 1plot_deviance_residuals(interested_data,event,glm_res) Model PredictionFinally, always take a look at the out-of-sample fit. Though the R squared is not as good as the in-sample, but there’s still an improvement comparing to Poisson regression and the linear regressions. 1234567test_admission_mean=Y_test.sum()/P_test.sum()Y_pred=res.predict(X_test,exposure=P_test)null_deviance_test=nb.deviance(Y_test,P_test*test_admission_mean)deviance_test=nb.deviance(Y_test,Y_pred)R2 = 1-deviance_test/null_deviance_testR2 0.4595499732848517","link":"/2021/09/26/projectcovidGLM/"},{"title":"Project Covid Admissions Based on Vaccine Progress, Part 3 -- Mixed-effect models and Bayesian Inference","text":"In the last two blogs, we went through linear regression and generalized linear regression in search of the best solution to the covid data by relaxing the assumptions of linear regression and OLS. Based on what have done so far, our conclusion is negative binomial regression fit the data best. However, still, we have this problem that for some states with high vaccination rates, the increasing cases are over-estimated, and a few state such as FL and KY, are more like outliers to the model. Today, we are going to fixed these issues with the mixed-effects model, specifically, the generalized linear mixed-effects model(GLMM), which is very popular in biostatistics and econometrics, especially useful for economists when modeling cross-country macro data. A mixed-effects model contains both fixed effects and random effects. They are particularly useful in settings where measurements are made on clusters of related statistical units. There are several ways people usually choose to deal with this type of inherently “lumpy” data. 1. Pooled Data RegressionOne can run a regular regression on $y$ versus $x$ ignoring the group information, like what we have done in the last two blogs. It works but: It throws away useful information. Linear Regression assumes observations are independent here they are not. The $\\alpha$ and $\\beta$ coefficients are unbiased estimates (not bad). The confidence intervals will be too optimistic. 2. Unpooled Data RegressionInstead one can run a regression for each of the groups. One needs to estimate too many coefficients (51 $\\alpha$s and 51 $\\beta$s in our case). This results in unbiased, but very noisy estimates if some groups have few observations. 3. Pooled Slope, Unpooled InterceptWe can instead fit a regression with a dummy variable for each group. $$ y_i = \\alpha + \\beta x + \\rho^T Z_i$$where $\\rho$ is now a vector with as many coefficients as groups, and $Z$ is a vector of dummies, one column per group (we must leave one group out to avoid co-linearity of Z). This method: Pools information for the slope $\\beta$. Treats each group intercept independently. If some group $g$ has few examples $\\rho_g$ will be noisy. 4. Penalized Linear RegressionTo pool over the different groups, we can introduce a regression penalty to penalize model coefficients to reduce the model degrees of freedom. Theoretically, the correct penalty is proportional to the ration of variances for the observations $\\sigma_Y^2$ and the random effects $\\sigma_\\rho^2$. Because there is no penalty for $\\alpha$, the mean over all the population will be matched exactly. 5. Linear Mixed Effect ModelFor people who are familiar with penalized regressions and regularizations like lasso and rigid, the estimation of mixed models would be very straightforward as essentially, they are the same. Same result can be obtained with less work by fitting to a Linear Mixed Effect Model with statsmodels.mixed_linear module. 6. Generalized Linear Mixed-effects Model (GLMM)Today, as we already know that negative binomial distribution fits the covid data better, we will skip the linear mixed model and start with the non-linear GLMM. Since the model is relatively complex, we will be using a much cooler statistical inference method – Bayesian. Preliminaries12345678910import numpy as npimport pandas as pdimport matplotlibimport matplotlib.pyplot as pltimport matplotlib.cm as cmimport seaborn as snsimport pymc3 as pmimport arviz as azimport sysimport covid_analysis as covid As usual, let’s prepare the data. Hospital admissions and vaccinations are from the CDC website, and the most up-to-date populations by state are from the US Census Bureau. 1data_dir=&quot;../data&quot; 1234567891011hospitalizations=pd.read_csv(f&quot;{data_dir}/covid_hospitalizations.csv&quot;,parse_dates=[&quot;date&quot;])vaccinations=pd.read_csv(f&quot;{data_dir}/covid_vaccinations.csv&quot;,parse_dates=[&quot;date&quot;])population=pd.read_csv(f&quot;{data_dir}/population.csv&quot;)event=&quot;admissions&quot;data=hospitalizationsdata=data.merge(vaccinations,on=[&quot;date&quot;,&quot;state&quot;])data=data.merge(population,on=&quot;state&quot;)data[&quot;vaccinated&quot;]=data[&quot;vaccinated&quot;]/data[&quot;population&quot;]data.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } date state used_beds admissions vaccinated population 0 2021-06-10 MT 66.0 16.0 0.397597 1080577 1 2021-06-21 MT 57.0 13.0 0.411614 1080577 2 2021-08-05 MT 149.0 23.0 0.440153 1080577 3 2021-07-15 MT 62.0 9.0 0.431722 1080577 4 2021-08-21 MT 224.0 59.0 0.449196 1080577 Split the train and test data, 123456789# train datesstart_train=&quot;2021-07-15&quot;end_train=&quot;2021-08-15&quot;train_data=data[(data[&apos;date&apos;] &gt;= start_train) &amp; (data[&apos;date&apos;] &lt;= end_train)].copy()date0=train_data[&quot;date&quot;].min()# test datestest_period=7 # daystest_end=pd.Timestamp(end_train)+pd.DateOffset(days=test_period)test_data=data[(data[&quot;date&quot;]&gt;end_train) &amp; (data[&quot;date&quot;]&lt;=test_end)].copy() Here we write a function to generate our desired matrix. This time, we will need an extra step to label data by state. 12345678910def define_variables(data,date0): N=len(data) T=(data[&quot;date&quot;]-date0).dt.days/30 X=pd.DataFrame({&quot;const&quot;:np.ones(N)}) X[&quot;vaccinated&quot;]=data[&quot;vaccinated&quot;].values X[&quot;T&quot;]=T.values X[&quot;T2&quot;]=T.values**2 P=data[&quot;population&quot;].values Z=pd.get_dummies(data[&quot;state&quot;]) return X,Z,P 123X_train,Z_train,P_train=define_variables(train_data,date0)Y_train=train_data[event]G_train=Z_train.values.argmax(axis=1) We have 51 regions in total, 123# number of states/regionsK = G_train.max()+1K 51 123# all states/regionsstates=Z_train.columnsstates Index([&apos;AK&apos;, &apos;AL&apos;, &apos;AR&apos;, &apos;AZ&apos;, &apos;CA&apos;, &apos;CO&apos;, &apos;CT&apos;, &apos;DC&apos;, &apos;DE&apos;, &apos;FL&apos;, &apos;GA&apos;, &apos;HI&apos;, &apos;IA&apos;, &apos;ID&apos;, &apos;IL&apos;, &apos;IN&apos;, &apos;KS&apos;, &apos;KY&apos;, &apos;LA&apos;, &apos;MA&apos;, &apos;MD&apos;, &apos;ME&apos;, &apos;MI&apos;, &apos;MN&apos;, &apos;MO&apos;, &apos;MS&apos;, &apos;MT&apos;, &apos;NC&apos;, &apos;ND&apos;, &apos;NE&apos;, &apos;NH&apos;, &apos;NJ&apos;, &apos;NM&apos;, &apos;NV&apos;, &apos;NY&apos;, &apos;OH&apos;, &apos;OK&apos;, &apos;OR&apos;, &apos;PA&apos;, &apos;RI&apos;, &apos;SC&apos;, &apos;SD&apos;, &apos;TN&apos;, &apos;TX&apos;, &apos;UT&apos;, &apos;VA&apos;, &apos;VT&apos;, &apos;WA&apos;, &apos;WI&apos;, &apos;WV&apos;, &apos;WY&apos;], dtype=&apos;object&apos;) Mixed Negative Binomial Regression ModelModel DefinitionWe have data from multiple dates, so it makes sense to incorporate Time as a extra variable. \\begin{eqnarray} \\eta_{i,j} =&amp; \\beta_0 + \\beta_1 x_{i,j} + \\beta_2 t_{j} + \\beta_3 t_{j}^2 + \\rho_i\\newline \\rho_i \\sim &amp; N(0,\\sigma_{\\rho}^2) \\newline \\lambda_{i,j} =&amp; e^{\\eta_{i,j}} \\newline y_{i,j} \\sim &amp; \\text{NB}(y;\\lambda_{i,j} P_i,\\alpha)\\end{eqnarray}with priors\\begin{eqnarray} \\beta_0 \\sim &amp; N( \\bar{y},1) \\newline \\beta_i \\sim &amp; N(0,1) \\newline \\log \\sigma_\\rho \\sim &amp; N(0,1) \\newline \\log \\alpha \\sim &amp; N(0,1)\\newline\\end{eqnarray}where Each index $i$ represents a state in the US. Each index $j$ represents a particular date. $y_{i,j}$ is the number of hospital admissions on state $i$ at date $j$. the random effects (random intercept) $\\rho_i$ for different states $i$ are independent and have a normal distribution. It will be fit to the data. $x_{i,j}$ is the percentage of the state population fully vaccinated on state $i$ at date $j$. $t_j$ is the number of days elapsed since the beginning of the training period. $P_i$ is the population of state $i$. $\\bar{y}$ is log of the average per person incidence on the event in the US Population. the parameters and $\\beta_1,\\beta_2,\\beta_3$ will be fitted to the observed data to maximize agreement with the model. the parameter $\\sigma_\\beta$ is our prior uncertainty about the value of $\\beta_i$, it is set to 1. To estimate the model coefficients, we will go for full Bayesian. Bayesian ApproachUnlike the classical frequentist methods, in Bayesian analysis, a parameter is summarized by an entire distribution of values instead of one fixed value, and it provides a natural and principled way of combining prior information or expert knowledge with the data observed. Both Bayesian methods and classical methods have advantages and disadvantages, and there are some similarities. When the sample size is large, Bayesian inference often provides results for parametric models that are very similar to the results produced by frequentist methods. Some advantages to using Bayesian analysis include the following: It provides a natural and principled way of combining prior information with data, within a solid decision theoretical framework. You can incorporate past information about a parameter and form a prior distribution for future analysis. It provides interpretable answers based on parameter distributions, such as “the true parameter has a probability of 0.95 of falling in a 95% credible interval.” It provides a convenient setting for a wide range of complex models, such as the GLMM model we are trying to build but not easy to get an analytical solution. MCMC, along with other numerical methods, makes computations tractable for virtually all parametric models. The disadvantages of Bayesian are quite obvious as well: It does not tell you how to select a prior. There is no correct way to choose a prior. Bayesian inferences require skills to translate subjective prior beliefs into a mathematically formulated prior. It often comes with a high computational cost, especially in models with a large number of parameters. To solve our model with Bayesian approach, we have to specify the distributions of the parameters/variables. The good thing is that there are convenient functions provided by pymc3. 123Y_bar=np.log(Y_train.sum()/P_train.sum())beta0=np.array([Y_bar,0,0,0,])sigma_beta=np.array([1,1,1,1]) I choose $N(0,1)$ for $\\beta$s , and $lognormal(0,1)$ for $\\sigma_\\rho$s just to speed up the convergence. 123456789101112with pm.Model() as mod: beta=pm.Normal(&quot;beta&quot;,mu=beta0, sigma=sigma_beta, shape=(len(beta0)) ) lsigma_group=pm.Normal(&quot;lsigma_group&quot;,mu=0,sigma=1) sigma_group=pm.Deterministic(&quot;sigma_group&quot;,np.exp(lsigma_group)) rho=pm.Normal(&quot;rho&quot;,mu=0.0,sigma=sigma_group,shape=(K)) # a vector of K values eta=pm.math.dot(X_train.values,beta)+rho[G_train] y_hat=pm.Deterministic(&quot;eta&quot;,pm.math.exp(eta)) a=pm.Lognormal(&quot;alpha&quot;,mu=0,sigma=1) y=pm.NegativeBinomial(&quot;y&quot;,mu=y_hat*P_train,alpha=a,observed=Y_train) Then we can start the Monte Carlo sampling process. 4 chains, of which 1000 samples are generated. That means there will be 4000 samples for each of the parameters. Be patient, the computation can take a while. (Honestly, sometimes HOURS if using personal laptops) Monte Carlo Sampling12with mod: trace = pm.sample(1000,chains=4,cores=4, tune=500) Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (4 chains in 4 jobs) NUTS: [alpha, rho, lsigma_group, beta] Sampling 4 chains, 0 divergences: 100%|█████████████████████████████████████████| 6000/6000 [04:36&lt;00:00, 21.72draws/s] The acceptance probability does not match the target. It is 0.8915155058428972, but should be close to 0.8. Try to increase the number of tuning steps. The rhat statistic is larger than 1.05 for some parameters. This indicates slight problems during sampling. The estimated number of effective samples is smaller than 200 for some parameters. Regression CoefficientsOne of the advantages of Bayesian is that a parameter is summarized by a distribution of values instead of a fixed value, which definately helps us get a better understanding of the things going on, like how high the uncertainty is, how much confidence the model has in the estimations. 1234with mod: summary=az.summary(trace, var_names=[&quot;beta&quot;,&quot;sigma_group&quot;], fmt=&quot;wide&quot;,round_to=2)summary.index=[&quot;intercept&quot;,&quot;vaccinated&quot;,&quot;T&quot;,&quot;T2&quot;,&quot;sigma_group&quot;]summary .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } mean sd hdi_3% hdi_97% mcse_mean mcse_sd ess_mean ess_sd ess_bulk ess_tail r_hat intercept -9.42 0.31 -10.06 -8.84 0.02 0.02 160.54 160.54 142.03 351.53 1.04 vaccinated -5.36 0.66 -6.57 -4.02 0.05 0.04 149.80 119.62 150.75 192.52 1.04 T 1.79 0.07 1.66 1.91 0.00 0.00 749.07 749.07 751.34 1212.85 1.01 T2 -0.37 0.06 -0.47 -0.25 0.00 0.00 916.41 916.41 917.97 1287.44 1.01 sigma_group 0.55 0.06 0.43 0.66 0.00 0.00 1010.83 1010.83 1001.36 1747.82 1.00 We can see the distribution of parameters very clearly, 12with mod: az.plot_trace(trace,var_names=[&quot;beta&quot;,&quot;sigma_group&quot;,&quot;alpha&quot;]) Random EffectsWe also get a distribution for the random, state specific, effects. 123with mod: random_effects=az.summary(trace, var_names=[&quot;rho&quot;], fmt=&quot;wide&quot;,round_to=2)random_effects.index=states 1random_effects.head(9) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } mean sd hdi_3% hdi_97% mcse_mean mcse_sd ess_mean ess_sd ess_bulk ess_tail r_hat AK 0.03 0.09 -0.15 0.20 0.01 0.01 63.60 63.60 65.35 339.69 1.05 AL 0.55 0.12 0.33 0.77 0.01 0.01 88.48 88.48 86.71 428.54 1.04 AR 0.71 0.11 0.51 0.92 0.01 0.01 74.26 74.26 74.61 435.70 1.05 AZ 0.12 0.08 -0.04 0.27 0.01 0.01 49.42 49.42 51.50 317.49 1.06 CA 0.34 0.09 0.19 0.51 0.01 0.01 47.83 43.42 46.91 344.41 1.07 CO 0.16 0.09 -0.01 0.34 0.01 0.01 45.42 42.62 44.54 297.58 1.07 CT 0.10 0.13 -0.18 0.33 0.02 0.01 76.68 76.68 72.85 279.41 1.05 DC -0.31 0.12 -0.52 -0.09 0.01 0.01 75.16 75.16 74.36 724.17 1.04 DE -0.48 0.10 -0.67 -0.28 0.01 0.01 74.12 74.12 72.69 614.42 1.05 Comparing to the other states, states such as FL, KY which are seen as outliers in our previous models have much larger random effects. 1random_effects.loc[[&apos;FL&apos;,&apos;KY&apos;]] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } mean sd hdi_3% hdi_97% mcse_mean mcse_sd ess_mean ess_sd ess_bulk ess_tail r_hat FL 1.64 0.08 1.48 1.78 0.01 0.01 44.39 43.59 45.16 292.35 1.07 KY 0.99 0.08 0.85 1.16 0.01 0.01 45.57 44.32 47.46 324.94 1.06 In Sample PredictionsNow, let’s make some predictions and see how the mixed model works. 12345def select_data(data,periods): dates=data[&quot;date&quot;].unique() selected_dates=covid.select_dates(dates,periods) used_data=data.merge(selected_dates,on=&quot;date&quot;) return used_data.groupby(&quot;date&quot;) 1234567891011121314151617181920212223242526def plot_result(data,norm,ax): P=data[&quot;population&quot;] m=ax.scatter(data[&quot;vaccinated&quot;],data[event]/data[&quot;population&quot;]*100_000, c=P,norm=norm,cmap=&quot;Greys&quot;,label=event) ax.errorbar(data[&quot;vaccinated&quot;],data[&quot;y_pred&quot;],yerr=data[&quot;y_std&quot;],fmt=&quot;D&quot;,alpha=0.25, label=&quot;predicted&quot;)#,c=P,norm=norm,cmap=&quot;Blues&quot;,label=&quot;predicted&quot;) ax.set_xlabel(&quot;vaccinated&quot;) ax.set_ylabel(f&quot;{event} per 100k&quot;) ax.legend() return mdef plot_time_facets(data,event): # Create two subplots and unpack the output array immediately fig, axes = plt.subplots(2, 2, figsize=(14,12),sharey=True,sharex=True) P=data[&quot;population&quot;] norm=matplotlib.colors.LogNorm(vmin=100_000, vmax=P.max(), clip=False) count=0 for date,group in select_data(data,4): col=count %2 row=count //2 ax=axes[row][col] ax.set_title(date.strftime(&quot;%Y-%m-%d&quot;)) im=plot_result(group,norm,ax) count+=1 cbar = fig.colorbar(im, ax=axes.ravel().tolist(), shrink=0.5,label=&quot;population&quot;) 123with pm.Model() as mod: y_pred=pm.NegativeBinomial(&quot;y_pred&quot;,mu=y_hat*100_000,alpha=a,shape=(len(X_train))) posterior_predictive = pm.sample_posterior_predictive(trace, var_names=[&quot;y_pred&quot;]) 100%|█████████████████████████████████████████████████████████████████████████████| 4000/4000 [00:05&lt;00:00, 798.51it/s] Remember, there are 4000 sample predictions for each of the records, I won’t plot out all of them, but will do their means with one standard deviation of uncertainty. The error bars would make prefect graphical representations. 12predicted=posterior_predictive[&quot;y_pred&quot;]predicted.shape,train_data.shape ((4000, 1632), (1632, 6)) 123456y_pred_mean=predicted.mean(axis=0)y_pred_std=predicted.std(axis=0)data=train_data.copy()data[&quot;y_pred&quot;]=y_pred_meandata[&quot;y_std&quot;]=y_pred_std Looks like the predictions are closer to the actuals since we incorporate the random effects. Hooray! 1plot_time_facets(data,event) Random InterceptsLet’s take a further look by state to get a better understanding of what I meant by random effects/intercepts. Below is all the observed data points marked by state. We can see that roughly the slopes of states are close, while the intercepts can vary a lot. The slopes are upward due to the passage of time, remember time is our explanatory variable as well, but we can only plot two dimensions. 12345678fig, ax = plt.subplots()P=data[&quot;population&quot;]norm=matplotlib.colors.LogNorm(vmin=100_000, vmax=P.max(), clip=False)m=ax.scatter(data[&quot;vaccinated&quot;],data[event]/data[&quot;population&quot;]*100_000, c=P,norm=norm,cmap=&quot;Greys&quot;,label=event, s = 5)ax.set_xlabel(&quot;vaccinated&quot;)ax.set_ylabel(f&quot;{event} per 100k by state&quot;)ax.legend() Let’s select three states in the middle, and plot their predictions against actuals. 12345678910111213141516171819202122def plot_states(data, states, var): data = data[data[&apos;state&apos;].isin(states)] data = data.sort_values(&apos;vaccinated&apos;) x = data[var] y = data[&quot;y_pred&quot;] std = data[&quot;y_std&quot;] cmp=cm.get_cmap(&quot;tab10&quot;) plt.xlim([0.4475,0.466]) plt.scatter(x,y, c=&apos;black&apos;,s=15) colors = np.array([&apos;tab:blue&apos;,&apos;tab:green&apos;,&apos;tab:orange&apos;,&apos;tab:red&apos;]) for i in range(len(states)): s = states[i] ds = data[data[&apos;state&apos;]==s] x = ds[var] y = ds[&quot;y_pred&quot;] std = ds[&quot;y_std&quot;] color = colors[i] plt.plot(x,y,&apos;k-&apos;,color=color,linewidth=2.0, label=f&quot;predicted {s}&quot;) plt.fill_between(x,y+std,y-std,alpha=0.15,color=color) plt.xlabel(&quot;vaccinated&quot;) plt.ylabel(f&quot;{event} per 100k by state&quot;) plt.legend(loc=&quot;upper left&quot;) This shows clearly how the mixed model leverages all the data points to estimate the correlation(slope) between admissions and vaccinations, but at the same time allows states to have different starts(intercept) due to fundamental differences which we consider are random. 1plot_states(data,[&apos;KY&apos;,&apos;AK&apos;,&apos;KS&apos;],&quot;vaccinated&quot;) VarianceThe Bayesian approach gives the distributions of the predictions, allow us to plot the confidence intervals (one unit of the standard deviation). As we can see, the variance of predictions is positively correlated with predicted admissions, meaning that there’s more uncertainty as the predicted value goes higher. Using GLMM based on the negative binomial distribution allows the magnitude of the variance to be a function of the predicted value, i.e., it relaxes the assumption of constant variance. 123456789def plot_state(data, state, color): data = data[data[&apos;state&apos;]==state] fig, ax = plt.subplots() ax.scatter(data[&quot;vaccinated&quot;],data[event]/data[&quot;population&quot;]*100_000,c=&apos;black&apos;,s=15,label=event) ax.errorbar(data[&quot;vaccinated&quot;],data[&quot;y_pred&quot;],yerr=data[&quot;y_std&quot;],fmt=&quot;o&quot;,alpha=0.25,c=color, label=&quot;predicted&quot;) ax.set_xlabel(&quot;vaccinated&quot;) ax.set_ylabel(f&quot;{event} per 100k&quot;) ax.legend() 1plot_state(data,&apos;KY&apos;,&apos;tab:blue&apos;) 1plot_state(data,&apos;FL&apos;,&apos;tab:green&apos;) Predicted Dependence on vaccination RateOne of the things that we are interested is that how does the vaccination rate impact the admissions? To see this better, I fixed the time at Aug.15, and let the vaccination rate to vary from 20% to 100%. 123456# given current time point, generate test set for vaccination rate from 20% to 100%V_N=201V=np.linspace(0.2,1,V_N)T=np.ones(V_N)T2=T**2X2_test=np.c_[np.ones(T_N),V,T,T2] 1234with pm.Model() as mod: eta2_pred=pm.math.dot(X2_test,beta) y_pred2=pm.Deterministic(&quot;y_pred2&quot;,pm.math.exp(eta2_pred)*100_000) posterior_predictive = pm.sample_posterior_predictive(trace, var_names=[&quot;y_pred2&quot;]) 100%|████████████████████████████████████████████████████████████████████████████| 4000/4000 [00:01&lt;00:00, 3891.10it/s] 1234# generate predicted mean and standard deviationpredicted=posterior_predictive[&quot;y_pred2&quot;]y_pred_mean=predicted.mean(axis=0)y_pred_std=predicted.std(axis=0) We can see that vaccinations and admissions are negatively correlated. And when vaccination rate is low, the prediction can be quite uncertain. 12345plt.plot(V,y_pred_mean,&quot;k--&quot;,linewidth=1)plt.fill_between(V,y_pred_mean+y_pred_std,y_pred_mean-y_pred_std,alpha=0.07,color=&quot;k&quot;)plt.fill_between(V,y_pred_mean+2*y_pred_std,y_pred_mean-2*y_pred_std,alpha=0.07,color=&quot;k&quot;)plt.xlabel(&quot;Vaccinated&quot;)plt.ylabel(f&quot;{event} per 100k&quot;) Predicted Time EvolutionThe other thing that we are most interested in is that how the third wave of covid19 is going to evolve. How many admissions are there going to be given the current vaccination rate? So I use the GLMM model to project the next two months from Aug.15th on. 123# given the average vaccination ratesmean_vac=(train_data[&quot;vaccinated&quot;]*train_data[&quot;population&quot;]).sum()/train_data[&quot;population&quot;].sum()mean_vac 0.48852180987716365 12345# generate test set for the past month and the next 2 monthsT_N=201T=np.linspace(0,3,T_N)T2=T**2X1_test=np.c_[np.ones(T_N),mean_vac*np.ones(T_N),T,T2] 1234with pm.Model() as mod: eta1_pred=pm.math.dot(X1_test,beta) y_pred1=pm.Deterministic(&quot;y_pred1&quot;,pm.math.exp(eta1_pred)*100_000) posterior_predictive = pm.sample_posterior_predictive(trace, var_names=[&quot;y_pred1&quot;]) 100%|████████████████████████████████████████████████████████████████████████████| 4000/4000 [00:01&lt;00:00, 3831.47it/s] 1234# generate predicted mean and standard deviationpredicted=posterior_predictive[&quot;y_pred1&quot;]y_pred_mean=predicted.mean(axis=0)y_pred_std=predicted.std(axis=0) The prediction tells us that the admission will peak in one and half month from Aug.15th, which is by the end of September. However, I would be careful as the model have low confidence when projecting beyond one month. Look at that wide confidential interval band after T=2. Basically, the reality can fall any where between the curve flattening out as early as the start of September to that we won’t be able to see a peak until mid October. But overall I am very optimistic as there’s at least 80% of chance that we’ll see it peaking before mid October. Eventually, we will get there. So hang in there, everyone!! 12345678T0=T[T&lt;1]plt.plot(T0,y_pred_mean[T&lt;1],&quot;k&quot;,linewidth=4,label=&quot;fitted&quot;)plt.plot(T,y_pred_mean,&quot;k--&quot;,linewidth=1,label=&quot;extrapolated&quot;)plt.fill_between(T,y_pred_mean+y_pred_std,y_pred_mean-y_pred_std,alpha=0.07,color=&quot;k&quot;)plt.fill_between(T,y_pred_mean+2*y_pred_std,y_pred_mean-2*y_pred_std,alpha=0.07,color=&quot;k&quot;,)plt.xlabel(&quot;T (months)&quot;)plt.ylabel(f&quot;{event} per 100k&quot;)plt.legend(loc=&quot;lower right&quot;) Officially we are done, cheers! I hope you find this project interesting and inspiring. If you read it all the way through - congrats, you are an incredibly patient person, have a cookie!","link":"/2021/09/28/projectcovidmixed/"}],"tags":[],"categories":[{"name":"NLP","slug":"NLP","link":"/categories/NLP/"}]}