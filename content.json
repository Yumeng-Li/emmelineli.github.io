{"pages":[{"title":"categories","text":"","link":"/categories/index.html"}],"posts":[{"title":"Training Word Embedding on Bloomberg News","text":"While working from home (stuck at home indeed), I get more time, so last month revisited some deep learning courses, and thought it would be awesome to come up with some projects to play with, as we all know knowledge would go rusty if lack of use! Initially, I was thinking about a machine translator using Attention Models, because I was once asked about it in an interview. The fund showed a particular interest in developing such a tool for purpose of distributing their research reports to non-English-speaking countries. However, the thousands-of-dollar worth dataset set me back. While if labeled dataset is that hard to get, why not run some unsurprised learning? Training word2vec doesn’t sound like a bad idea as I’ve been writing views on FICC (fixed income, currencies, and commodities) on my other non-geeky-at-all blog. Won’t it be fun to run word embeddings on Bloomberg news, see if people talk about the right things when they analyze inflation and gold prices? Now let’s rock! First it’s a bit introduction to word embedding. The term refers to a set of techniques in natural language processing that can map words into high dimensional vectors of real numbers. The vectors/embeddings can be seen as featurized representations of words, which preserve semantic properties, hence are commonly used as inputs in NLP tasks. Here is an example, While in real cases, the dimension of vectors is usually a lot higher. We will use a tool called word2vec to convert words into embeddings, gensim has it implemented in a very easy-to-use way. Word2vec is a technique for natural language processing. The word2vec algorithm uses a neural network model to learn word associations from a large corpus of text. Once trained, such a model can detect synonymous words or suggest additional words for a partial sentence. More explanation on word2vec can be found here. Creating CorpusScarping Addresses of Bloomberg ArticlesI would like to run the embeddings only on the most recent posts, so first need to grab their urls through a search page. Import the libraries, 1234567import reimport requestsimport stringimport numpy as npimport gensim.models.word2vec as word2vecfrom bs4 import BeautifulSoupfrom time import sleep A function that can scrape the urls of articles that show up in the top p pages related to a specific topic, 123456789101112def scrape_bloomberg_urls(subject, maxPage, headers): urls = [] for p in range(1,maxPage): searchUrl = &apos;https://www.bloomberg.com/search?query=&apos; + subject + &apos;&amp;sort=relevance:desc&apos; + &apos;&amp;page=&apos; + str(p) response = requests.get(searchUrl, headers=headers) soup = BeautifulSoup(response.content, &apos;html.parser&apos;) regex = re.compile(&apos;.*headline.*&apos;) for tag in soup.findAll(&apos;a&apos;, {&quot;class&quot; : regex}, href = True): href = tag.attrs[&apos;href&apos;] if &apos;/news/articles/&apos; in href and href not in urls: urls.append(href) return urls Set up headers to pass to the request, websites have gone crazy about blocking robots, 1234567headers = { &apos;user-agent&apos;: &apos;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36&apos;, &apos;referrer&apos;: &apos;https://google.com&apos;, &apos;Accept&apos;: &apos;text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8&apos;, &apos;Accept-Encoding&apos;: &apos;gzip, deflate, br&apos;, &apos;Accept-Language&apos;: &apos;en-US,en;q=0.9&apos;, &apos;Pragma&apos;: &apos;no-cache&apos;} We get 155 articles here, not a large dataset so it may give us weird results, but should be good enough for an exercise, 12In: len(urls)Out: 155 1234567In: urls[:5]Out: [&apos;https://www.bloomberg.com/news/articles/2020-08-07/inflation-trend-a-friend-in-real-yield-contest-seasia-rates&apos;, &apos;https://www.bloomberg.com/news/articles/2020-08-06/blackrock-joins-crescendo-of-inflation-warnings-amid-virus-fight&apos;, &apos;https://www.bloomberg.com/news/articles/2020-08-06/china-inflation-rate-headed-for-0-on-cooling-food-prices-chart&apos;, &apos;https://www.bloomberg.com/news/articles/2020-08-06/inflation-binds-czechs-after-fast-rate-cuts-decision-day-guide&apos;, &apos;https://www.bloomberg.com/news/articles/2020-08-07/jpmorgan-rejects-threat-to-dollar-status-flagged-by-goldman&apos;] Parsing ArticlesThis step is to parse articles through the urls and save the corpus into a local text file. 123456789101112def parse_article(urls, headers): corpus = [] for url in urls: response = requests.get(url, headers=headers) soup = BeautifulSoup(response.content,&apos;lxml&apos;) for tag in soup.find_all(&apos;p&apos;): content = tag.get_text() cleanedContent = content.tra nslate(str.maketrans(&apos;&apos;, &apos;&apos;, string.punctuation)).lower() corpus.append(cleanedContent) seconds = np.random.randint(low=5, high=20) sleep(seconds) return corpus A function serves as a writer, 12345def write_to_txt(outdir, subject, contentList): outputfile = f&apos;{outdir}/{subject}.txt&apos; with open(outputfile, &apos;a&apos;) as file: for i in contentList: file.write(f&apos;{i}\\n&apos;) Run the functions, 12corpus = parse_article(urls, headers)write_to_txt(&apos;Documents/Blog/&apos;, &apos;corpus&apos;, corpus) Training the ModelNow that we have created the corpus, let’s train the model using word2vec from gensim. Gensim has a cool function LineSentence that can directly read sentences from a text file with one sentence a line. Now you probably get why I had the corpus saved this way. 12sentences = word2vec.LineSentence(&apos;corpus.txt&apos;)model = word2vec.Word2Vec(sentences, min_count=10, workers=8, iter=500, window=15, size=300, negative=50) The model I choose is using the Skip-Gram algorithm with Negative Sampling. Hyper-parameters above are what I find work well, see below for their definitions, just in case you would like to tune them yourself, size – Dimensionality of the word vectors. window – Maximum distance between the current and predicted word within a sentence. min_count – Ignores all words with total frequency lower than this. workers – Use these many worker threads to train the model (=faster training with multicore machines). negative – If &gt; 0, negative sampling will be used, the int for negative specifies how many “noise words” should be drawn (usually between 5-20). If set to 0, no negative sampling is used. Checking Out the ResultsThe goal of this exercise to find out what people talk about when they analyze inflation expectation (basically what currently drives the stock market, TIPS and metals to roar higher and higher) and gold (my favorite and probably the most promising asset in the next 10 years). Word2vev model can find words that have been used in a similar context with the word we care about. Here we employ a method most_similar to reveal them. Let’s first check out the words people usually mention when they talk about inflation. I print out the top 20 words. The meaningful ones are, “target”, “bank’s”, “bank” - of course central banks target on inflation and their monetary policies shape expectations. “nominal”, “negative”, “yields” - words usually describing rates, make sense. “food” - measure of inflation. “employment” - make a lot sense if you remember the Phillips curve. The results are actually very good, authors did understand and explain inflations well. 12345678910111213141516171819202122In: model.wv.most_similar(&apos;inflation&apos;,topn=20)Out:[(&apos;target&apos;, 0.25702178478240967), (&apos;strip&apos;, 0.18441016972064972), (&apos;securities&apos;, 0.18246109783649445), (&apos;full&apos;, 0.16774789988994598), (&apos;nominal&apos;, 0.16606125235557556), (&apos;bank’s&apos;, 0.162990003824234), (&apos;yields&apos;, 0.15935908257961273), (&apos;negative&apos;, 0.15806841850280762), (&apos;remain&apos;, 0.15371079742908478), (&apos;levels&apos;, 0.14426037669181824), (&apos;well&apos;, 0.1438026875257492), (&apos;current&apos;, 0.13828279078006744), (&apos;attractive&apos;, 0.13690069317817688), (&apos;showed&apos;, 0.1277044713497162), (&apos;bank&apos;, 0.12733221054077148), (&apos;food&apos;, 0.12377716600894928), (&apos;similar&apos;, 0.11955425888299942), (&apos;the&apos;, 0.11911500245332718), (&apos;employment&apos;, 0.11866464465856552), (&apos;keep&apos;, 0.11818670481443405)] Then let’s try it for gold. It’s good to see silver rank top, the two metals do hold a strong correlation despite the natures of them are very different, gold generally behaves as a bond of real rates, while silver should be viewed more as a commodity. (People make mistakes on this, see the story of Hunt Brothers) Other words are just descriptive, focusing on telling readers what’s going on in the markets. While this is not helpful, Bloomberg! People want to know why, you may want to talk more about real rates, inflation expectations, debt and global growth in the future. 1234567891011121314151617181920212223242526272829303132In: model.wv.most_similar(&apos;gold&apos;,topn=30)Out:[(&apos;gold’s&apos;, 0.26054084300994873), (&apos;silver&apos;, 0.25277888774871826), (&apos;ounce&apos;, 0.2523342967033386), (&apos;bullion&apos;, 0.2294640988111496), (&apos;2300&apos;, 0.22592982649803162), (&apos;spot&apos;, 0.2171008288860321), (&apos;climbing&apos;, 0.19733953475952148), (&apos;metal&apos;, 0.1939847469329834), (&apos;comex&apos;, 0.19371576607227325), (&apos;rally&apos;, 0.18643531203269958), (&apos;exchangetraded&apos;, 0.1859540343284607), (&apos;a&apos;, 0.18539577722549438), (&apos;delivery&apos;, 0.17595867812633514), (&apos;reaching&apos;, 0.1702871322631836), (&apos;strip&apos;, 0.16905872523784637), (&apos;posted&apos;, 0.16247007250785828), (&apos;lows&apos;, 0.16047437489032745), (&apos;as&apos;, 0.1585429608821869), (&apos;analysis&apos;, 0.1577225774526596), (&apos;2011&apos;, 0.15711694955825806), (&apos;precious&apos;, 0.15656355023384094), (&apos;threat&apos;, 0.1542907953262329), (&apos;more&apos;, 0.1541929841041565), (&apos;drive&apos;, 0.15310749411582947), (&apos;every&apos;, 0.1524747610092163), (&apos;analyst&apos;, 0.1517343521118164), (&apos;managing&apos;, 0.14977654814720154), (&apos;price&apos;, 0.1497490406036377), (&apos;amounts&apos;, 0.14969849586486816), (&apos;backed&apos;, 0.1450338065624237)]","link":"/2020/08/02/word2vec-on-bbg/"}],"tags":[],"categories":[{"name":"NLP","slug":"NLP","link":"/categories/NLP/"}]}