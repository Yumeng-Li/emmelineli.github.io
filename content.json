{"pages":[{"title":"categories","text":"","link":"/categories/index.html"}],"posts":[{"title":"Training Word Embedding on Bloomberg News","text":"While working from home (stuck at home indeed), I get more time, so last month revisited some deep learning courses, and thought it would be awesome if I can come up with some projects to play with, as we all know knowledge would go rusty if lack of use! Initially, I was thinking about a machine translator using Attention Models, because I was once asked about it in an interview. The fund showed a particular interest in developing such a tool for purpose of distributing their research reports to non-English-speaking countries. However, the thousands-of-dollar worth dataset set me back. While if labeled dataset is hard to get, why not run some unsurprised learning? Training word2vec is not a bad idea as I’ve been writing views on FICC (fixed income, currencies, and commodities) on my other non-geeky-at-all blog. Won’t it be fun to run word embeddings on Bloomberg views, see if people talk about the right things when they analyze inflation and gold prices? Now let’s rock! First it’s a bit introduction of word embedding. The term refers to a set of techniques in natural language processing that can map words into high dimensional vectors of real numbers. The vectors or embeddings can be seen as featurized representations of words, which preserve semantic properties and hence are commonly used as inputs in NPL tasks. Here is an example, While in real cases, the dimension of vectors is usually a lot higher. We will use a tool called word2vec to convert words into embeddings, gensim has it implemented in an easy-to-use way. Word2vec is a technique for natural language processing. The word2vec algorithm uses a neural network model to learn word associations from a large corpus of text. Once trained, such a model can detect synonymous words or suggest additional words for a partial sentence. More explanation on word2vec can be found here Create CorpusScarping Links to Bloomberg ArticlesI would like to run the embeddings only on the most recent articles, so first need to grab their urls through a search page. Import the libraries, 1234567import reimport requestsfrom bs4 import BeautifulSoupimport stringimport gensim.models.word2vec as word2vecfrom time import sleepimport numpy as np A function that can scrape the urls of articles that show up in the top p pages related to a specific topic, 123456789101112def scrape_bloomberg_urls(subject, maxPage, headers): urls = [] for p in range(1,maxPage): searchUrl = &apos;https://www.bloomberg.com/search?query=&apos; + subject + &apos;&amp;sort=relevance:desc&apos; + &apos;&amp;page=&apos; + str(p) response = requests.get(searchUrl, headers=headers) soup = BeautifulSoup(response.content, &apos;html.parser&apos;) regex = re.compile(&apos;.*headline.*&apos;) for tag in soup.findAll(&apos;a&apos;, {&quot;class&quot; : regex}, href = True): href = tag.attrs[&apos;href&apos;] if &apos;/news/articles/&apos; in href and href not in urls: urls.append(href) return urls Set up header passed to request, websites have gone crazy about blocking robots, 1234567headers = { &apos;user-agent&apos;: &apos;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36&apos;, &apos;referrer&apos;: &apos;https://google.com&apos;, &apos;Accept&apos;: &apos;text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8&apos;, &apos;Accept-Encoding&apos;: &apos;gzip, deflate, br&apos;, &apos;Accept-Language&apos;: &apos;en-US,en;q=0.9&apos;, &apos;Pragma&apos;: &apos;no-cache&apos;} I got 155 articles here, not a large dataset so it may give weird predictions, but good enough for an exercise, 12In: len(urls)Out: 155 1234567In: urls[:5]Out: [&apos;https://www.bloomberg.com/news/articles/2020-08-07/inflation-trend-a-friend-in-real-yield-contest-seasia-rates&apos;, &apos;https://www.bloomberg.com/news/articles/2020-08-06/blackrock-joins-crescendo-of-inflation-warnings-amid-virus-fight&apos;, &apos;https://www.bloomberg.com/news/articles/2020-08-06/china-inflation-rate-headed-for-0-on-cooling-food-prices-chart&apos;, &apos;https://www.bloomberg.com/news/articles/2020-08-06/inflation-binds-czechs-after-fast-rate-cuts-decision-day-guide&apos;, &apos;https://www.bloomberg.com/news/articles/2020-08-07/jpmorgan-rejects-threat-to-dollar-status-flagged-by-goldman&apos;] Parse ArticlesThis step is to parse the articles through the urls and save the corpus into a local txt file. 123456789101112def parse_article(urls, headers): corpus = [] for url in urls: response = requests.get(url, headers=headers) soup = BeautifulSoup(response.content,&apos;lxml&apos;) for tag in soup.find_all(&apos;p&apos;): content = tag.get_text() cleanedContent = content.tra nslate(str.maketrans(&apos;&apos;, &apos;&apos;, string.punctuation)).lower() corpus.append(cleanedContent) seconds = np.random.randint(low=5, high=20) sleep(seconds) return corpus A function serves as a writer, 12345def write_to_txt(outdir, subject, contentList): outputfile = f&apos;{outdir}/{subject}.txt&apos; with open(outputfile, &apos;a&apos;) as file: for i in contentList: file.write(f&apos;{i}\\n&apos;) Run the functions, 12corpus = parse_article(urls, headers)write_to_txt(&apos;Documents/Blog/&apos;, &apos;corpus&apos;, corpus) Training the ModelNow that we have created the corpus, let’s run the model using word2vec from gensim. Gensim has a cool function LineSentence that can directly read sentences from a text file with one sentence a line. Now you got why I saved the corpus this way at the first place. 12sentences = word2vec.LineSentence(&apos;all.txt&apos;)model = word2vec.Word2Vec(sentences, min_count=5, workers=8, iter=500, window=15, size=300, negative=50) 123`````` bash 123`````` bash 123`````` bash","link":"/2020/08/02/hello-world/"}],"tags":[],"categories":[{"name":"NLP","slug":"NLP","link":"/categories/NLP/"}]}