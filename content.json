{"pages":[{"title":"categories","text":"","link":"/categories/index.html"}],"posts":[{"title":"Training Word Embedding on Bloomberg News","text":"While working from home (stuck at home indeed), I get a lot more extra time, so last month I revisited my old deep learning course, and thought it would be awesome if I can come up with some projects to play with, as we all know knowledge would go rusty if lack of use… Initially, I was thinking about a machine translator using Attention Models, because I was once asked about it in an interview. The fund showed a particular interest in developing such a tool for purpose of distributing their research reports to non-English-speaking countries. However, the thousands-of-dollar worth dataset set me back. While if labeled dataset is hard to get, why not run some unsurprised learning? Training word2vec is not a bad idea as I’ve been writing views on FICC (fixed income, currencies, and commodities) on my other non-geeky-at-all blog. Won’t it be fun to run word embeddings on Bloomberg views, see if people talk about the right things when they analyze inflation and gold prices? Now let’s rock! First it’s a bit introduction of word embedding. The term refers to a set of techniques in natural language processing that can map words into high dimensional vectors of real numbers. The vectors or embeddings can be seen as featurized representations of words, which preserve semantic properties and hence are commonly used as inputs in NPL tasks. Here is an example, ![vectors](http://yumeng-li.github.io/pics/embeddingVectors.png =30x) While in real cases, the dimension of vectors is usually a lot higher. Word2vec is one of the word embedding techniques, also the tool that we are going to use today. Word2vec is a technique for natural language processing. The word2vec algorithm uses a neural network model to learn word associations from a large corpus of text. Once trained, such a model can detect synonymous words or suggest additional words for a partial sentence. Scarping Bloomberg NewsScarping Bloomberg News123456789101112def scrape_bloomberg_urls(subject, maxPage, headers): urls = [] for p in range(1,maxPage): searchUrl = &apos;https://www.bloomberg.com/search?query=&apos; + subject + &apos;&amp;sort=relevance:desc&apos; + &apos;&amp;page=&apos; + str(p) response = requests.get(searchUrl, headers=headers) soup = BeautifulSoup(response.content, &apos;html.parser&apos;) regex = re.compile(&apos;.*headline.*&apos;) for tag in soup.findAll(&apos;a&apos;, {&quot;class&quot; : regex}, href = True): href = tag.attrs[&apos;href&apos;] if &apos;/news/articles/&apos; in href and href not in urls: urls.append(href) return urls More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","link":"/2020/08/02/hello-world/"}],"tags":[],"categories":[{"name":"NLP","slug":"NLP","link":"/categories/NLP/"}]}